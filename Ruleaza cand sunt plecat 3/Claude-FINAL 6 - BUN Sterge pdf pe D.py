#!/usr/bin/env python3
"""
Automatizare descƒÉrcare PDF-uri din Arcanum (FIXED VERSION cu SORTARE CRONOLOGICƒÇ):
- FIXED: ScaneazƒÉ corect toate fi»ôierele existente de pe disk
- FIXED: PƒÉstreazƒÉ progresul par»õial √Æntre zile
- FIXED: ProceseazƒÉ »ôi combinƒÉ corect TOATE PDF-urile pentru fiecare issue
- FIXED: Resume logic corect pentru issue-urile par»õiale
- FIXED: DetecteazƒÉ corect prefix-urile pentru fi»ôiere
- FIXED: VerificƒÉ corect issue-urile complete pentru skip URLs
- FIXED: EliminƒÉ dublurile automat
- FIXED: DetecteazƒÉ mai bine numƒÉrul total de pagini
- FIXED: Sortare cronologicƒÉ corectƒÉ √Æn downloaded_issues
"""

import time
import os
import sys
import re
import json
import shutil
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException, ElementClickInterceptedException

import logging
import sys

def setup_logging():
    """ConfigureazƒÉ logging √Æn timp real"""
    log_dir = r"E:\Carte\BB\17 - Site Leadership\alte\Ionel Balauta\Aryeht\Task 1 - Traduce tot site-ul\Doar Google Web\Andreea\Meditatii\2023\++Arcanum Download + Chrome\Ruleaza cand sunt plecat 3\Logs"
    os.makedirs(log_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"arcanum_download_{timestamp}.log")

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[
            logging.FileHandler(log_file, mode='w', encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )

    for handler in logging.getLogger().handlers:
        if isinstance(handler, logging.FileHandler):
            handler.stream.reconfigure(line_buffering=True)

    print(f"üìù LOGGING ACTIVAT: {log_file}")
    return log_file

# Colec»õiile adi»õionale (procesate DUPƒÇ colec»õia principalƒÉ din main())
ADDITIONAL_COLLECTIONS = [
    "https://adt.arcanum.com/ro/collection/StudiiSiCercetariMecanicaSiAplicata/", # lasi asta obligatoriu
    "https://adt.arcanum.com/ro/collection/RevistaMatematicaDinTimisoara/",
    "https://adt.arcanum.com/ro/collection/StudiiSiCercetariMatematice/",
    "https://adt.arcanum.com/ro/collection/AnaleleUnivBucuresti_MatematicaMecanicaFizica/",
    "https://adt.arcanum.com/hu/collection/AMatematikaTanitasa/",
    "https://adt.arcanum.com/hu/collection/AFizikaTanitasa/",
    "https://adt.arcanum.com/hu/collection/AKemiaTanitasa/",
    "https://adt.arcanum.com/sk/collection/Polnohospodarstvo_OSA/",
    "https://adt.arcanum.com/de/collection/MonatshefteFurChemie/",
    "https://adt.arcanum.com/de/collection/ZeitschriftFurPhysikUndMathematik/",
    "https://adt.arcanum.com/de/collection/Mathematische/",
    "https://adt.arcanum.com/de/collection/ZeitschriftFurPhysikUndMathematik/",
    "https://adt.arcanum.com/ro/collection/BuletInstPolitehIasi_1/",
    "https://adt.arcanum.com/ro/collection/AutomaticaSiElectronica/",
    "https://adt.arcanum.com/ro/collection/StudiisiCercetariDeCalculEconomicSiCiberneticaEconomica/",
    "https://adt.arcanum.com/ro/collection/GazetaMatematica/",
    "https://adt.arcanum.com/ro/collection/GInfo/",
    "https://adt.arcanum.com/ro/collection/Hidrotechnica/",
    "https://adt.arcanum.com/ro/collection/Energetica/",
    "https://adt.arcanum.com/ro/collection/IndustriaConstructiilor/",
    "https://adt.arcanum.com/ro/collection/RevistaConstructilorMaterialelorDeConstructii/",
    "https://adt.arcanum.com/ro/collection/ConstructiaDeMasini/",
    "https://adt.arcanum.com/ro/collection/Electrotehnica/",
    "https://adt.arcanum.com/ro/collection/StudiiSiCercetariDeEnergetica/",
    "https://adt.arcanum.com/ro/collection/CulturaFizicaSiSport/",
    "https://adt.arcanum.com/ro/collection/Almanahul_Satelor/",
    "https://adt.arcanum.com/ro/collection/SteauaRomaniei/",
    "https://adt.arcanum.com/ro/collection/RevistaCailorFerate/",
    "https://adt.arcanum.com/ro/collection/RevistaDeFolclor/",
    "https://adt.arcanum.com/ro/collection/AlmanahBTT/",
    "https://adt.arcanum.com/ro/collection/MinePetrolGaze/",
    "https://adt.arcanum.com/ro/collection/BuletinulInstitutuluiPolitehnicBucuresti_Mecanica/",
    "https://adt.arcanum.com/ro/collection/RevistaMinelor/",
    "https://adt.arcanum.com/ro/collection/StudiiSiCercetariDeGeologie/",
    "https://adt.arcanum.com/ro/collection/RevistaIndustriaAlimentare/",
    "https://adt.arcanum.com/ro/collection/IndustriaLemnului/",
    "https://adt.arcanum.com/ro/collection/Electricitatea/",
    "https://adt.arcanum.com/ro/collection/SzatmariMuzeumKiadvanyai_Evkonyv_ADT/",
    "https://adt.arcanum.com/ro/collection/ConstructiaDeMasini/",
    "https://adt.arcanum.com/ro/collection/SufletNou/",
    "https://adt.arcanum.com/ro/collection/Rondul/",
    "https://adt.arcanum.com/ro/collection/RomaniaMilitara/",
    "https://adt.arcanum.com/ro/collection/Magazin/",
    "https://adt.arcanum.com/ro/collection/TribunaSibiului/",
    "https://adt.arcanum.com/ro/collection/Metalurgia/",
    "https://adt.arcanum.com/ro/collection/RevistaEconomica1974/",
    "https://adt.arcanum.com/ro/collection/StudiiSiCercetariDeMetalurgie/",
    "https://adt.arcanum.com/ro/collection/BuletinulInstitutuluiPolitehnicBucuresti_ChimieMetalurgie/",
    "https://adt.arcanum.com/ro/collection/RomaniaMuncitoare/",
    "https://adt.arcanum.com/ro/collection/Cronica/",
    "https://adt.arcanum.com/ro/collection/Marisia_ADT/",
    "https://adt.arcanum.com/ro/collection/RevistaDeEtnografieSiFolclor/",
    "https://adt.arcanum.com/ro/collection/RevistaMuzeelor/",
    "https://adt.arcanum.com/ro/collection/GazetaDeDuminica/",
    "https://adt.arcanum.com/ro/collection/BuletInstPolitehIasi_0/",
    "https://adt.arcanum.com/ro/collection/Fotbal/",
    "https://adt.arcanum.com/ro/collection/JurnalulNational/",
    "https://adt.arcanum.com/ro/collection/UnireaBlajPoporului/",
    "https://adt.arcanum.com/ro/collection/TranssylvaniaNostra/",
    "https://adt.arcanum.com/ro/collection/CuvantulPoporului/",
    "https://adt.arcanum.com/ro/collection/Agrarul/",
    "https://adt.arcanum.com/ro/collection/Radiofonia/",
    "https://adt.arcanum.com/ro/collection/CurierulDeIasi/",
    "https://adt.arcanum.com/ro/collection/BuletinulUniversitatiiDinBrasov/",
    "https://adt.arcanum.com/ro/collection/CurierulFoaiaIntereselorGenerale/",
    "https://adt.arcanum.com/ro/collection/EconomiaNationala/",
    "https://adt.arcanum.com/ro/collection/Constitutionalul/",
    "https://adt.arcanum.com/ro/collection/Semnalul/",
    "https://adt.arcanum.com/ro/collection/Rampa/",
    "https://adt.arcanum.com/ro/collection/ViataRomaneasca/",
    "https://adt.arcanum.com/ro/collection/SteauaRosie/",
    "https://adt.arcanum.com/ro/collection/Almanah_ScinteiaTineretului/",
    "https://adt.arcanum.com/ro/collection/MuzeulDigitalAlRomanuluiRomanesc/",
    "https://adt.arcanum.com/ro/collection/EvenimentulZilei/",
    "https://adt.arcanum.com/ro/collection/CurierulFoaiaIntereselorGenerale/",
    "https://adt.arcanum.com/hu/collection/IdegenNyelvekTanitasa/",
    "https://adt.arcanum.com/hu/collection/AzEnekZeneTanitasa/",
    "https://adt.arcanum.com/hu/collection/ABiologiaTanitasa/",
    "https://adt.arcanum.com/hu/collection/ErtekezesekEmlekezesek/",
    "https://adt.arcanum.com/hu/collection/Books_SorozatonKivul/",
    "https://adt.arcanum.com/hu/collection/Books_22_OrvoslasTermeszetrajz/",
    "https://adt.arcanum.com/hu/collection/DomolkiKonyvek/",
    "https://adt.arcanum.com/de/collection/LiterarischeBerichteUngarn/",
    "https://adt.arcanum.com/sk/collection/SlovenskyZakonnik/",
    "https://adt.arcanum.com/sk/collection/Theologos/"
]

# Skip URLs statice (hardcoded)
STATIC_SKIP_URLS = {
    "https://adt.arcanum.com/ro/view/Convietuirea_1997-1998"
}

DAILY_LIMIT = 1050
STATE_FILENAME = "state.json"
SKIP_URLS_FILENAME = "skip_urls.json"


class ChromePDFDownloader:
    def __init__(self, main_collection_url, download_dir=None, batch_size=50, timeout=15):
        self.main_collection_url = main_collection_url
        self.batch_size = batch_size
        self.timeout = timeout
        self.download_dir = download_dir or os.getcwd()
        self.driver = None
        self.wait = None
        self.attached_existing = False
        self.state_path = os.path.join(self.download_dir, STATE_FILENAME)
        self.skip_urls_path = os.path.join(self.download_dir, SKIP_URLS_FILENAME)
        self.current_issue_url = None
        self.dynamic_skip_urls = set()
        self._load_skip_urls()
        self._load_state()
        self.fix_existing_json()

    def _load_skip_urls(self):
        """√éncarcƒÉ skip URLs din fi»ôierul separat"""
        self.dynamic_skip_urls = set(STATIC_SKIP_URLS)  # √éncepe cu cele statice

        if os.path.exists(self.skip_urls_path):
            try:
                with open(self.skip_urls_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    completed_urls = data.get("completed_urls", [])
                    completed_collections = data.get("completed_collections", [])

                    self.dynamic_skip_urls.update(url.rstrip('/') for url in completed_urls)
                    self.dynamic_skip_urls.update(url.rstrip('/') for url in completed_collections)

                    print(f"üìã √éncƒÉrcat {len(completed_urls)} URL-uri complet descƒÉrcate din {SKIP_URLS_FILENAME}")
                    print(f"üìã √éncƒÉrcat {len(completed_collections)} colec»õii complet procesate din {SKIP_URLS_FILENAME}")
            except Exception as e:
                print(f"‚ö† Eroare la citirea {SKIP_URLS_FILENAME}: {e}")

        print(f"üö´ Total URL-uri de skip: {len(self.dynamic_skip_urls)}")

    def _save_skip_urls(self):
        """FIXED: VerificƒÉ corect dacƒÉ un issue este complet - FOLOSE»òTE last_successful_segment_end!"""
        try:
            completed_urls = []
            for item in self.state.get("downloaded_issues", []):
                # VERIFICARE CORECTƒÇ: folose»ôte last_successful_segment_end, NU pages!
                completed_at = item.get("completed_at")
                total_pages = item.get("total_pages")
                last_segment = item.get("last_successful_segment_end", 0)
                pages = item.get("pages", 0)  # Pentru debug

                # CONDI»öIE FIXATƒÇ: verificƒÉ progresul REAL (last_segment), nu pages!
                if (completed_at and  # Marcat ca terminat
                    total_pages and  # Are total_pages setat
                    total_pages > 0 and  # Total valid
                    last_segment >= total_pages):  # Progresul REAL este complet

                    completed_urls.append(item["url"])
                    print(f"‚úÖ Issue complet pentru skip: {item['url']} ({last_segment}/{total_pages})")

                    # DEBUG: Afi»ôeazƒÉ discrepan»õele
                    if pages != last_segment:
                        print(f"   ‚ö† DISCREPAN»öƒÇ: pages={pages}, last_segment={last_segment}")
                else:
                    # DEBUG: Afi»ôeazƒÉ de ce nu e considerat complet
                    if item.get("url"):  # Doar dacƒÉ are URL valid
                        print(f"üîÑ Issue incomplet: {item.get('url', 'NO_URL')}")
                        print(f"   completed_at: {bool(completed_at)}")
                        print(f"   total_pages: {total_pages}")
                        print(f"   last_segment: {last_segment}")
                        print(f"   pages: {pages}")

                        # VerificƒÉ fiecare condi»õie individual
                        if not completed_at:
                            print(f"   ‚Üí Lipse»ôte completed_at")
                        elif not total_pages or total_pages <= 0:
                            print(f"   ‚Üí total_pages invalid")
                        elif last_segment < total_pages:
                            print(f"   ‚Üí Progres incomplet: {last_segment}/{total_pages}")

            # AdaugƒÉ »ôi cele statice
            all_completed = list(STATIC_SKIP_URLS) + completed_urls

            # PƒÉstreazƒÉ »ôi colec»õiile complete dacƒÉ existƒÉ
            existing_data = {}
            if os.path.exists(self.skip_urls_path):
                with open(self.skip_urls_path, "r", encoding="utf-8") as f:
                    existing_data = json.load(f)

            data = {
                "last_updated": datetime.now().isoformat(),
                "completed_urls": sorted(list(set(all_completed))),
                "completed_collections": existing_data.get("completed_collections", [])
            }

            with open(self.skip_urls_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            print(f"üíæ Salvat {len(data['completed_urls'])} URL-uri CORECT VERIFICATE √Æn {SKIP_URLS_FILENAME}")

            # RAPORT FINAL pentru debugging
            print(f"üìã ISSUES COMPLETE √Æn skip_urls:")
            for url in sorted(completed_urls):
                year = url.split('_')[-1] if '_' in url else 'UNKNOWN'
                print(f"   ‚úÖ {year}")

        except Exception as e:
            print(f"‚ö† Eroare la salvarea {SKIP_URLS_FILENAME}: {e}")

    def _safe_folder_name(self, name: str) -> str:
        return re.sub(r'[<>:"/\\|?*]', '_', name).strip()

    def _decode_unicode_escapes(self, obj):
        """DecodificƒÉ secven»õele unicode din JSON"""
        if isinstance(obj, dict):
            return {key: self._decode_unicode_escapes(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._decode_unicode_escapes(item) for item in obj]
        elif isinstance(obj, str):
            # DecodificƒÉ secven»õele unicode ca \u0103, \u0219
            try:
                return obj.encode('utf-8').decode('unicode_escape').encode('latin-1').decode('utf-8') if '\\u' in obj else obj
            except:
                return obj
        else:
            return obj

    def is_issue_complete_by_end_page(self, end_page):
        """FIXED: DeterminƒÉ dacƒÉ un issue e complet pe baza ultimei pagini"""
        # VERIFICARE CRITICƒÇ: DacƒÉ e doar 1 paginƒÉ, probabil e o eroare de detectare
        if end_page <= 1:
            print(f"‚ö† ALERTƒÇ: end_page={end_page} pare sƒÉ fie o eroare de detectare!")
            return False  # NU considera complet dacƒÉ e doar 1 paginƒÉ

        # Pentru issue-uri normale, verificƒÉ dacƒÉ ultima paginƒÉ nu e multiplu rotund
        return not ((end_page + 1) % 50 == 0 or (end_page + 1) % 100 == 0)

    def extract_issue_id_from_filename(self, filename):
        """FIXED: Extrage ID-ul issue-ului din numele fi»ôierului (fƒÉrƒÉ timestamp)"""
        # CautƒÉ pattern-ul: PrefixIssue-TIMESTAMP__pages
        match = re.search(r'([^-]+(?:-[^-]+)*)-\d+__pages', filename)
        if match:
            return match.group(1)
        return None

    def extract_issue_url_from_filename(self, filename):
        """FIXED: Extrage URL-ul issue-ului din numele fi»ôierului"""
        issue_id = self.extract_issue_id_from_filename(filename)
        if not issue_id:
            return None

        if "Convietuirea" in issue_id:
            return f"https://adt.arcanum.com/ro/view/{issue_id}"
        elif "GazetaMatematica" in issue_id:
            return f"https://adt.arcanum.com/en/view/{issue_id}"
        else:
            return f"https://adt.arcanum.com/ro/view/{issue_id}"

    def get_all_pdf_segments_for_issue(self, issue_url):
        """FIXED: ScaneazƒÉ toate fi»ôierele PDF pentru un issue specific"""
        issue_id = issue_url.rstrip('/').split('/')[-1]
        segments = []

        try:
            for filename in os.listdir(self.download_dir):
                if not filename.lower().endswith('.pdf'):
                    continue

                file_issue_id = self.extract_issue_id_from_filename(filename)
                if file_issue_id == issue_id:
                    # Extrage intervalul de pagini
                    match = re.search(r'__pages(\d+)-(\d+)\.pdf', filename)
                    if match:
                        start_page = int(match.group(1))
                        end_page = int(match.group(2))
                        segments.append({
                            'filename': filename,
                            'start': start_page,
                            'end': end_page,
                            'path': os.path.join(self.download_dir, filename)
                        })

        except Exception as e:
            print(f"‚ö† Eroare la scanarea fi»ôierelor pentru {issue_url}: {e}")

        # SorteazƒÉ dupƒÉ pagina de √Ænceput
        segments.sort(key=lambda x: x['start'])
        return segments

    def get_existing_pdf_segments(self, issue_url):
        """FIXED: ScaneazƒÉ toate segmentele existente »ôi returneazƒÉ ultima paginƒÉ"""
        segments = self.get_all_pdf_segments_for_issue(issue_url)

        if not segments:
            return 0

        # GƒÉse»ôte cea mai mare paginƒÉ finalƒÉ
        max_page = max(seg['end'] for seg in segments)

        print(f"üìä Fi»ôiere PDF existente pentru {issue_url}:")
        for seg in segments:
            print(f"   üìÑ {seg['filename']} (pagini {seg['start']}-{seg['end']})")

        return max_page

    def reconstruct_all_issues_from_disk(self):
        """FIXED: Reconstruie»ôte complet progresul din fi»ôierele de pe disk"""
        print("üîç SCANEZ COMPLET toate fi»ôierele PDF de pe disk...")

        # GrupeazƒÉ fi»ôierele dupƒÉ issue ID
        issues_on_disk = {}

        try:
            for filename in os.listdir(self.download_dir):
                if not filename.lower().endswith('.pdf'):
                    continue

                issue_id = self.extract_issue_id_from_filename(filename)
                if not issue_id:
                    continue

                # Extrage intervalul de pagini
                match = re.search(r'__pages(\d+)-(\d+)\.pdf', filename)
                if not match:
                    continue

                start_page = int(match.group(1))
                end_page = int(match.group(2))

                if issue_id not in issues_on_disk:
                    issues_on_disk[issue_id] = {
                        'segments': [],
                        'max_page': 0,
                        'url': self.extract_issue_url_from_filename(filename)
                    }

                issues_on_disk[issue_id]['segments'].append({
                    'filename': filename,
                    'start': start_page,
                    'end': end_page
                })

                if end_page > issues_on_disk[issue_id]['max_page']:
                    issues_on_disk[issue_id]['max_page'] = end_page

        except Exception as e:
            print(f"‚ö† Eroare la scanarea disk-ului: {e}")
            return {}

        # Afi»ôeazƒÉ rezultatele scanƒÉrii
        print(f"üìä GƒÉsite {len(issues_on_disk)} issue-uri pe disk:")
        for issue_id, data in issues_on_disk.items():
            segments_count = len(data['segments'])
            max_page = data['max_page']
            url = data['url']

            print(f"   üìÅ {issue_id}: {segments_count} segmente, max pagina {max_page}")
            print(f"      üîó URL: {url}")

            # Afi»ôeazƒÉ segmentele sortate
            data['segments'].sort(key=lambda x: x['start'])
            for seg in data['segments'][:3]:  # Primele 3
                print(f"      üìÑ {seg['filename']} ({seg['start']}-{seg['end']})")
            if segments_count > 3:
                print(f"      üìÑ ... »ôi √ÆncƒÉ {segments_count - 3} segmente")

        return issues_on_disk

    def sync_json_with_disk_files(self):
        """SAFE: √émbogƒÉ»õe»ôte informa»õiile din JSON cu cele de pe disk, ZERO pierderi + SORTARE CRONOLOGICƒÇ CORECTƒÇ"""
        print("üîÑ MERGE SAFE - combinez informa»õiile din JSON cu cele de pe disk...")

        # PASUL 1: ScaneazƒÉ complet disk-ul
        issues_on_disk = self.reconstruct_all_issues_from_disk()

        # PASUL 2: PƒÇSTREAZƒÇ TOATE issue-urile existente din JSON (ZERO pierderi)
        existing_issues_by_url = {}
        for item in self.state.get("downloaded_issues", []):
            url = item.get("url", "").rstrip('/')
            existing_issues_by_url[url] = item.copy()  # DEEP COPY pentru siguran»õƒÉ

        print(f"üìã PƒÇSTREZ {len(existing_issues_by_url)} issue-uri din JSON existent")

        # PASUL 3: MERGE cu datele de pe disk (doar √ÆmbogƒÉ»õe»ôte, nu »ôterge)
        enriched_count = 0
        new_from_disk_count = 0

        for issue_id, disk_data in issues_on_disk.items():
            url = disk_data['url']
            if not url:
                continue

            max_page = disk_data['max_page']
            segments_count = len(disk_data['segments'])
            is_complete = self.is_issue_complete_by_end_page(max_page)

            if url in existing_issues_by_url:
                # √éMBOGƒÇ»öE»òTE issue-ul existent (doar dacƒÉ progresul e mai mare)
                existing_issue = existing_issues_by_url[url]
                current_progress = existing_issue.get("last_successful_segment_end", 0)

                if max_page > current_progress:
                    # √éMBOGƒÇ»öE»òTE doar c√¢mpurile necesare, pƒÉstreazƒÉ restul
                    existing_issue["last_successful_segment_end"] = max_page
                    if not existing_issue.get("total_pages"):
                        existing_issue["total_pages"] = max_page
                    enriched_count += 1
                    print(f"üîÑ √éMBOGƒÇ»öIT: {url} - progres {current_progress} ‚Üí {max_page}")

                # MarcheazƒÉ ca complet DOAR dacƒÉ nu era deja marcat
                if is_complete and not existing_issue.get("completed_at"):
                    existing_issue["completed_at"] = datetime.now().isoformat(timespec="seconds")
                    existing_issue["pages"] = max_page
                    existing_issue["total_pages"] = max_page
                    print(f"‚úÖ MARCAT ca complet: {url} ({max_page} pagini)")

            else:
                # Issue complet nou gƒÉsit doar pe disk - ADAUGƒÇ
                new_issue = {
                    "url": url,
                    "title": issue_id.replace("-", " ").replace("_", " "),
                    "subtitle": "",
                    "pages": max_page if is_complete else 0,
                    "completed_at": datetime.now().isoformat(timespec="seconds") if is_complete else "",
                    "last_successful_segment_end": max_page,
                    "total_pages": max_page if is_complete else None
                }
                existing_issues_by_url[url] = new_issue
                new_from_disk_count += 1
                print(f"‚ûï ADƒÇUGAT nou din disk: {url} ({max_page} pagini, {segments_count} segmente)")

        # PASUL 4: Reconstruie»ôte lista finalƒÉ (TOATE issue-urile pƒÉstrate)
        all_issues_list = list(existing_issues_by_url.values())

        # PASUL 5: SORTARE CRONOLOGICƒÇ CORECTƒÇ
        partial_issues = []
        complete_issues = []

        for issue in all_issues_list:
            is_partial = (issue.get("last_successful_segment_end", 0) > 0 and
                         not issue.get("completed_at") and
                         issue.get("total_pages") and
                         issue.get("last_successful_segment_end", 0) < issue.get("total_pages", 0))

            if is_partial:
                partial_issues.append(issue)
                print(f"üîÑ Issue par»õial: {issue['url']} ({issue.get('last_successful_segment_end', 0)}/{issue.get('total_pages', 0)} pagini)")
            else:
                complete_issues.append(issue)

        # SORTARE CRONOLOGICƒÇ PENTRU COMPLETE ISSUES
        # SorteazƒÉ issue-urile complete dupƒÉ completed_at (cel mai recent primul)
        def sort_key_for_complete(issue):
            completed_at = issue.get("completed_at", "")
            if completed_at:
                try:
                    # Converte»ôte la datetime pentru sortare corectƒÉ
                    return datetime.fromisoformat(completed_at.replace('Z', '+00:00'))
                except:
                    return datetime.min
            else:
                # Issue-urile fƒÉrƒÉ completed_at merg la sf√¢r»ôit
                return datetime.min

        # SorteazƒÉ: par»õiale dupƒÉ progres (desc), complete dupƒÉ data (desc - cel mai recent primul)
        partial_issues.sort(key=lambda x: x.get("last_successful_segment_end", 0), reverse=True)
        complete_issues.sort(key=sort_key_for_complete, reverse=True)  # Cel mai recent primul

        print(f"\nüìä SORTARE CRONOLOGICƒÇ APLICATƒÇ:")
        print(f"   üîÑ Issue-uri par»õiale: {len(partial_issues)} (sortate dupƒÉ progres)")

        if complete_issues:
            print(f"   ‚úÖ Issue-uri complete: {len(complete_issues)} (sortate cronologic)")
            print(f"      üìÖ Cel mai recent: {complete_issues[0].get('completed_at', 'N/A')}")
            print(f"      üìÖ Cel mai vechi: {complete_issues[-1].get('completed_at', 'N/A')}")

            # Afi»ôeazƒÉ primele 5 pentru verificare
            print(f"      üîç Ordinea cronologicƒÉ (primele 5):")
            for i, issue in enumerate(complete_issues[:5]):
                url = issue.get('url', '').split('/')[-1]
                completed_at = issue.get('completed_at', 'N/A')
                print(f"         {i+1}. {url} - {completed_at}")

        # PASUL 6: ActualizeazƒÉ starea SAFE (pƒÉstreazƒÉ tot ce nu modificƒÉm)
        original_count = self.state.get("count", 0)
        final_issues = partial_issues + complete_issues  # Par»õiale primul, apoi complete cronologic
        actual_complete_count = len([i for i in final_issues if i.get("completed_at")])

        # PƒÇSTREAZƒÇ toate c√¢mpurile existente, actualizeazƒÉ doar ce e necesar
        self.state["downloaded_issues"] = final_issues
        self.state["count"] = max(original_count, actual_complete_count)  # Nu scade niciodatƒÉ

        self._save_state_safe()

        print(f"‚úÖ MERGE COMPLET cu SORTARE CRONOLOGICƒÇ CORECTƒÇ - ZERO pierderi:")
        print(f"   üìä Total issues: {len(final_issues)} (√Ænainte: {len(existing_issues_by_url) - new_from_disk_count})")
        print(f"   üîÑ √émbogƒÉ»õite: {enriched_count}")
        print(f"   ‚ûï AdƒÉugate din disk: {new_from_disk_count}")
        print(f"   üîÑ Par»õiale: {len(partial_issues)}")
        print(f"   ‚úÖ Complete: {len(complete_issues)}")
        print(f"   üéØ Count pƒÉstrat/actualizat: {original_count} ‚Üí {self.state['count']}")

        if partial_issues:
            print("üéØ Issue-urile par»õiale vor fi procesate primele!")

        print("üìÖ Issue-urile complete sunt acum sortate cronologic (cel mai recent primul)!")

    def cleanup_duplicate_issues(self):
        """NOUƒÇ FUNC»öIE: EliminƒÉ dublurile din state.json"""
        print("üßπ CURƒÇ»öARE: Verific »ôi elimin dublurile din state.json...")

        issues = self.state.get("downloaded_issues", [])
        if not issues:
            return

        # GrupeazƒÉ dupƒÉ URL normalizat
        url_groups = {}
        for i, item in enumerate(issues):
            url = item.get("url", "").rstrip('/').lower()
            if not url:
                continue

            if url not in url_groups:
                url_groups[url] = []
            url_groups[url].append((i, item))

        # GƒÉse»ôte »ôi rezolvƒÉ dublurile
        duplicates_found = 0
        clean_issues = []
        processed_urls = set()

        for original_url, group in url_groups.items():
            if len(group) > 1:
                duplicates_found += 1
                print(f"üîç DUBLURƒÇ gƒÉsitƒÉ pentru {original_url}: {len(group)} intrƒÉri")

                # GƒÉse»ôte cea mai completƒÉ versiune
                best_item = None
                best_score = -1

                for idx, item in group:
                    score = 0
                    if item.get("completed_at"): score += 100
                    if item.get("total_pages"): score += 50
                    if item.get("title"): score += 10
                    if item.get("last_successful_segment_end", 0) > 0: score += 20

                    print(f"   üìä Index {idx}: score {score}, completed: {bool(item.get('completed_at'))}")

                    if score > best_score:
                        best_score = score
                        best_item = item

                print(f"   ‚úÖ PƒÉstrez cea mai completƒÉ versiune (score: {best_score})")
                clean_issues.append(best_item)
            else:
                # Nu e dublurƒÉ, pƒÉstreazƒÉ-l
                clean_issues.append(group[0][1])

            processed_urls.add(original_url)

        if duplicates_found > 0:
            print(f"üßπ ELIMINAT {duplicates_found} dubluri din {len(issues)} issues")
            print(f"üìä RƒÉmas cu {len(clean_issues)} issues unice")

            self.state["downloaded_issues"] = clean_issues
            self._save_state_safe()
        else:
            print("‚úÖ Nu am gƒÉsit dubluri √Æn state.json")

    def is_issue_really_complete(self, item):
            """HELPER: VerificƒÉ dacƒÉ un issue este REAL complet (nu doar marcat ca atare)"""
            completed_at = item.get("completed_at")
            last_segment = item.get("last_successful_segment_end", 0)
            total_pages = item.get("total_pages")
            pages = item.get("pages", 0)

            # Un issue este REAL complet dacƒÉ:
            # 1. Are completed_at setat »òI
            # 2. Are progresul complet (last_segment >= total_pages) »òI
            # 3. Are pages > 0 (nu e marcat gre»ôit)
            return (
                completed_at and
                total_pages and
                total_pages > 0 and
                last_segment >= total_pages and
                pages > 0
            )

    def fix_incorrectly_marked_complete_issues(self):
            """NOUƒÇ FUNC»öIE: CorecteazƒÉ issue-urile marcate gre»ôit ca complete"""
            print("üîß CORECTEZ issue-urile marcate GRE»òIT ca complete...")

            fixes_applied = 0

            for item in self.state.get("downloaded_issues", []):
                completed_at = item.get("completed_at")
                last_segment = item.get("last_successful_segment_end", 0)
                total_pages = item.get("total_pages")
                pages = item.get("pages", 0)
                url = item.get("url", "")

                # DetecteazƒÉ issue-uri marcate gre»ôit ca complete
                if (completed_at and
                    pages == 0 and
                    total_pages and
                    last_segment < total_pages):

                    print(f"üö® CORECTEZ issue marcat GRE»òIT ca complet: {url}")
                    print(f"   √énainte: completed_at={completed_at}, pages={pages}")
                    print(f"   Progres real: {last_segment}/{total_pages}")

                    # »òterge completed_at pentru a-l face par»õial din nou
                    item["completed_at"] = ""
                    item["pages"] = 0  # AsigurƒÉ-te cƒÉ pages rƒÉm√¢ne 0

                    fixes_applied += 1
                    print(f"   DupƒÉ: completed_at='', pages=0 (va fi reluat)")

            if fixes_applied > 0:
                print(f"üîß CORECTAT {fixes_applied} issue-uri marcate gre»ôit ca complete")
                self._save_state_safe()

                # ActualizeazƒÉ »ôi skip URLs
                self._save_skip_urls()
            else:
                print("‚úÖ Nu am gƒÉsit issue-uri marcate gre»ôit ca complete")

            return fixes_applied

    def get_pending_partial_issues(self):
        """IMPROVED: GƒÉse»ôte TOATE issue-urile par»õiale »ôi le sorteazƒÉ corect"""
        pending_partials = []

        for item in self.state.get("downloaded_issues", []):
            url = item.get("url", "").rstrip('/')
            last_segment = item.get("last_successful_segment_end", 0)
            total_pages = item.get("total_pages")
            completed_at = item.get("completed_at", "")

            # Skip URL-urile complet descƒÉrcate
            if url in self.dynamic_skip_urls:
                continue

            # CONDI»öIE √éMBUNƒÇTƒÇ»öITƒÇ pentru issue-uri par»õiale
            is_partial = (
                last_segment > 0 and  # Are progres
                total_pages and total_pages > 0 and  # Are total_pages valid
                last_segment < total_pages and  # Nu e complet
                not completed_at  # Nu e marcat ca terminat
            )

            if is_partial:
                # CALCULEAZƒÇ procentul de completare pentru prioritizare
                completion_percent = (last_segment / total_pages) * 100
                item_with_priority = item.copy()
                item_with_priority['completion_percent'] = completion_percent
                item_with_priority['remaining_pages'] = total_pages - last_segment

                pending_partials.append(item_with_priority)
                print(f"üîÑ Issue par»õial gƒÉsit: {url}")
                print(f"   üìä Progres: {last_segment}/{total_pages} ({completion_percent:.1f}%)")
                print(f"   üéØ RƒÉm√¢n: {total_pages - last_segment} pagini")

        # SORTARE √éMBUNƒÇTƒÇ»öITƒÇ: prioritizeazƒÉ dupƒÉ completitudine (cel mai aproape de final primul)
        pending_partials.sort(key=lambda x: x['completion_percent'], reverse=True)

        print(f"\nüìã ORDINEA DE PROCESARE A ISSUE-URILOR PAR»öIALE:")
        for i, item in enumerate(pending_partials):
            url = item['url']
            percent = item['completion_percent']
            remaining = item['remaining_pages']
            print(f"   {i+1}. {url.split('/')[-1]}: {percent:.1f}% complet, {remaining} pagini rƒÉmase")

        return pending_partials

    def _normalize_downloaded_issues(self, raw):
        normalized = []
        for item in raw:
            if isinstance(item, str):
                normalized.append({
                    "url": item.rstrip('/'),
                    "title": "",
                    "subtitle": "",
                    "pages": 0,
                    "completed_at": "",
                    "last_successful_segment_end": 0,
                    "total_pages": None
                })
            elif isinstance(item, dict):
                normalized.append({
                    "url": item.get("url", "").rstrip('/'),
                    "title": item.get("title", ""),
                    "subtitle": item.get("subtitle", ""),
                    "pages": item.get("pages", 0),
                    "completed_at": item.get("completed_at", ""),
                    "last_successful_segment_end": item.get("last_successful_segment_end", 0),
                    "total_pages": item.get("total_pages")
                })
        return normalized

    def _load_state(self):
        """ULTRA SAFE: Nu »ôterge NICIODATƒÇ datele existente"""
        today = datetime.now().strftime("%Y-%m-%d")

        if os.path.exists(self.state_path):
            try:
                with open(self.state_path, "r", encoding="utf-8") as f:
                    loaded = json.load(f)
                    loaded = self._decode_unicode_escapes(loaded)

                # PƒÇSTREAZƒÇ TOATE issue-urile existente - ZERO »òTERS
                existing_issues = self._normalize_downloaded_issues(loaded.get("downloaded_issues", []))

                print(f"üìã √éNCƒÇRCAT {len(existing_issues)} issue-uri din state.json")

                # GƒÉse»ôte issue-urile par»õiale
                partial_issues = []
                for issue in existing_issues:
                    last_segment = issue.get("last_successful_segment_end", 0)
                    total_pages = issue.get("total_pages")
                    completed_at = issue.get("completed_at", "")

                    if (last_segment > 0 and not completed_at and total_pages and last_segment < total_pages):
                        partial_issues.append(issue)
                        print(f"üîÑ PAR»öIAL: {issue['url']} - {last_segment}/{total_pages} pagini")

                complete_count = len([i for i in existing_issues if i.get("completed_at")])

                # PƒÇSTREAZƒÇ TOT - doar actualizeazƒÉ data
                self.state = {
                    "date": today,
                    "count": loaded.get("count", complete_count),
                    "downloaded_issues": existing_issues,  # TOATE PƒÇSTRATE
                    "pages_downloaded": loaded.get("pages_downloaded", 0),
                    "recent_links": loaded.get("recent_links", []),
                    "daily_limit_hit": False,
                    "main_collection_completed": loaded.get("main_collection_completed", False),
                    "current_additional_collection_index": loaded.get("current_additional_collection_index", 0)
                }

                print(f"‚úÖ PƒÇSTRAT TOT: {complete_count} complete, {len(partial_issues)} par»õiale")

            except Exception as e:
                print(f"‚ùå JSON CORRUPT: {e}")
                print(f"üõ†Ô∏è RECUPEREZ din backup sau disk...")

                # √éncearcƒÉ backup
                backup_path = self.state_path + ".backup"
                if os.path.exists(backup_path):
                    print(f"üîÑ Restabilesc din backup...")
                    shutil.copy2(backup_path, self.state_path)
                    return self._load_state()  # Recursiv cu backup

                # Altfel √Æncepe gol dar SCANEAZƒÇ DISK-UL
                print(f"üîç SCANEZ DISK-UL pentru recuperare...")
                self.state = {
                    "date": today,
                    "count": 0,
                    "downloaded_issues": [],
                    "pages_downloaded": 0,
                    "recent_links": [],
                    "daily_limit_hit": False,
                    "main_collection_completed": False,
                    "current_additional_collection_index": 0
                }
        else:
            print(f"üìÑ Nu existƒÉ state.json")
            self.state = {
                "date": today,
                "count": 0,
                "downloaded_issues": [],
                "pages_downloaded": 0,
                "recent_links": [],
                "daily_limit_hit": False,
                "main_collection_completed": False,
                "current_additional_collection_index": 0
            }

        self._save_state()

    def _save_state_safe(self):
        """SAFE: SalveazƒÉ starea doar dacƒÉ existƒÉ modificƒÉri, pƒÉstreazƒÉ backup"""
        try:
            # CreeazƒÉ backup √Ænainte de salvare
            if os.path.exists(self.state_path):
                backup_path = self.state_path + ".backup"
                shutil.copy2(self.state_path, backup_path)

            with open(self.state_path, "w", encoding="utf-8") as f:
                json.dump(self.state, f, indent=2, ensure_ascii=False)

        except Exception as e:
            print(f"‚ö† Nu am putut salva state-ul: {e}")
            # √éncearcƒÉ sƒÉ restabileascƒÉ din backup
            backup_path = self.state_path + ".backup"
            if os.path.exists(backup_path):
                print(f"üîÑ √éncerc sƒÉ restabilesc din backup...")
                try:
                    shutil.copy2(backup_path, self.state_path)
                    print(f"‚úÖ State restabilit din backup")
                except:
                    print(f"‚ùå Nu am putut restabili din backup")

    def _save_state(self):
        """WRAPPER: Folose»ôte salvarea safe"""
        self._save_state_safe()

    def fix_existing_json(self):
        """Func»õie temporarƒÉ pentru a repara caracterele din JSON existent"""
        if os.path.exists(self.state_path):
            with open(self.state_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            data = self._decode_unicode_escapes(data)

            with open(self.state_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            print("‚úÖ JSON reparat cu caractere rom√¢ne»ôti")

    def remaining_quota(self):
        return max(0, DAILY_LIMIT - self.state.get("count", 0))

    def _update_partial_issue_progress(self, issue_url, last_successful_segment_end, total_pages=None, title=None, subtitle=None):
        """FIXED: Previne dublurile - verificƒÉ »ôi dupƒÉ title dacƒÉ URL-ul nu se potrive»ôte"""
        normalized = issue_url.rstrip('/')
        updated = False

        # STEP 1: CautƒÉ dupƒÉ URL exact
        for i, item in enumerate(self.state.setdefault("downloaded_issues", [])):
            if item["url"] == normalized:
                # ACTUALIZEAZƒÇ issue-ul existent
                if last_successful_segment_end > item.get("last_successful_segment_end", 0):
                    item["last_successful_segment_end"] = last_successful_segment_end

                if total_pages is not None and not item.get("total_pages"):
                    item["total_pages"] = total_pages

                if title and not item.get("title"):
                    item["title"] = title

                if subtitle and not item.get("subtitle"):
                    item["subtitle"] = subtitle

                # MutƒÉ la √Ænceput pentru prioritate
                updated_item = self.state["downloaded_issues"].pop(i)
                self.state["downloaded_issues"].insert(0, updated_item)
                updated = True
                print(f"üîÑ ACTUALIZAT progres pentru: {normalized} ‚Üí {last_successful_segment_end} pagini")
                break

        # STEP 2: DacƒÉ nu gƒÉse»ôti dupƒÉ URL, cautƒÉ dupƒÉ title (prevenire dubluri)
        if not updated and title:
            for i, item in enumerate(self.state["downloaded_issues"]):
                if item.get("title") == title and not item["url"].startswith("http"):
                    # GƒÇSIT dublu cu title ca URL - »ôterge-l!
                    print(f"üóëÔ∏è »òTERG DUBLU GRE»òIT: {item['url']} (era title √Æn loc de URL)")
                    self.state["downloaded_issues"].pop(i)
                    break

        # STEP 3: Doar dacƒÉ nu existƒÉ deloc, creeazƒÉ nou
        if not updated:
            # VALIDEAZƒÇ cƒÉ URL-ul e corect
            if not normalized.startswith("https://"):
                print(f"‚ùå URL INVALID: {normalized} - nu creez issue nou!")
                return

            new_issue = {
                "url": normalized,
                "title": title or "",
                "subtitle": subtitle or "",
                "pages": 0,
                "completed_at": "",
                "last_successful_segment_end": last_successful_segment_end,
                "total_pages": total_pages
            }
            self.state["downloaded_issues"].insert(0, new_issue)
            print(f"‚ûï ADƒÇUGAT issue nou √Æn progres: {normalized}")

        self._save_state_safe()
        print(f"üíæ Progres salvat SAFE: {normalized} - pagini {last_successful_segment_end}/{total_pages or '?'}")

    def mark_issue_done(self, issue_url, pages_count, title=None, subtitle=None, total_pages=None):
        """ULTRA SAFE: VerificƒÉri stricte √Ænainte de a marca ca terminat + DETECTARE MAGHIARƒÇ"""
        normalized = issue_url.rstrip('/')
        now_iso = datetime.now().isoformat(timespec="seconds")

        print(f"üîí VERIFICƒÇRI ULTRA SAFE pentru marcarea ca terminat: {normalized}")

        # VERIFICARE 0: DetecteazƒÉ posibila problemƒÉ cu maghiara
        if total_pages == 1 and pages_count == 1:
            print(f"üö® ALERTƒÇ CRITICƒÇ: total_pages=1 »ôi pages_count=1")
            print(f"üîç PosibilƒÉ problemƒÉ de detectare pentru interfa»õa maghiarƒÉ!")
            print(f"üõ°Ô∏è REFUZ sƒÉ marchez ca terminat - probabil e o eroare!")

            # √éncearcƒÉ sƒÉ re-detecteze numƒÉrul corect de pagini
            print(f"üîÑ √éncerc re-detectarea numƒÉrului total de pagini...")
            try:
                if self.driver and self.current_issue_url == normalized:
                    real_total = self.get_total_pages(max_attempts=3)
                    if real_total > 1:
                        print(f"‚úÖ RE-DETECTAT: {real_total} pagini √Æn loc de 1!")
                        # MarcheazƒÉ ca par»õial cu progresul real
                        self._update_partial_issue_progress(
                            normalized, pages_count, total_pages=real_total, title=title, subtitle=subtitle
                        )
                        return
            except:
                pass

            print(f"üõ°Ô∏è BLOCARE SAFETY: NU marchez issue-uri cu 1 paginƒÉ ca terminate!")
            return

        # VERIFICARE 2: pages_count trebuie sƒÉ fie aproape de total_pages
        completion_percentage = (pages_count / total_pages) * 100

        if completion_percentage < 95:  # Trebuie sƒÉ fie cel pu»õin 95% complet
            print(f"‚ùå BLOCARE SAFETY: Progres insuficient pentru {normalized}")
            print(f"üìä Progres: {pages_count}/{total_pages} ({completion_percentage:.1f}%)")
            print(f"üõ°Ô∏è Trebuie cel pu»õin 95% pentru a marca ca terminat!")
            print(f"üîÑ MarcheazƒÉ ca par»õial √Æn loc de terminat")

            # MarcheazƒÉ ca par»õial, NU ca terminat
            self._update_partial_issue_progress(
                normalized, pages_count, total_pages=total_pages, title=title, subtitle=subtitle
            )
            return

        # VERIFICARE 3: DetecteazƒÉ batch size suspicious
        if pages_count < 100 and total_pages > 500:
            print(f"‚ùå BLOCARE SAFETY: Progres suspect de mic pentru {normalized}")
            print(f"üìä {pages_count} pagini par sƒÉ fie doar primul batch din {total_pages}")
            print(f"üõ°Ô∏è Probabil s-a oprit prematur, NU marchez ca terminat")

            # MarcheazƒÉ ca par»õial
            self._update_partial_issue_progress(
                normalized, pages_count, total_pages=total_pages, title=title, subtitle=subtitle
            )
            return

        # VERIFICARE 4: VerificƒÉ dacƒÉ pages_count pare sƒÉ fie doar primul segment
        if total_pages >= 1000 and pages_count < 100:
            print(f"‚ùå BLOCARE SAFETY: {pages_count} pagini din {total_pages} pare primul segment")
            print(f"üõ°Ô∏è NU marchez issues mari ca terminate cu progres at√¢t de mic")

            # MarcheazƒÉ ca par»õial
            self._update_partial_issue_progress(
                normalized, pages_count, total_pages=total_pages, title=title, subtitle=subtitle
            )
            return

        # ===== TOATE VERIFICƒÇRILE AU TRECUT - SAFE SƒÇ MARCHEZ CA TERMINAT =====

        print(f"‚úÖ TOATE VERIFICƒÇRILE ULTRA SAFE trecute pentru {normalized}")
        print(f"üìä Progres: {pages_count}/{total_pages} ({completion_percentage:.1f}%)")
        print(f"üéØ Marchez ca TERMINAT")

        # ContinuƒÉ cu logica originalƒÉ de marcare ca terminat...
        existing = None
        existing_index = -1

        # CƒÇUTARE √éMBUNƒÇTƒÇ»öITƒÇ: √ÆncearcƒÉ mai multe variante de URL
        search_variants = [
            normalized,
            normalized + '/',
            normalized.replace('https://', 'http://'),
            normalized.replace('http://', 'https://')
        ]

        for i, item in enumerate(self.state.setdefault("downloaded_issues", [])):
            item_url = item.get("url", "").rstrip('/')
            if item_url in search_variants or normalized in [item_url, item_url + '/']:
                existing = item
                existing_index = i
                print(f"üîç GƒÇSIT issue existent la index {i}: {item_url}")
                break

        # CreeazƒÉ record-ul de completare
        completion_data = {
            "pages": pages_count,
            "completed_at": now_iso,
            "last_successful_segment_end": pages_count,
            "total_pages": total_pages  # SETEAZƒÇ √éNTOTDEAUNA!
        }

        # AdaugƒÉ title/subtitle doar dacƒÉ nu existƒÉ sau sunt goale
        if title:
            completion_data["title"] = title
        if subtitle:
            completion_data["subtitle"] = subtitle

        if existing:
            # √éMBOGƒÇ»öE»òTE issue-ul existent
            for key, value in completion_data.items():
                if key in ["title", "subtitle"]:
                    if not existing.get(key):
                        existing[key] = value
                else:
                    existing[key] = value

            # SCOATE din pozi»õia curentƒÉ
            updated_issue = self.state["downloaded_issues"].pop(existing_index)
            print(f"‚úÖ ACTUALIZAT »ôi SCOS din pozi»õia {existing_index}: {normalized}")
        else:
            # CreeazƒÉ issue nou complet
            updated_issue = {
                "url": normalized,
                "title": title or "",
                "subtitle": subtitle or "",
                **completion_data
            }
            print(f"‚ûï CREAT issue nou: {normalized}")

        # INSEREAZƒÇ √éN POZI»öIA CRONOLOGICƒÇ CORECTƒÇ
        # GƒÉse»ôte primul issue cu completed_at mai vechi dec√¢t cel curent
        insert_position = 0

        # Sari peste issue-urile par»õiale (care sunt mereu primele)
        while (insert_position < len(self.state["downloaded_issues"]) and
               not self.state["downloaded_issues"][insert_position].get("completed_at")):
            insert_position += 1

        # GƒÉse»ôte pozi»õia corectƒÉ √Æntre issue-urile complete (sortate cronologic descendent)
        while insert_position < len(self.state["downloaded_issues"]):
            other_completed_at = self.state["downloaded_issues"][insert_position].get("completed_at", "")
            if other_completed_at and other_completed_at < now_iso:
                break
            insert_position += 1

        # InsereazƒÉ √Æn pozi»õia cronologicƒÉ corectƒÉ
        self.state["downloaded_issues"].insert(insert_position, updated_issue)
        print(f"üìÖ INSERAT √Æn pozi»õia CRONOLOGICƒÇ {insert_position} (dupƒÉ issue-urile par»õiale »ôi √Æn ordine de completed_at)")

        # ActualizeazƒÉ contoarele SAFE
        completed_count = len([i for i in self.state["downloaded_issues"] if i.get("completed_at")])
        self.state["count"] = max(self.state.get("count", 0), completed_count)

        # ActualizeazƒÉ pages_downloaded SAFE
        current_pages = self.state.get("pages_downloaded", 0)
        self.state["pages_downloaded"] = current_pages + pages_count

        # AdaugƒÉ √Æn recent_links (pƒÉstreazƒÉ max 10)
        recent_entry = {
            "url": normalized,
            "title": (existing and existing.get("title")) or title or "",
            "subtitle": (existing and existing.get("subtitle")) or subtitle or "",
            "pages": pages_count,
            "timestamp": now_iso
        }
        recent_links = self.state.setdefault("recent_links", [])
        recent_links.insert(0, recent_entry)
        self.state["recent_links"] = recent_links[:10]

        # ReseteazƒÉ flag-ul de limitƒÉ
        self.state["daily_limit_hit"] = False

        # AdaugƒÉ √Æn skip URLs
        self.dynamic_skip_urls.add(normalized)

        self._save_state_safe()
        self._save_skip_urls()

        print(f"‚úÖ Issue marcat ca terminat cu SORTARE CRONOLOGICƒÇ CORECTƒÇ: {normalized}")
        print(f"üìä Detalii: {pages_count} pagini, total_pages: {total_pages}")
        print(f"üìä Total complet: {self.state['count']}, Total pagini: {self.state['pages_downloaded']}")
        print(f"üìÖ Plasat √Æn pozi»õia cronologicƒÉ {insert_position} din {len(self.state['downloaded_issues'])}")

    def mark_collection_complete(self, collection_url):
        """MarcheazƒÉ o colec»õie ca fiind complet procesatƒÉ √Æn skip_urls.json"""
        try:
            normalized_collection = collection_url.rstrip('/')

            # AdaugƒÉ √Æn dynamic skip URLs
            self.dynamic_skip_urls.add(normalized_collection)

            # SalveazƒÉ √Æn skip_urls.json cu un marker special pentru colec»õii
            skip_data = {}
            if os.path.exists(self.skip_urls_path):
                with open(self.skip_urls_path, "r", encoding="utf-8") as f:
                    skip_data = json.load(f)

            completed_collections = skip_data.get("completed_collections", [])
            if normalized_collection not in completed_collections:
                completed_collections.append(normalized_collection)
                skip_data["completed_collections"] = completed_collections
                skip_data["last_updated"] = datetime.now().isoformat()

                with open(self.skip_urls_path, "w", encoding="utf-8") as f:
                    json.dump(skip_data, f, indent=2, ensure_ascii=False)

                print(f"‚úÖ Colec»õia marcatƒÉ ca completƒÉ: {normalized_collection}")
        except Exception as e:
            print(f"‚ö† Eroare la marcarea colec»õiei complete: {e}")

    def setup_chrome_driver(self):
        try:
            print("üîß Ini»õializare WebDriver ‚Äì √Æncerc conectare la instan»õa Chrome existentƒÉ via remote debugging...")
            chrome_options = Options()
            chrome_options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")
            prefs = {
                "download.default_directory": os.path.abspath(self.download_dir),
                "download.prompt_for_download": False,
                "download.directory_upgrade": True,
                "safebrowsing.enabled": True,
            }
            chrome_options.add_experimental_option("prefs", prefs)
            try:
                self.driver = webdriver.Chrome(options=chrome_options)
                self.wait = WebDriverWait(self.driver, self.timeout)
                self.attached_existing = True
                print("‚úÖ Conectat la instan»õa Chrome existentƒÉ cu succes.")
                return True
            except WebDriverException as e:
                print(f"‚ö† Conexiune la Chrome existent e»ôuat ({e}); pornesc o instan»õƒÉ nouƒÉ.")
                chrome_options = Options()
                chrome_options.add_experimental_option("prefs", prefs)
                chrome_options.add_argument("--no-sandbox")
                chrome_options.add_argument("--disable-dev-shm-usage")
                chrome_options.add_argument("--disable-gpu")
                chrome_options.add_argument("--window-size=1920,1080")
                chrome_options.add_argument("--incognito")
                self.driver = webdriver.Chrome(options=chrome_options)
                self.wait = WebDriverWait(self.driver, self.timeout)
                self.attached_existing = False
                print("‚úÖ Chrome nou pornit cu succes.")
                return True
        except WebDriverException as e:
            print(f"‚ùå Eroare la ini»õializarea WebDriver-ului: {e}")
            return False

    def navigate_to_page(self, url):
        try:
            # VERIFICƒÇ √éNT√ÇI DACƒÇ BROWSER-UL MAI EXISTƒÇ
            try:
                _ = self.driver.current_url
            except:
                print("‚ö† Browser √Ænchis, √Æncerc reconectare...")
                if not self.setup_chrome_driver():
                    print("‚ùå Nu pot reconecta browser-ul")
                    return False

            print(f"üåê Navighez cƒÉtre: {url}")
            self.driver.get(url)
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'body')))
            print("‚úÖ Pagina √ÆncƒÉrcatƒÉ.")
            return True
        except Exception as e:
            print(f"‚ùå Eroare la navigare sau √ÆncƒÉrcare: {e}")
            # √éNCEARCƒÇ O RECONECTARE CA ULTIM RESORT
            try:
                print("üîÑ √éncerc reconectare de urgen»õƒÉ...")
                if self.setup_chrome_driver():
                    self.driver.get(url)
                    self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'body')))
                    print("‚úÖ Reconectat »ôi navigat cu succes!")
                    return True
            except:
                pass
            return False

    def get_issue_metadata(self):
        title = ""
        subtitle = ""
        try:
            breadcrumb = self.driver.find_element(By.CSS_SELECTOR, "li.breadcrumb-item.active")
            try:
                sub_elem = breadcrumb.find_element(By.CSS_SELECTOR, "#pdfview-pdfcontents span")
                subtitle = sub_elem.text.strip()
            except Exception:
                subtitle = ""
            raw = breadcrumb.text.strip()
            if subtitle and subtitle in raw:
                title = raw.replace(subtitle, "").strip()
            else:
                title = raw
        except Exception:
            pass
        return title, subtitle

    def get_total_pages(self, max_attempts=5, delay_between=1.0):
        """FIXED: DetecteazƒÉ corect numƒÉrul total de pagini INCLUSIV pentru limba maghiarƒÉ"""
        for attempt in range(1, max_attempts + 1):
            try:
                # Metoda 1: CautƒÉ pattern-uri specifice pentru maghiarƒÉ »òI alte limbi
                page_patterns = [
                    r'(\d+)\s*/\s*(\d+)',           # "1 / 146" (rom√¢nƒÉ/englezƒÉ)
                    r'/\s*(\d+)',                   # "/ 146" (maghiarƒÉ - PRINCIPAL)
                    r'of\s+(\d+)',                  # "of 146" (englezƒÉ)
                    r'din\s+(\d+)',                 # "din 146" (rom√¢nƒÉ)
                    r'(\d+)\s*oldal',               # "146 oldal" (maghiarƒÉ)
                    r'√∂sszesen\s+(\d+)',            # "√∂sszesen 146" (maghiarƒÉ)
                ]

                # PRIORITATE: CautƒÉ mai √Ænt√¢i √Æn clasa CSS specificƒÉ maghiarƒÉ
                try:
                    # Pattern specific pentru interfa»õa maghiarƒÉ din screenshot
                    adornment_divs = self.driver.find_elements(By.CSS_SELECTOR,
                        'div.MuiInputAdornment-root.MuiInputAdornment-positionEnd')

                    for div in adornment_divs:
                        text = div.text.strip()
                        print(f"üîç Verific div adornment: '{text}'")

                        # CautƒÉ pattern-ul "/ 146"
                        match = re.search(r'/\s*(\d+)', text)
                        if match:
                            total = int(match.group(1))
                            print(f"‚úÖ TOTAL PAGINI detectat din adornment maghiar: {total}")
                            return total
                except Exception as e:
                    print(f"‚ö† Eroare √Æn detectare maghiarƒÉ: {e}")

                # Metoda 2: CautƒÉ √Æn toate elementele cu text (backup)
                all_texts = self.driver.find_elements(By.XPATH,
                    "//*[contains(text(), '/') or contains(text(), 'of') or contains(text(), 'din') or contains(text(), 'oldal')]")

                for el in all_texts:
                    text = el.text.strip()
                    print(f"üîç Verific text element: '{text}'")

                    for pattern in page_patterns:
                        matches = re.findall(pattern, text)
                        if matches:
                            if pattern == page_patterns[0]:  # "numƒÉr / total"
                                current, total = matches[0]
                                total = int(total)
                                print(f"‚úÖ TOTAL PAGINI detectat din '{text}': {total} (curent: {current})")
                                return total
                            else:  # "/ total", "of total", etc.
                                total = int(matches[0])
                                print(f"‚úÖ TOTAL PAGINI detectat din '{text}': {total}")
                                return total

                # Metoda 3: JavaScript mai robust pentru maghiarƒÉ
                js_result = self.driver.execute_script(r"""
                    const patterns = [
                        /\/\s*(\d+)/g,                    // / 146 (PRIORITATE pentru maghiarƒÉ)
                        /(\d+)\s*\/\s*(\d+)/g,           // 1 / 146
                        /of\s+(\d+)/g,                   // of 146
                        /din\s+(\d+)/g,                  // din 146
                        /(\d+)\s*oldal/g,                // 146 oldal
                        /√∂sszesen\s+(\d+)/g              // √∂sszesen 146
                    ];

                    // CautƒÉ √Æn toate nodurile text
                    const walker = document.createTreeWalker(document.body, NodeFilter.SHOW_TEXT);
                    const results = [];

                    while(walker.nextNode()) {
                        const text = walker.currentNode.nodeValue;
                        if (!text || text.trim().length < 2) continue;

                        for (let pattern of patterns) {
                            const matches = [...text.matchAll(pattern)];
                            if (matches.length > 0) {
                                const match = matches[0];
                                let total, current = 0;

                                if (match.length === 3) {  // "numƒÉr / total"
                                    current = parseInt(match[1]);
                                    total = parseInt(match[2]);
                                } else {  // "/ total"
                                    total = parseInt(match[1]);
                                }

                                if (total && total > 0) {
                                    results.push({
                                        text: text.trim(),
                                        total: total,
                                        current: current,
                                        pattern: pattern.source
                                    });
                                }
                            }
                        }
                    }

                    // SorteazƒÉ dupƒÉ total (cel mai mare primul) »ôi returneazƒÉ primul
                    results.sort((a, b) => b.total - a.total);
                    return results.length > 0 ? results[0] : null;
                """)

                if js_result:
                    total = js_result['total']
                    current = js_result.get('current', 0)
                    text = js_result['text']
                    pattern = js_result['pattern']
                    print(f"‚úÖ TOTAL PAGINI detectat prin JS: {total} din '{text}' (pattern: {pattern})")
                    return total

                print(f"‚ö† ({attempt}) Nu am gƒÉsit √ÆncƒÉ numƒÉrul total de pagini, re√Æncerc √Æn {delay_between}s...")
                time.sleep(delay_between)

            except Exception as e:
                print(f"‚ö† ({attempt}) Eroare √Æn get_total_pages: {e}")
                time.sleep(delay_between)

        print("‚ùå Nu s-a reu»ôit extragerea numƒÉrului total de pagini dupƒÉ multiple √ÆncercƒÉri.")
        return 0

    def debug_page_detection(self):
        """Func»õie de debugging pentru a vedea ce detecteazƒÉ √Æn interfa»õa maghiarƒÉ"""
        try:
            print("üîç DEBUG: Analizez interfa»õa pentru detectarea paginilor...")

            # 1. VerificƒÉ adornment-urile
            adornments = self.driver.find_elements(By.CSS_SELECTOR,
                'div.MuiInputAdornment-root')
            print(f"üìä GƒÉsite {len(adornments)} adornment-uri:")
            for i, div in enumerate(adornments):
                text = div.text.strip()
                html = div.get_attribute('outerHTML')[:100]
                print(f"   {i+1}. Text: '{text}' | HTML: {html}...")

            # 2. CautƒÉ toate elementele cu "/"
            slash_elements = self.driver.find_elements(By.XPATH, "//*[contains(text(), '/')]")
            print(f"üìä GƒÉsite {len(slash_elements)} elemente cu '/':")
            for i, el in enumerate(slash_elements[:5]):  # Primele 5
                text = el.text.strip()
                tag = el.tag_name
                print(f"   {i+1}. <{tag}>: '{text}'")

            # 3. JavaScript debug
            js_result = self.driver.execute_script("""
                const allText = document.body.innerText;
                const lines = allText.split('\\n');
                const relevantLines = lines.filter(line =>
                    line.includes('/') ||
                    line.includes('oldal') ||
                    line.includes('√∂sszesen')
                );
                return relevantLines.slice(0, 10);
            """)

            print(f"üìä Linii relevante din JS:")
            for i, line in enumerate(js_result):
                print(f"   {i+1}. '{line.strip()}'")

        except Exception as e:
            print(f"‚ùå Eroare √Æn debug: {e}")

    def open_save_popup(self):
        try:
            try:
                self.wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR, 'div.MuiDialog-container')))
            except Exception:
                self.driver.switch_to.active_element.send_keys(Keys.ESCAPE)
                time.sleep(0.5)
            svg = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'svg[data-testid="SaveAltIcon"]')))
            button = svg
            if svg.tag_name.lower() == "svg":
                try:
                    button = svg.find_element(By.XPATH, "./ancestor::button")
                except Exception:
                    pass
            for attempt in range(1, 4):
                try:
                    button.click()
                    return True
                except ElementClickInterceptedException:
                    print(f"‚ö† click interceptat (√Æncercarea {attempt}), trimit ESC »ôi reiau...")
                    self.driver.switch_to.active_element.send_keys(Keys.ESCAPE)
                    time.sleep(1)
                    continue
            print("‚ùå Nu am reu»ôit sƒÉ dau click pe butonul de deschidere a popup-ului dupƒÉ retry-uri.")
            return False
        except Exception as e:
            print(f"‚ùå Nu am reu»ôit sƒÉ deschid popup-ul de salvare: {e}")
            return False

    def detect_save_button_multilingual(self):
        """
        DetecteazƒÉ butonul de salvare √Æn orice limbƒÉ suportatƒÉ de Arcanum
        """
        # Lista cu toate variantele de text pentru butonul de salvare
        save_button_texts = [
            "Salva»õi",    # Rom√¢nƒÉ
            "Save",       # EnglezƒÉ
            "Ment√©s",     # MaghiarƒÉ
            "Ulo≈æi≈•",     # SlovacƒÉ/CehƒÉ
            "Speichern",  # GermanƒÉ
            "Salvar",     # SpaniolƒÉ (dacƒÉ e cazul)
            "Sauvegarder" # FrancezƒÉ (dacƒÉ e cazul)
        ]

        for text in save_button_texts:
            try:
                save_btn = self.driver.find_element(By.XPATH,
                    f'//button[.//text()[contains(normalize-space(.), "{text}")]]')
                if save_btn and save_btn.is_enabled():
                    print(f"‚úÖ Buton de salvare gƒÉsit cu textul: '{text}'")
                    return save_btn
            except:
                continue

        # DacƒÉ nu gƒÉse»ôte cu textul, √ÆncearcƒÉ dupƒÉ clasele CSS (backup method)
        try:
            buttons = self.driver.find_elements(By.CSS_SELECTOR,
                'button[class*="MuiButton"][class*="Primary"]')
            for btn in buttons:
                text = btn.text.strip().lower()
                # VerificƒÉ dacƒÉ con»õine cuvinte cheie √Æn orice limbƒÉ
                if any(keyword in text for keyword in ['salv', 'save', 'ment', 'ulo≈æ', 'speich']):
                    print(f"‚úÖ Buton de salvare gƒÉsit prin CSS cu textul: '{btn.text}'")
                    return btn
        except:
            pass

        return None

    def fill_and_save_range(self, start, end):
        try:
            first_input = self.wait.until(EC.presence_of_element_located((By.ID, "first page")))
            print("‚è≥ A»ôtept 2s √Ænainte de a completa primul input...")
            time.sleep(2)
            first_input.send_keys(Keys.CONTROL + "a")
            first_input.send_keys(str(start))
            print(f"‚úèÔ∏è Am introdus primul numƒÉr: {start}")
            print("‚è≥ A»ôtept 2s √Ænainte de a completa al doilea input...")
            time.sleep(2)
            last_input = self.wait.until(EC.presence_of_element_located((By.ID, "last page")))
            last_input.send_keys(Keys.CONTROL + "a")
            last_input.send_keys(str(end))
            print(f"‚úèÔ∏è Am introdus al doilea numƒÉr: {end}")
            print("‚è≥ A»ôtept 2s √Ænainte de a apƒÉsa butonul de salvare...")
            time.sleep(2)

            # UtilizeazƒÉ func»õia multilingvƒÉ pentru a gƒÉsi butonul
            save_btn = self.detect_save_button_multilingual()

            if save_btn:
                save_btn.click()
                print(f"‚úÖ Segmentul {start}-{end} salvat.")
                return True
            else:
                print(f"‚ùå Nu am gƒÉsit butonul de salvare √Æn nicio limbƒÉ pentru segmentul {start}-{end}")
                return False

        except Exception as e:
            print(f"‚ùå Eroare la completarea/salvarea intervalului {start}-{end}: {e}")
            return False

    def check_daily_limit_in_all_windows(self, set_flag=True):
        # return False  # Add this line at the top to disable detection
        """VerificƒÉ mesajul de limitƒÉ zilnicƒÉ √Æn toate ferestrele deschise"""
        current_window = self.driver.current_window_handle
        limit_reached = False

        try:
            all_handles = self.driver.window_handles
            for handle in all_handles:
                try:
                    self.driver.switch_to.window(handle)
                    body_text = self.driver.find_element(By.TAG_NAME, "body").text

                    if ("Daily download limit reached" in body_text or
                        "Terms and conditions" in body_text):
                        print(f"‚ö† Limita zilnicƒÉ detectatƒÉ √Æn fereastra: {handle}")
                        limit_reached = True

                        if handle != current_window and len(all_handles) > 1:
                            print(f"üóô √énchid fereastra cu limita zilnicƒÉ: {handle}")
                            self.driver.close()
                        break

                except Exception as e:
                    continue

            try:
                if current_window in self.driver.window_handles:
                    self.driver.switch_to.window(current_window)
                elif self.driver.window_handles:
                    self.driver.switch_to.window(self.driver.window_handles[0])
            except Exception:
                pass

        except Exception as e:
            print(f"‚ö† Eroare la verificarea ferestrelor: {e}")

        if limit_reached and set_flag:
            self.state["daily_limit_hit"] = True
            self._save_state()

        return limit_reached

    def check_for_daily_limit_popup(self):
        """
        FIXED: VerificƒÉ dacƒÉ s-a deschis o filƒÉ nouƒÉ cu mesajul de limitƒÉ zilnicƒÉ dupƒÉ descƒÉrcare.
        EXCLUDERE EXPLICITƒÇ pentru about:blank »ôi alte file normale de browser
        """
        try:
            current_handles = set(self.driver.window_handles)

            print(f"üîç Verific {len(current_handles)} file deschise pentru limita zilnicƒÉ...")

            # VerificƒÉ toate filele deschise pentru mesajul de limitƒÉ
            for handle in current_handles:
                try:
                    self.driver.switch_to.window(handle)

                    # Ob»õine URL-ul pentru debugging
                    current_url = self.driver.current_url

                    # SKIP EXPLICIT pentru about:blank »ôi alte file normale de browser
                    if (current_url == "about:blank" or
                        current_url.startswith("chrome://") or
                        current_url.startswith("chrome-extension://") or
                        current_url.startswith("data:") or
                        not current_url or current_url.strip() == ""):
                        print(f"‚úÖ Skip filƒÉ normalƒÉ de browser: {current_url}")
                        continue

                    # Ob»õine textul complet al paginii
                    body_text = self.driver.find_element(By.TAG_NAME, "body").text.strip()

                    # Ob»õine sursa HTML pentru verificarea structurii
                    try:
                        page_source = self.driver.page_source
                    except:
                        page_source = ""

                    # DETECTOARE MULTIPLE pentru limita zilnicƒÉ
                    limit_detected = False
                    detection_reason = ""

                    # 1. NOUA PAGINƒÇ: "Vezi Termeni de utilizare"
                    if ("Vezi" in body_text and
                        ("Termeni de utilizare" in body_text or "conditii-de-utilizare" in current_url)):
                        limit_detected = True
                        detection_reason = "NOUƒÇ PAGINƒÇ - Vezi Termeni de utilizare"

                    # 2. VECHEA PAGINƒÇ: "Daily download limit reached"
                    elif "Daily download limit reached" in body_text:
                        limit_detected = True
                        detection_reason = "VECHE PAGINƒÇ - Daily download limit reached"

                    # 3. DETECTARE PRIN URL: dacƒÉ URL-ul con»õine "conditii-de-utilizare"
                    elif "conditii-de-utilizare" in current_url:
                        limit_detected = True
                        detection_reason = "URL DETECTARE - conditii-de-utilizare"

                    # 4. DETECTARE PRIN LINK: cautƒÉ linkul specific
                    elif "www.arcanum.com/ro/adt/conditii-de-utilizare" in body_text:
                        limit_detected = True
                        detection_reason = "LINK DETECTARE - arcanum.com conditii"

                    # 4. DETECTARE PRIN LINK: cautƒÉ linkul specific
                    elif "www.arcanum.com/en/adt/terms-and-conditions" in body_text:
                        limit_detected = True
                        detection_reason = "LINK DETECTARE - arcanum.com conditii"

                    # 4. DETECTARE PRIN LINK: cautƒÉ linkul specific
                    elif "www.arcanum.com/hu/adt/felhasznalasi-feltetelek" in body_text:
                        limit_detected = True
                        detection_reason = "LINK DETECTARE - arcanum.com conditii"

                    # 5. **NOUƒÇ DETECTARE**: VerificƒÉ structura HTML normalƒÉ
                    elif page_source and not self._has_normal_html_structure(page_source):
                        # VerificƒÉ dacƒÉ e o paginƒÉ anormalƒÉ (fƒÉrƒÉ structura HTML standard)
                        # »ôi dacƒÉ con»õinutul e suspect de mic sau con»õine cuvinte cheie
                        # DOAR dacƒÉ nu e about:blank (deja verificat mai sus)
                        if (len(body_text.strip()) < 500 and
                            (any(keyword in body_text.lower() for keyword in
                                ['limit', 'vezi', 'termeni', 'utilizare', 'download', 'reached', 'Download-Limit']) or
                             len(body_text.strip()) < 100)):
                            limit_detected = True
                            detection_reason = "STRUCTURƒÇ HTML ANORMALƒÇ - probabil paginƒÉ de limitƒÉ"

                    # 6. DETECTARE GENERALƒÇ: paginƒÉ suspicioasƒÉ cu pu»õin con»õinut »ôi "Vezi"
                    elif (len(body_text.strip()) < 200 and
                          "Vezi" in body_text and
                          len(body_text.split()) < 20):
                        limit_detected = True
                        detection_reason = "DETECTARE GENERALƒÇ - paginƒÉ suspicioasƒÉ cu 'Vezi'"

                    # DEBUGGING: Afi»ôeazƒÉ con»õinutul suspicios
                    if (self._is_suspicious_page(body_text, current_url, page_source)):
                        html_structure_ok = self._has_normal_html_structure(page_source)
                        print(f"üîç FILƒÇ SUSPICIOASƒÇ {handle}:")
                        print(f"   üìÑ URL: {current_url}")
                        print(f"   üìù Con»õinut ({len(body_text)} chars): '{body_text[:200]}{'...' if len(body_text) > 200 else ''}'")
                        print(f"   üèóÔ∏è StructurƒÉ HTML normalƒÉ: {html_structure_ok}")
                        print(f"   üéØ Detectat limit: {limit_detected} ({detection_reason})")

                    # AC»öIUNE: DacƒÉ limita a fost detectatƒÉ
                    if limit_detected:
                        print(f"üõë LIMITƒÇ ZILNICƒÇ DETECTATƒÇ √Æn filƒÉ: {handle}")
                        print(f"üîç MOTIV: {detection_reason}")
                        print(f"üìÑ URL complet: {current_url}")
                        print(f"üìù Con»õinut complet filƒÉ:")
                        print(f"   '{body_text}'")
                        print(f"üèóÔ∏è StructurƒÉ HTML: {self._has_normal_html_structure(page_source)}")

                        # √énchide fila cu limita (dar doar dacƒÉ nu e singura filƒÉ)
                        if len(current_handles) > 1:
                            print(f"üóô √énchid fila cu limita: {handle}")
                            self.driver.close()

                            # Revine la prima filƒÉ disponibilƒÉ
                            if self.driver.window_handles:
                                self.driver.switch_to.window(self.driver.window_handles[0])
                                print(f"‚Ü©Ô∏è Revin la fila principalƒÉ")
                        else:
                            print(f"‚ö† Nu √Ænchid fila - este singura rƒÉmasƒÉ")

                        # SeteazƒÉ flag-ul »ôi opre»ôte procesarea
                        self.state["daily_limit_hit"] = True
                        self._save_state()
                        print(f"üõë Flag daily_limit_hit setat √Æn state.json")

                        return True

                except Exception as e:
                    print(f"‚ö† Eroare la verificarea filei {handle}: {e}")
                    continue

            print(f"‚úÖ Nu am detectat limita zilnicƒÉ √Æn {len(current_handles)} file")
            return False

        except Exception as e:
            print(f"‚ùå Eroare fatalƒÉ √Æn verificarea popup-ului de limitƒÉ: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _has_normal_html_structure(self, page_source):
        """
        FIXED: VerificƒÉ dacƒÉ pagina are structura HTML normalƒÉ specificƒÉ site-ului Arcanum.
        IMPORTANT: Chrome page_source nu √Æntotdeauna include DOCTYPE, deci nu ne bazƒÉm pe el!
        """
        if not page_source:
            return False

        # NormalizeazƒÉ spa»õiile »ôi new lines pentru verificare
        normalized_source = ' '.join(page_source.strip().split())
        normalized_start = normalized_source[:500].lower()

        # INDICATORI POZITIVI pentru pagini normale Arcanum
        normal_indicators = [
            'html lang="ro"',                    # Toate paginile Arcanum au asta
            '<title>',                           # Toate au titlu
            '<head>',                           # Toate au head
            'ziarele arcanum',                  # √én titlu
            'meta charset="utf-8"',             # Meta charset standard
            'meta name="viewport"'              # Meta viewport standard
        ]

        # INDICATORI NEGATIVI pentru pagini de limitƒÉ/eroare
        limit_indicators = [
            'vezi',                             # "Vezi Termeni de utilizare"
            'conditii-de-utilizare',            # URL sau link cƒÉtre condi»õii
            'daily download limit',             # Vechiul mesaj
            'terms and conditions'              # Versiunea englezƒÉ
        ]

        # ContorizeazƒÉ indicatorii pozitivi
        positive_count = sum(1 for indicator in normal_indicators
                            if indicator in normalized_start)

        # ContorizeazƒÉ indicatorii negativi
        negative_count = sum(1 for indicator in limit_indicators
                            if indicator in normalized_start)

        # VerificƒÉ dimensiunea - paginile de limitƒÉ sunt foarte mici
        is_too_small = len(normalized_source) < 300

        # LOGICA DE DECIZIE:
        # 1. DacƒÉ are indicatori negativi »òI e micƒÉ ‚Üí paginƒÉ de limitƒÉ
        if negative_count > 0 and is_too_small:
            print(f"üö® PAGINƒÇ DE LIMITƒÇ detectatƒÉ:")
            print(f"   Indicatori negativi: {negative_count}")
            print(f"   Dimensiune: {len(normalized_source)} chars")
            print(f"   Con»õinut: '{normalized_source[:200]}'")
            return False

        # 2. DacƒÉ are suficien»õi indicatori pozitivi ‚Üí paginƒÉ normalƒÉ
        if positive_count >= 4:  # Cel pu»õin 4 din 6 indicatori pozitivi
            return True

        # 3. DacƒÉ e foarte micƒÉ »ôi fƒÉrƒÉ indicatori pozitivi ‚Üí suspicioasƒÉ
        if is_too_small and positive_count < 2:
            print(f"üîç PAGINƒÇ SUSPICIOASƒÇ (prea micƒÉ »ôi fƒÉrƒÉ indicatori):")
            print(f"   Indicatori pozitivi: {positive_count}/6")
            print(f"   Dimensiune: {len(normalized_source)} chars")
            print(f"   Con»õinut: '{normalized_source[:200]}'")
            return False

        # 4. √én toate celelalte cazuri ‚Üí considerƒÉ normalƒÉ
        return True

    def _is_suspicious_page(self, body_text, url, page_source):
        """
        FIXED: Helper mai inteligent pentru a determina dacƒÉ o paginƒÉ meritƒÉ debugging
        EXCLUDERE EXPLICITƒÇ pentru about:blank »ôi alte file normale de browser
        """

        # EXCLUDERE EXPLICITƒÇ pentru about:blank »ôi alte file normale de browser
        if url == "about:blank" or "about:blank" in url:
            return False  # Nu e suspicioasƒÉ - e paginƒÉ normalƒÉ de browser

        # Exclude »ôi alte URL-uri normale de Chrome
        if url.startswith("chrome://") or url.startswith("chrome-extension://"):
            return False

        # Exclude URL-urile goale sau None
        if not url or url.strip() == "":
            return False

        # Indicatori clari de pagini problematice
        clear_limit_signs = [
            "Vezi" in body_text and len(body_text) < 200,
            "conditii" in body_text.lower(),
            "limit" in body_text.lower() and len(body_text) < 500,
            "daily download" in body_text.lower()
        ]

        # Pagini foarte mici sunt √Æntotdeauna suspicioase DOAR dacƒÉ nu sunt about:blank
        too_small = len(body_text.strip()) < 100

        # NU detecta ca suspicioase paginile normale mari
        is_normal_arcanum = (
            len(body_text) > 500 and
            "Analele" in body_text and
            ("UniversitƒÉ»õii" in body_text or "MatematicƒÉ" in body_text)
        )

        if is_normal_arcanum:
            return False  # Nu e suspicioasƒÉ - e paginƒÉ normalƒÉ Arcanum

        return any(clear_limit_signs) or too_small

    def save_page_range(self, start, end, retries=1):
            """FIXED: VerificƒÉ URL-ul √Ænainte de fiecare descƒÉrcare + verificƒÉ limita zilnicƒÉ"""
            for attempt in range(1, retries + 2):
                print(f"üîÑ √éncep segmentul {start}-{end}, √Æncercarea {attempt}")

                # VERIFICARE CRITICƒÇ: Suntem pe documentul corect?
                try:
                    current_url = self.driver.current_url
                    if self.current_issue_url not in current_url:
                        print(f"üö® EROARE: Browser-ul a navigat la URL gre»ôit!")
                        print(f"   A»ôteptat: {self.current_issue_url}")
                        print(f"   Actual: {current_url}")
                        print(f"üîÑ Renavigez la documentul corect...")

                        if not self.navigate_to_page(self.current_issue_url):
                            print(f"‚ùå Nu pot renaviga la {self.current_issue_url}")
                            return False

                        time.sleep(3)  # A»ôteaptƒÉ √ÆncƒÉrcarea completƒÉ
                        print(f"‚úÖ Renavigat cu succes la documentul corect")
                except Exception as e:
                    print(f"‚ö† Eroare la verificarea URL-ului: {e}")

                if not self.open_save_popup():
                    print(f"‚ö† E»ôec la deschiderea popup-ului pentru {start}-{end}")
                    time.sleep(1)
                    continue

                success = self.fill_and_save_range(start, end)
                if success:
                    print("‚è≥ A»ôtept 5s pentru finalizarea descƒÉrcƒÉrii segmentului...")
                    time.sleep(5)

                    # VERIFICƒÇ LIMITA ZILNICƒÇ IMEDIAT DUPƒÇ DESCƒÇRCARE
                    if self.check_for_daily_limit_popup():
                        print(f"üõë OPRIRE INSTANT - LimitƒÉ zilnicƒÉ detectatƒÉ dupƒÉ segmentul {start}-{end}")
                        return False

                    print(f"‚úÖ Segmentul {start}-{end} descƒÉrcat cu succes")
                    return True
                else:
                    print(f"‚ö† Retry pentru segmentul {start}-{end}")
                    time.sleep(1)
            print(f"‚ùå Renun»õ la segmentul {start}-{end} dupƒÉ {retries+1} √ÆncercƒÉri.")
            return False

    def save_all_pages_in_batches(self, resume_from=1):
        """FIXED: RefƒÉ segmentele incomplete √Æn loc sƒÉ continue din mijloc"""
        total = self.get_total_pages()
        if total <= 0:
            print("‚ö† Nu am ob»õinut numƒÉrul total de pagini; nu pot continua.")
            return 0, False

        print(f"üéØ TOTAL PAGINI DETECTAT: {total}")

        bs = self.batch_size  # 50

        # PASUL 1: CalculeazƒÉ segmentele standard
        all_segments = []

        # Primul segment: 1 p√¢nƒÉ la bs-1 (1-49)
        first_end = min(bs - 1, total)
        if first_end >= 1:
            all_segments.append((1, first_end))

        # Segmentele urmƒÉtoare: bs, bs*2-1, etc. (50-99, 100-149, etc.)
        current = bs
        while current <= total:
            end = min(current + bs - 1, total)
            all_segments.append((current, end))
            current += bs

        print(f"üìä SEGMENTE STANDARD CALCULATE: {len(all_segments)}")
        for i, (start, end) in enumerate(all_segments):
            print(f"   {i+1}. Segment {start}-{end}")

        # PASUL 2: VerificƒÉ ce segmente sunt COMPLET descƒÉrcate pe disk
        completed_segments = []

        for i, (seg_start, seg_end) in enumerate(all_segments):
            # VerificƒÉ dacƒÉ existƒÉ un fi»ôier care acoperƒÉ COMPLET segmentul
            segments_on_disk = self.get_all_pdf_segments_for_issue(self.current_issue_url)

            segment_complete = False
            for disk_seg in segments_on_disk:
                disk_start = disk_seg['start']
                disk_end = disk_seg['end']

                # VerificƒÉ dacƒÉ segmentul de pe disk acoperƒÉ COMPLET segmentul standard
                if disk_start <= seg_start and disk_end >= seg_end:
                    segment_complete = True
                    print(f"‚úÖ Segment {i+1} ({seg_start}-{seg_end}) COMPLET pe disk: {disk_seg['filename']}")
                    break

            if segment_complete:
                completed_segments.append(i)
            else:
                # VerificƒÉ dacƒÉ existƒÉ fi»ôiere par»õiale pentru acest segment
                partial_files = [seg for seg in segments_on_disk
                               if seg['start'] >= seg_start and seg['end'] <= seg_end]
                if partial_files:
                    print(f"‚ö† Segment {i+1} ({seg_start}-{seg_end}) PAR»öIAL pe disk:")
                    for pf in partial_files:
                        print(f"   üìÑ {pf['filename']} (pagini {pf['start']}-{pf['end']}) - VA FI REFƒÇCUT")
                else:
                    print(f"üÜï Segment {i+1} ({seg_start}-{seg_end}) lipse»ôte complet")

        # PASUL 3: √éncepe cu primul segment incomplet
        start_segment_index = 0
        for i in range(len(all_segments)):
            if i not in completed_segments:
                start_segment_index = i
                break
        else:
            # Toate segmentele sunt complete
            print("‚úÖ Toate segmentele sunt complete pe disk!")
            return total, False

        print(f"üéØ √éNCEP cu segmentul {start_segment_index + 1} (primul incomplet)")

        # PASUL 4: ProceseazƒÉ segmentele √Æncep√¢nd cu primul incomplet
        segments_to_process = all_segments[start_segment_index:]

        print(f"üéØ PROCESEZ {len(segments_to_process)} segmente √Æncep√¢nd cu segmentul {start_segment_index + 1}")

        # PASUL 5: »òTERGE fi»ôierele par»õiale pentru segmentele care vor fi refƒÉcute
        for i, (seg_start, seg_end) in enumerate(segments_to_process):
            actual_index = start_segment_index + i
            if actual_index not in completed_segments:
                # »òterge fi»ôierele par»õiale pentru acest segment
                segments_on_disk = self.get_all_pdf_segments_for_issue(self.current_issue_url)
                for disk_seg in segments_on_disk:
                    if disk_seg['start'] >= seg_start and disk_seg['end'] <= seg_end:
                        try:
                            os.remove(disk_seg['path'])
                            print(f"üóëÔ∏è »òTERG fi»ôier par»õial: {disk_seg['filename']}")
                        except Exception as e:
                            print(f"‚ö† Nu am putut »ôterge {disk_seg['filename']}: {e}")

        last_successful_page = 0 if start_segment_index == 0 else all_segments[start_segment_index - 1][1]
        failed_segments = []
        consecutive_failures = 0
        MAX_CONSECUTIVE_FAILURES = 3

        for i, (start, end) in enumerate(segments_to_process):
            print(f"üì¶ Procesez segmentul COMPLET {start}-{end} ({i+1}/{len(segments_to_process)})")

            result = self.save_page_range(start, end, retries=3)

            if not result:
                if self.state.get("daily_limit_hit", False):
                    print(f"üõë OPRIRE - LimitƒÉ zilnicƒÉ atinsƒÉ la segmentul {start}-{end}")
                    return last_successful_page, True

                print(f"‚ùå SEGMENT E»òUAT: {start}-{end}")
                failed_segments.append((start, end))
                consecutive_failures += 1

                if consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
                    print(f"üö® PREA MULTE E»òECURI CONSECUTIVE ({consecutive_failures})")
                    print(f"üîÑ √éNCERC RECOVERY COMPLET...")

                    try:
                        if not self.setup_chrome_driver():
                            print(f"‚ùå Recovery e»ôuat - opresc procesarea")
                            break
                        if not self.navigate_to_page(self.current_issue_url):
                            print(f"‚ùå Nu pot renaviga dupƒÉ recovery")
                            break

                        consecutive_failures = 0
                        print(f"‚úÖ Recovery reu»ôit - continui cu urmƒÉtoarele segmente")
                        time.sleep(5)

                    except Exception as e:
                        print(f"‚ùå Eroare √Æn recovery: {e}")
                        break
                else:
                    print(f"üîÑ E»ôecuri consecutive: {consecutive_failures}/{MAX_CONSECUTIVE_FAILURES}")
                    print(f"‚è≥ Continui cu urmƒÉtorul segment dupƒÉ pauzƒÉ...")
                    time.sleep(2)
            else:
                # SEGMENT REU»òIT
                consecutive_failures = 0
                last_successful_page = end
                self._update_partial_issue_progress(self.current_issue_url, end, total_pages=total)
                print(f"‚úÖ Progres salvat: pagini p√¢nƒÉ la {end}")

            time.sleep(1)

        # RAPORTARE FINALƒÇ
        successful_segments = len(segments_to_process) - len(failed_segments)
        print(f"üìä PROGRES FINAL: {last_successful_page}/{total} pagini")
        print(f"üìä SEGMENTE: {successful_segments}/{len(segments_to_process)} reu»ôite")

        if failed_segments:
            print(f"üìä SEGMENTE E»òUATE: {len(failed_segments)}")
            for start, end in failed_segments:
                print(f"   ‚ùå {start}-{end}")

        completion_rate = (last_successful_page / total) * 100
        failure_rate = (len(failed_segments) / len(segments_to_process)) * 100 if segments_to_process else 0
        is_complete = completion_rate >= 98 and failure_rate < 5

        return last_successful_page, False

    def extract_issue_links_from_collection(self):
        """
        FIXED: Extrage toate linkurile de issue din colec»õie, inclusiv pentru limba ungarƒÉ
        Folose»ôte selector generic pentru a detecta orice limbƒÉ (/view/, /ro/view/, /en/view/, /hu/view/)
        """
        try:
            # A»ôteaptƒÉ ca lista sƒÉ se √Æncarce
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.list-group')))

            # SELECTOR GENERIC: orice link care con»õine '/view/' √Æn href
            anchors = self.driver.find_elements(By.CSS_SELECTOR, 'li.list-group-item a[href*="/view/"]')

            print(f"üîç Am gƒÉsit {len(anchors)} linkuri brute cu '/view/' √Æn colec»õie")

            links = []
            for a in anchors:
                href = a.get_attribute("href")
                if href and '/view/' in href:
                    # NormalizeazƒÉ URL-ul (eliminƒÉ parametrii »ôi slash-ul final)
                    normalized = href.split('?')[0].rstrip('/')
                    links.append(normalized)

            # EliminƒÉ dublurile pƒÉstr√¢nd ordinea
            unique = []
            seen = set()
            for l in links:
                if l not in seen:
                    seen.add(l)
                    unique.append(l)

            print(f"üîó Am gƒÉsit {len(unique)} linkuri UNICE de issue √Æn colec»õie")

            # DEBUGGING pentru colec»õiile problematice
            if len(unique) == 0:
                print(f"üîç DEBUG - Nu am gƒÉsit linkuri. Analizez structura paginii...")

                # VerificƒÉ dacƒÉ existƒÉ lista de grupuri
                try:
                    list_groups = self.driver.find_elements(By.CSS_SELECTOR, 'ul.list-group')
                    print(f"üîç Liste grup gƒÉsite: {len(list_groups)}")

                    list_items = self.driver.find_elements(By.CSS_SELECTOR, 'li.list-group-item')
                    print(f"üîç Elemente listƒÉ gƒÉsite: {len(list_items)}")

                    # VerificƒÉ toate linkurile din paginƒÉ
                    all_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*="/view/"]')
                    print(f"üîç TOATE linkurile cu '/view/' din paginƒÉ: {len(all_links)}")

                    for i, link in enumerate(all_links[:10]):  # Primele 10 pentru debugging
                        href = link.get_attribute("href")
                        text = link.text.strip()[:50]
                        print(f"   {i+1}. üîó {href}")
                        print(f"      üìù Text: '{text}'")

                    # VerificƒÉ structura HTML a primelor elemente
                    if list_items:
                        print(f"üîç Primul element listƒÉ HTML:")
                        print(f"   {list_items[0].get_attribute('outerHTML')[:200]}...")

                except Exception as debug_e:
                    print(f"‚ö† Eroare √Æn debugging: {debug_e}")
            else:
                # Afi»ôeazƒÉ primele c√¢teva linkuri gƒÉsite pentru verificare
                print(f"üìã Primele linkuri gƒÉsite:")
                for i, link in enumerate(unique[:5]):
                    # Extrage anul sau identificatorul din URL
                    parts = link.split('/')[-1].split('_')
                    identifier = parts[-1] if len(parts) > 1 else "N/A"
                    print(f"   {i+1}. üîó {identifier}: {link}")

                if len(unique) > 5:
                    print(f"   üìä ... »ôi √ÆncƒÉ {len(unique) - 5} linkuri")

            return unique

        except Exception as e:
            print(f"‚ùå Eroare la extragerea linkurilor din colec»õie: {e}")

            # Debugging suplimentar √Æn caz de eroare
            try:
                current_url = self.driver.current_url
                page_title = self.driver.title
                print(f"üîç URL curent: {current_url}")
                print(f"üîç Titlu paginƒÉ: {page_title}")

                # VerificƒÉ dacƒÉ pagina s-a √ÆncƒÉrcat corect
                body_text = self.driver.find_element(By.TAG_NAME, "body").text[:200]
                print(f"üîç √énceput con»õinut: '{body_text}...'")

            except Exception as debug_e:
                print(f"‚ö† Eroare √Æn debugging dupƒÉ eroare: {debug_e}")

            return []

    def extract_page_range_from_filename(self, filename):
        """Extrage range-ul de pagini din numele fi»ôierului pentru sortare corectƒÉ"""
        match = re.search(r'__pages(\d+)-(\d+)\.pdf', filename)
        if match:
            start_page = int(match.group(1))
            end_page = int(match.group(2))
            return (start_page, end_page)
        return (0, 0)

    def copy_and_combine_issue_pdfs(self, issue_url: str, issue_title: str):
        """
        FIXED: MUTƒÇ fi»ôierele √Æn folder »ôi le combinƒÉ (nu mai pƒÉstreazƒÉ pe D:)
        ADDED: Face backup √Æn g:Temporare √Ænainte de procesare
        """
        issue_id = issue_url.rstrip('/').split('/')[-1]
        folder_name = self._safe_folder_name(issue_title or issue_id)
        dest_dir = os.path.join(self.download_dir, folder_name)
        os.makedirs(dest_dir, exist_ok=True)

        # DIRECTORUL DE BACKUP
        backup_base_dir = r"g:\Temporare"
        backup_dir = os.path.join(backup_base_dir, folder_name)

        print(f"üìÅ Procesez PDF-urile pentru '{issue_title}' cu ID '{issue_id}'")

        # ‚è≥ A»òTEAPTƒÇ CA TOATE FI»òIERELE SƒÇ FIE COMPLET DESCƒÇRCATE
        print("‚è≥ A»ôtept 10 secunde ca toate fi»ôierele sƒÉ se termine de descƒÉrcat...")
        time.sleep(10)

        # PASUL 1: GƒÉse»ôte TOATE fi»ôierele pentru acest issue
        all_segments = self.get_all_pdf_segments_for_issue(issue_url)

        if not all_segments:
            print(f"‚ÑπÔ∏è Nu am gƒÉsit fi»ôiere PDF pentru '{issue_title}' cu ID '{issue_id}'.")
            return

        print(f"üîç Am gƒÉsit {len(all_segments)} fi»ôiere PDF pentru '{issue_id}':")
        for seg in all_segments:
            print(f"   üìÑ {seg['filename']} (pagini {seg['start']}-{seg['end']})")

        # PASUL 1.5: CREEAZƒÇ BACKUP-UL √éNAINTE DE PROCESARE
        print(f"üíæ Creez backup √Æn: {backup_dir}")
        try:
            os.makedirs(backup_dir, exist_ok=True)
            backup_success = True
            backup_size_total = 0

            for seg in all_segments:
                src = seg['path']
                backup_dst = os.path.join(backup_dir, seg['filename'])

                try:
                    shutil.copy2(src, backup_dst)  # copy2 pƒÉstreazƒÉ »ôi metadata
                    file_size = os.path.getsize(backup_dst)
                    backup_size_total += file_size
                    print(f"üíæ BACKUP: {seg['filename']} ‚Üí g:\\Temporare\\{folder_name}\\")
                except Exception as e:
                    print(f"‚ö† EROARE backup pentru {seg['filename']}: {e}")
                    backup_success = False

            backup_size_mb = backup_size_total / (1024 * 1024)
            if backup_success:
                print(f"‚úÖ BACKUP COMPLET: {len(all_segments)} fi»ôiere ({backup_size_mb:.2f} MB) √Æn {backup_dir}")
            else:
                print(f"‚ö† BACKUP PAR»öIAL: Unele fi»ôiere nu au putut fi copiate √Æn backup")

        except Exception as e:
            print(f"‚ùå EROARE la crearea backup-ului: {e}")
            print(f"üõ°Ô∏è OPRESC PROCESAREA pentru siguran»õƒÉ - fi»ôierele rƒÉm√¢n pe D:\\")
            return

        # PASUL 2: MUTƒÇ (nu copiazƒÉ) TOATE fi»ôierele √Æn folder (DOAR DUPƒÇ backup SUCCESS)
        moved_files = []
        for seg in all_segments:
            src = seg['path']
            dst = os.path.join(dest_dir, seg['filename'])
            try:
                shutil.move(src, dst)  # MOVE √Æn loc de COPY
                moved_files.append(dst)
                print(f"üìÑ MUTAT: {seg['filename']} ‚Üí {folder_name}/")
            except Exception as e:
                print(f"‚ö† Nu am reu»ôit sƒÉ mut {seg['filename']}: {e}")

        if not moved_files:
            print(f"‚ùå Nu am reu»ôit sƒÉ mut niciun fi»ôier pentru '{issue_title}'.")
            return

        print(f"üìÅ Toate {len(moved_files)} PDF-urile pentru '{issue_title}' au fost MUTATE √Æn '{dest_dir}'.")
        print(f"üíæ BACKUP SIGUR gƒÉsit √Æn: {backup_dir}")

        # PASUL 3: CombinƒÉ PDF-urile √Æn ordinea corectƒÉ
        output_file = os.path.join(dest_dir, f"{folder_name}.pdf")

        try:
            if len(moved_files) > 1:
                print(f"üîó Combinez {len(moved_files)} fi»ôiere PDF √Æn ordinea corectƒÉ...")

                # SorteazƒÉ fi»ôierele dupƒÉ range-ul de pagini
                files_with_ranges = []
                for file_path in moved_files:
                    filename = os.path.basename(file_path)
                    start_page, end_page = self.extract_page_range_from_filename(filename)
                    files_with_ranges.append((start_page, end_page, file_path))

                # SorteazƒÉ dupƒÉ pagina de √Ænceput
                files_with_ranges.sort(key=lambda x: x[0])
                sorted_files = [x[2] for x in files_with_ranges]

                # Afi»ôeazƒÉ ordinea de combinare
                print("üìã Ordinea de combinare:")
                for start, end, path in files_with_ranges:
                    filename = os.path.basename(path)
                    print(f"   üìÑ {filename} (pagini {start}-{end})")

                from PyPDF2 import PdfMerger
                merger = PdfMerger()

                for pdf_path in sorted_files:
                    try:
                        merger.append(pdf_path)
                        filename = os.path.basename(pdf_path)
                        print(f"   ‚úÖ AdƒÉugat √Æn ordine: {filename}")
                    except Exception as e:
                        print(f"   ‚ö† Eroare la adƒÉugarea {pdf_path}: {e}")

                # Scrie fi»ôierul combinat
                merger.write(output_file)
                merger.close()

                # VerificƒÉ cƒÉ fi»ôierul combinat a fost creat cu succes
                if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
                    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)
                    print(f"‚úÖ Fi»ôierul combinat creat cu succes: {file_size_mb:.2f} MB")

                    # »òTERGE SEGMENTELE DIN FOLDER (nu mai sunt copii, sunt originalele mutate)
                    deleted_count = 0
                    total_deleted_size = 0

                    for file_to_delete in moved_files:
                        try:
                            file_size = os.path.getsize(file_to_delete)
                            os.remove(file_to_delete)
                            deleted_count += 1
                            total_deleted_size += file_size
                            print(f"   üóëÔ∏è »òters segment: {os.path.basename(file_to_delete)}")
                        except Exception as e:
                            print(f"   ‚ö† Nu am putut »ôterge {file_to_delete}: {e}")

                    deleted_size_mb = total_deleted_size / (1024 * 1024)
                    print(f"üéâ FINALIZAT: PƒÉstrat doar fi»ôierul combinat '{os.path.basename(output_file)}'")
                    print(f"üóëÔ∏è »òterse {deleted_count} segmente originale ({deleted_size_mb:.2f} MB)")
                    print(f"üíæ BACKUP SIGUR: Segmentele originale pƒÉstrate √Æn {backup_dir}")

                else:
                    print(f"‚ùå EROARE: Fi»ôierul combinat nu a fost creat corect!")
                    print(f"üõ°Ô∏è SIGURAN»öƒÇ: PƒÉstrez segmentele pentru siguran»õƒÉ")
                    print(f"üíæ BACKUP DISPONIBIL: {backup_dir}")

            elif len(moved_files) == 1:
                # Un singur fi»ôier - doar redenume»ôte
                original_file = moved_files[0]
                original_size_mb = os.path.getsize(original_file) / (1024 * 1024)

                try:
                    os.replace(original_file, output_file)
                    print(f"‚úÖ Fi»ôierul redenumit √Æn: {os.path.basename(output_file)} ({original_size_mb:.2f} MB)")
                    print(f"üíæ BACKUP SIGUR: Originalul pƒÉstrat √Æn {backup_dir}")
                except Exception as e:
                    print(f"‚ö† Nu am putut redenumi {original_file}: {e}")

            else:
                print(f"‚ÑπÔ∏è Nu existƒÉ fi»ôiere PDF de combinat √Æn '{dest_dir}'.")

        except Exception as e:
            print(f"‚ùå EROARE la combinarea PDF-urilor: {e}")
            print(f"üõ°Ô∏è SIGURAN»öƒÇ: PƒÉstrez segmentele din cauza erorii")
            print(f"üíæ BACKUP DISPONIBIL: {backup_dir}")
            return

        # PASUL 4: Raport final
        try:
            if os.path.exists(output_file):
                final_size_mb = os.path.getsize(output_file) / (1024 * 1024)

                print(f"\nüìã RAPORT FINAL pentru '{issue_title}':")
                print(f"   üìÅ Folder destina»õie: {dest_dir}")
                print(f"   üìÑ Fi»ôier final: {os.path.basename(output_file)} ({final_size_mb:.2f} MB)")
                print(f"   üîç Combinat din {len(all_segments)} segmente")
                print(f"   üíæ BACKUP SIGUR: {backup_dir} ({backup_size_mb:.2f} MB)")
                print(f"   ‚úÖ STATUS: SUCCES - fi»ôier complet creat, backup realizat, segmente »ôterse de pe D:\\")
            else:
                print(f"‚ö† Nu s-a putut crea fi»ôierul final pentru '{issue_title}'")
                print(f"üíæ BACKUP DISPONIBIL: {backup_dir}")
        except Exception as e:
            print(f"‚ö† Eroare la raportul final: {e}")

        print(f"=" * 60)

    def find_next_issue_in_collection_order(self, collection_links, last_completed_url):
        """
        FIXED: GƒÉse»ôte urmƒÉtorul issue de procesat √Æn ordinea din HTML, nu primul din listƒÉ
        """
        if not last_completed_url:
            # DacƒÉ nu avem istoric, √Æncepe cu primul din listƒÉ
            return collection_links[0] if collection_links else None

        try:
            last_index = collection_links.index(last_completed_url.rstrip('/'))
            # ReturneazƒÉ urmƒÉtorul din listƒÉ dupƒÉ cel completat
            next_index = last_index + 1
            if next_index < len(collection_links):
                next_url = collection_links[next_index]
                print(f"üéØ UrmƒÉtorul issue dupƒÉ '{last_completed_url}' este: '{next_url}'")
                return next_url
            else:
                print(f"‚úÖ Toate issue-urile din colec»õie au fost procesate!")
                return None
        except ValueError:
            # DacƒÉ last_completed_url nu e √Æn lista curentƒÉ, √Æncepe cu primul
            print(f"‚ö† URL-ul '{last_completed_url}' nu e √Æn colec»õia curentƒÉ, √Æncep cu primul din listƒÉ")
            return collection_links[0] if collection_links else None

    def get_last_completed_issue_from_collection(self, collection_links):
            """FIXED: GƒÉse»ôte ultimul issue REAL complet descƒÉrcat din colec»õia curentƒÉ"""
            for item in self.state.get("downloaded_issues", []):
                url = item.get("url", "").rstrip('/')
                if url in [link.rstrip('/') for link in collection_links]:

                    # VERIFICARE CORECTƒÇ: Issue-ul trebuie sƒÉ fie REAL complet
                    if self.is_issue_really_complete(item):
                        print(f"üèÅ Ultimul issue REAL complet din colec»õie: {url}")
                        return url
                    elif item.get("completed_at"):
                        last_segment = item.get("last_successful_segment_end", 0)
                        total_pages = item.get("total_pages")
                        pages = item.get("pages", 0)
                        print(f"‚ö† Issue marcat ca complet dar INCOMPLET: {url} ({last_segment}/{total_pages}, pages: {pages})")

            print("üÜï Niciun issue REAL complet gƒÉsit √Æn colec»õia curentƒÉ")
            return None

    def open_new_tab_and_download(self, url):
        """FIXED: Se focuseazƒÉ pe un singur issue p√¢nƒÉ la final cu verificƒÉri ultra-safe"""
        normalized_url = url.rstrip('/')

        # DOAR verificƒÉrile esen»õiale la √Ænceput
        if normalized_url in self.dynamic_skip_urls:
            print(f"‚è≠Ô∏è Sar peste {url} (√Æn skip list).")
            return False

        already_done = any(
            item.get("url") == normalized_url and item.get("completed_at") and
            item.get("last_successful_segment_end", 0) >= (item.get("total_pages") or float('inf'))
            for item in self.state.get("downloaded_issues", [])
        )
        if already_done:
            print(f"‚è≠Ô∏è Sar peste {url} (deja descƒÉrcat complet).")
            return False

        print(f"\nüéØ √éNCEP FOCUSAREA PE: {url}")
        print("=" * 60)

        try:
            if not self.attached_existing:
                self.ensure_alive_fallback()

            # Deschide fila nouƒÉ
            prev_handles = set(self.driver.window_handles)
            self.driver.execute_script("window.open('');")
            new_handles = set(self.driver.window_handles)
            diff = new_handles - prev_handles
            new_handle = diff.pop() if diff else self.driver.window_handles[-1]
            self.driver.switch_to.window(new_handle)

            if not self.navigate_to_page(url):
                print(f"‚ùå Nu am putut naviga la {url}")
                return False

            time.sleep(2)

            # VerificƒÉ DOAR o datƒÉ la √Ænceput pentru limitƒÉ
            if self.check_daily_limit_in_all_windows(set_flag=False):
                print("‚ö† PaginƒÉ cu limitƒÉ zilnicƒÉ detectatƒÉ - opresc aici.")
                self.state["daily_limit_hit"] = True
                self._save_state()
                return False

            print("‚úÖ Pagina e OK, √Æncep extragerea metadatelor...")
            title, subtitle = self.get_issue_metadata()

            # FIXED: ScaneazƒÉ corect fi»ôierele existente pentru acest issue specific
            existing_pages = self.get_existing_pdf_segments(url)
            print(f"üìä Pagini existente pe disk: {existing_pages}")

            resume_from = 1
            json_progress = 0
            for item in self.state.get("downloaded_issues", []):
                if item.get("url") == normalized_url:
                    json_progress = item.get("last_successful_segment_end", 0)
                    total_pages = item.get("total_pages")
                    if json_progress and total_pages and json_progress >= total_pages:
                        resume_from = None
                    break

            actual_progress = max(json_progress, existing_pages)
            if actual_progress > 0 and resume_from is not None:
                resume_from = actual_progress + 1
                print(f"üìÑ Reiau de la pagina {resume_from} (JSON: {json_progress}, Disk: {existing_pages})")

            if resume_from is None:
                print(f"‚è≠Ô∏è Issue-ul {url} este deja complet.")
                return False

            self.current_issue_url = normalized_url

            # FIXED: Ob»õine total_pages »ôi actualizeazƒÉ progresul
            total_pages = self.get_total_pages()
            if total_pages > 0:
                self._update_partial_issue_progress(normalized_url, actual_progress, total_pages=total_pages, title=title, subtitle=subtitle)
            else:
                print("‚ö† Nu am putut ob»õine numƒÉrul total de pagini!")

            print(f"üîí INTR√ÇND √éN MODUL FOCUS - nu mai fac alte verificƒÉri p√¢nƒÉ nu termin!")

            # ==================== DESCƒÇRCAREA PROPRIU-ZISƒÇ ====================
            print(f"üì• √éNCEPE DESCƒÇRCAREA pentru {url}...")
            pages_done, limit_hit = self.save_all_pages_in_batches(resume_from=resume_from)

            print(f"üìä REZULTAT DESCƒÇRCARE:")
            print(f"   üìÑ Pagini descƒÉrcate: {pages_done}")
            print(f"   üìÑ Total necesar: {total_pages}")
            print(f"   üõë LimitƒÉ zilnicƒÉ: {limit_hit}")

            if pages_done == 0 and not limit_hit:
                print(f"‚ö† DescƒÉrcarea pentru {url} a e»ôuat complet.")
                return False

            if limit_hit:
                print(f"‚ö† LimitƒÉ zilnicƒÉ atinsƒÉ √Æn timpul descƒÉrcƒÉrii pentru {url}; progresul par»õial a fost salvat.")
                return False

            # ==================== VERIFICƒÇRI ULTRA SAFE √éNAINTE DE FINALIZARE ====================

            print(f"üîç VERIFICƒÇRI FINALE ULTRA SAFE pentru {url}...")
            print(f"üìä Rezultat descƒÉrcare: {pages_done} pagini din {total_pages}")

            # VerificƒÉ dacƒÉ total_pages a fost detectat corect
            if total_pages <= 0:
                print(f"‚ùå OPRIRE SAFETY: total_pages nu a fost detectat corect ({total_pages})")
                print(f"üõ°Ô∏è NU marchez ca terminat fƒÉrƒÉ total_pages valid")
                print(f"üîÑ PƒÉstrez ca par»õial cu progres {pages_done}")

                self._update_partial_issue_progress(
                    normalized_url, pages_done, total_pages=None, title=title, subtitle=subtitle
                )
                return True  # Succes par»õial

            # VERIFICARE CRITICƒÇ: Progresul trebuie sƒÉ fie aproape complet
            completion_percent = (pages_done / total_pages) * 100
            print(f"üìä Completitudine calculatƒÉ: {completion_percent:.1f}%")

            if completion_percent < 95:  # Cel pu»õin 95%
                print(f"‚ùå BLOCARE SAFETY: Progres insuficient pentru marcare ca terminat")
                print(f"üìä Progres: {pages_done}/{total_pages} ({completion_percent:.1f}%)")
                print(f"üõ°Ô∏è Trebuie cel pu»õin 95% pentru a marca ca terminat!")
                print(f"üîÑ PƒÉstrez ca PAR»öIAL pentru continuare ulterioarƒÉ")

                # MarcheazƒÉ ca par»õial, NU ca terminat
                self._update_partial_issue_progress(
                    normalized_url, pages_done, total_pages=total_pages, title=title, subtitle=subtitle
                )

                print(f"üíæ Issue {url} pƒÉstrat ca par»õial: {pages_done}/{total_pages}")
                print(f"üîÑ Va fi continuat automat la urmƒÉtoarea rulare")
                return True  # Succes par»õial - va continua mai t√¢rziu

            # VERIFICARE SUPLIMENTARƒÇ: Issues mari cu progres mic
            if total_pages >= 500 and pages_done < 200:
                print(f"‚ùå BLOCARE SPECIALƒÇ: Issue mare cu progres suspect de mic")
                print(f"üìä {pages_done} pagini din {total_pages} pare e»ôec de descƒÉrcare!")
                print(f"üõ°Ô∏è Probabil eroare tehnicƒÉ sau limitƒÉ - NU marchez terminat")

                self._update_partial_issue_progress(
                    normalized_url, pages_done, total_pages=total_pages, title=title, subtitle=subtitle
                )
                return True  # Succes par»õial

            # ===== TOATE VERIFICƒÇRILE AU TRECUT - SAFE SƒÇ MARCHEZ CA TERMINAT =====

            print(f"‚úÖ TOATE VERIFICƒÇRILE ULTRA SAFE au trecut pentru {url}")
            print(f"üéØ Progres: {pages_done}/{total_pages} ({completion_percent:.1f}%)")
            print(f"üéØ Marchez ca TERMINAT COMPLET")

            # PAUZƒÇ CRITICƒÇ 1: A»ôteaptƒÉ ca toate fi»ôierele sƒÉ fie complet scrise pe disk
            print("‚è≥ SINCRONIZARE: A»ôtept 30 secunde ca toate fi»ôierele sƒÉ fie complet salvate pe disk...")
            time.sleep(30)

            # FIXED: MarcheazƒÉ ca terminat cu total_pages corect
            self.mark_issue_done(url, pages_done, title=title, subtitle=subtitle, total_pages=total_pages)
            print(f"‚úÖ Issue marcat ca terminat √Æn JSON: {url} ({pages_done} pagini)")

            # PAUZƒÇ CRITICƒÇ 2: A»ôteaptƒÉ ca JSON sƒÉ fie salvat complet
            print("‚è≥ SINCRONIZARE: A»ôtept 5 secunde pentru sincronizarea JSON...")
            time.sleep(5)

            # ==================== PROCESAREA PDF-URILOR ====================
            print(f"üîÑ √éNCEPE PROCESAREA PDF-URILOR pentru {url}...")

            # VerificƒÉ din nou cƒÉ toate fi»ôierele sunt pe disk
            final_segments = self.get_all_pdf_segments_for_issue(url)
            print(f"üîç VERIFICARE FINALƒÇ: Am gƒÉsit {len(final_segments)} fi»ôiere PDF pentru acest issue")

            if len(final_segments) == 0:
                print(f"‚ö† PROBLEMƒÇ: Nu am gƒÉsit fi»ôiere PDF pentru {url}!")
                return False

            # CopiazƒÉ »ôi combinƒÉ PDF-urile
            self.copy_and_combine_issue_pdfs(url, title or normalized_url)

            # PAUZƒÇ CRITICƒÇ 3: A»ôteaptƒÉ ca procesarea PDF sƒÉ fie completƒÉ
            print("‚è≥ SINCRONIZARE: A»ôtept 15 secunde dupƒÉ procesarea PDF-urilor...")
            time.sleep(15)

            # ==================== FINALIZARE COMPLETƒÇ ====================
            print("=" * 60)
            print(f"üéâ FOCUSAREA COMPLETƒÇ PE {url} FINALIZATƒÇ CU SUCCES!")
            print(f"üìä REZULTAT: {pages_done} pagini descƒÉrcate »ôi procesate")
            print("=" * 60)

            # PAUZƒÇ FINALƒÇ: √énainte sƒÉ treacƒÉ la urmƒÉtorul issue
            print("‚è≥ PAUZƒÇ FINALƒÇ: 10 secunde √Ænainte de urmƒÉtorul issue...")
            time.sleep(10)

            return True

        except WebDriverException as e:
            print(f"‚ùå Eroare WebDriver pentru {url}: {e}")
            return False
        except Exception as e:
            print(f"‚ùå Eroare √Æn open_new_tab_and_download pentru {url}: {e}")
            return False
        finally:
            try:
                # NU √éNCHIDE DACƒÇ E ULTIMA FEREASTRƒÇ
                if len(self.driver.window_handles) > 1:
                    self.driver.close()
                    self.driver.switch_to.window(self.driver.window_handles[0])
                else:
                    # Doar revine la prima fereastrƒÉ fƒÉrƒÉ sƒÉ √ÆnchidƒÉ
                    if self.driver.window_handles:
                        self.driver.switch_to.window(self.driver.window_handles[0])
            except Exception as e:
                print(f"‚ö† Eroare √Æn finally: {e}")
                pass

    def ensure_alive_fallback(self):
        try:
            _ = self.driver.title
        except Exception as e:
            print(f"‚ö† Conexiune WebDriver moartƒÉ ({e}), pornesc instan»õƒÉ nouƒÉ ca fallback.")
            prefs = {
                "download.default_directory": os.path.abspath(self.download_dir),
                "download.prompt_for_download": False,
                "download.directory_upgrade": True,
                "safebrowsing.enabled": True,
            }
            chrome_options = Options()
            chrome_options.add_experimental_option("prefs", prefs)
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--incognito")
            self.driver = webdriver.Chrome(options=chrome_options)
            self.wait = WebDriverWait(self.driver, self.timeout)
            self.attached_existing = False

    def run_collection(self, collection_url):
        """FIXED: ProceseazƒÉ UN SINGUR issue pe r√¢nd, fƒÉrƒÉ sƒÉ sarƒÉ la altele"""
        print(f"üåê √éncep procesarea colec»õiei: {collection_url}")
        if not self.driver:
            print("‚ùå Driver neini»õializat.")
            return False
        if not self.navigate_to_page(collection_url):
            return False

        # VerificƒÉ limita DOAR la √Ænceput
        if self.state.get("daily_limit_hit", False):
            print("‚ö† Nu mai pot continua din cauza limitei zilnice setate anterior.")
            return True

        if self.remaining_quota() <= 0:
            print(f"‚ö† Limita zilnicƒÉ de {DAILY_LIMIT} issue-uri atinsƒÉ.")
            return True

        issue_links = self.extract_issue_links_from_collection()
        if not issue_links:
            print("‚ö† Nu s-au gƒÉsit issue-uri √Æn colec»õie.")
            return False

        # FIXED: GƒÉse»ôte ultimul issue complet din colec»õie »ôi continuƒÉ cu urmƒÉtorul
        last_completed = self.get_last_completed_issue_from_collection(issue_links)
        next_to_process = self.find_next_issue_in_collection_order(issue_links, last_completed)

        if not next_to_process:
            print("‚úÖ Toate issue-urile din aceastƒÉ colec»õie sunt complete!")
            return True

        # ProceseazƒÉ issue-urile √Æncep√¢nd cu urmƒÉtorul dupƒÉ cel completat
        start_index = issue_links.index(next_to_process)
        pending_links = issue_links[start_index:]

        # FiltreazƒÉ doar cele care nu sunt √Æn skip list sau complete
        actual_pending = []
        for link in pending_links:
            normalized = link.rstrip('/')
            if normalized in self.dynamic_skip_urls:
                continue
            exists = next((i for i in self.state.get("downloaded_issues", []) if i.get("url") == normalized), None)
            if not exists or not exists.get("completed_at"):
                actual_pending.append(link)

        print(f"üìä Procesez {len(actual_pending)} issue-uri din colec»õia curentƒÉ (√Æncep√¢nd cu {next_to_process})")

        # PROCESEAZƒÇ C√ÇTE UN ISSUE PE R√ÇND - fƒÉrƒÉ sƒÉ sarƒÉ la altele
        processed_any = False
        for i, link in enumerate(actual_pending):
            print(f"\nüî¢ ISSUE {i+1}/{len(actual_pending)}: {link}")

            # VerificƒÉ cota DOAR √Ænainte sƒÉ √ÆnceapƒÉ un issue nou
            if self.remaining_quota() <= 0:
                print(f"‚ö† Limita zilnicƒÉ de {DAILY_LIMIT} issue-uri atinsƒÉ √Ænainte de a √Æncepe acest issue.")
                break

            if self.state.get("daily_limit_hit", False):
                print("‚ö† Flag daily_limit_hit setat - opresc procesarea.")
                break

            # FOCUSEAZƒÇ PE UN SINGUR ISSUE
            print(f"üéØ MƒÉ focusez EXCLUSIV pe: {link}")
            result = self.open_new_tab_and_download(link)

            if result:
                processed_any = True
                print(f"‚úÖ Issue-ul {link} procesat cu succes!")
            else:
                print(f"‚ö† Issue-ul {link} nu a fost procesat.")

            # VerificƒÉ din nou cota dupƒÉ procesare
            if self.remaining_quota() <= 0 or self.state.get("daily_limit_hit", False):
                print("‚ö† Limita zilnicƒÉ atinsƒÉ dupƒÉ procesarea acestui issue.")
                break

            # PauzƒÉ √Æntre issue-uri
            if i < len(actual_pending) - 1:  # Nu pune pauzƒÉ dupƒÉ ultimul
                print("‚è≥ PauzƒÉ de 2s √Æntre issue-uri...")
                time.sleep(2)

        # ACUM VERIFICAREA E DUPƒÇ BUCLA FOR, NU √éNƒÇUNTRUL EI!
        # FIXED: VerificƒÉ dacƒÉ TOATE issue-urile din aceastƒÉ colec»õie sunt complete
        all_done = True
        total_issues = len(issue_links)
        completed_issues = 0
        pending_issues = []

        for link in issue_links:
            normalized = link.rstrip('/')

            # VerificƒÉ √Æn skip URLs (complet descƒÉrcate)
            if normalized in self.dynamic_skip_urls:
                completed_issues += 1
                continue

            # VerificƒÉ √Æn state.json
            exists = next((i for i in self.state.get("downloaded_issues", []) if i.get("url") == normalized), None)

            if exists and exists.get("completed_at") and \
               exists.get("total_pages") and \
               exists.get("last_successful_segment_end", 0) >= exists.get("total_pages", 0):
                completed_issues += 1
            else:
                all_done = False
                pending_issues.append(link)

        print(f"üìä VERIFICARE COLEC»öIE: {completed_issues}/{total_issues} issue-uri complete")
        if pending_issues:
            print(f"üîÑ Issue-uri rƒÉmase: {len(pending_issues)}")
            for idx, link in enumerate(pending_issues[:5]):  # Afi»ôeazƒÉ primele 5
                print(f"   {idx+1}. {link}")
            if len(pending_issues) > 5:
                print(f"   ... »ôi √ÆncƒÉ {len(pending_issues) - 5} issue-uri")

        return all_done

    def process_pending_partials_first(self):
        """FIXED: ProceseazƒÉ mai √Ænt√¢i issue-urile par»õiale, indiferent de colec»õie"""
        pending_partials = self.get_pending_partial_issues()

        if not pending_partials:
            print("‚úÖ Nu existƒÉ issue-uri par»õiale de procesat.")
            return True

        print(f"\nüéØ PRIORITATE: Procesez {len(pending_partials)} issue-uri par»õiale:")
        for item in pending_partials:
            url = item.get("url")
            progress = item.get("last_successful_segment_end", 0)
            total = item.get("total_pages", 0)
            print(f"   üîÑ {url} - pagini {progress}/{total}")

        # ProceseazƒÉ issue-urile par»õiale
        processed_any = False
        for item in pending_partials:
            if self.remaining_quota() <= 0 or self.state.get("daily_limit_hit", False):
                print(f"‚ö† Limita zilnicƒÉ atinsƒÉ √Æn timpul issue-urilor par»õiale.")
                break

            url = item.get("url")
            result = self.open_new_tab_and_download(url)
            if result:
                processed_any = True
            time.sleep(1)

        return processed_any

    def run_additional_collections(self):
        """FIXED: ProceseazƒÉ colec»õiile adi»õionale √Æn ordinea corectƒÉ"""
        start_index = self.state.get("current_additional_collection_index", 0)

        # VERIFICƒÇ cƒÉ indexul e valid
        if start_index >= len(ADDITIONAL_COLLECTIONS):
            print("‚úÖ TOATE colec»õiile adi»õionale au fost procesate!")
            return True

        print(f"üîÑ Continuez cu colec»õiile adi»õionale de la indexul {start_index}")

        for i in range(start_index, len(ADDITIONAL_COLLECTIONS)):
            collection_url = ADDITIONAL_COLLECTIONS[i]

            print(f"\nüìö COLEC»öIA {i+1}/{len(ADDITIONAL_COLLECTIONS)}: {collection_url}")

            # ADAUGƒÇ ACEASTƒÇ VERIFICARE
            if collection_url.rstrip('/') in self.dynamic_skip_urls:
                print(f"‚è≠Ô∏è Sar peste colec»õia {i+1}/{len(ADDITIONAL_COLLECTIONS)} (deja completƒÉ): {collection_url}")
                self.state["current_additional_collection_index"] = i + 1
                self._save_state()
                continue

            if self.remaining_quota() <= 0 or self.state.get("daily_limit_hit", False):
                print(f"‚ö† Limita zilnicƒÉ atinsƒÉ, opresc procesarea colec»õiilor adi»õionale.")
                break

            print(f"\nüîÑ Procesez colec»õia adi»õionalƒÉ {i+1}/{len(ADDITIONAL_COLLECTIONS)}: {collection_url}")

            self.state["current_additional_collection_index"] = i
            self._save_state()

            collection_completed = self.run_collection(collection_url)

            if collection_completed and not self.state.get("daily_limit_hit", False):
                print(f"‚úÖ Colec»õia adi»õionalƒÉ {i+1} completƒÉ, o marchez »ôi trec la urmƒÉtoarea.")
                self.mark_collection_complete(collection_url)
                self.state["current_additional_collection_index"] = i + 1
                self._save_state()
                print(f"üîÑ Continu cu urmƒÉtoarea colec»õie adi»õionalƒÉ...")
            elif self.state.get("daily_limit_hit", False):
                print("‚ö† Limita zilnicƒÉ atinsƒÉ - opresc procesarea.")
                break
            else:
                print(f"‚ö† Colec»õia adi»õionalƒÉ {i+1} nu este completƒÉ √ÆncƒÉ - voi continua cu urmƒÉtoarea!")
                print(f"üîÑ Continu cu urmƒÉtoarea colec»õie adi»õionalƒÉ...")

        current_index = self.state.get("current_additional_collection_index", 0)
        if current_index >= len(ADDITIONAL_COLLECTIONS):
            print("üéâ TOATE colec»õiile adi»õionale au fost procesate!")
            print("üîÑ Voi continua cu urmƒÉtoarea colec»õie disponibilƒÉ...")
            return False  # Returnez False pentru a continua cu urmƒÉtoarea colec»õie
        else:
            remaining = len(ADDITIONAL_COLLECTIONS) - current_index
            print(f"‚ö† Mai rƒÉm√¢n {remaining} colec»õii adi»õionale de procesat.")
            print("üîÑ Voi continua cu urmƒÉtoarea colec»õie...")
            return False  # Returnez False pentru a continua

    def run(self):
            print("üß™ √éncep executarea Chrome PDF Downloader FIXED cu SORTARE CRONOLOGICƒÇ")
            print("=" * 60)

            try:
                if not self.setup_chrome_driver():
                    return False

                print("üîÑ Resetez flag-ul de limitƒÉ zilnicƒÉ »ôi verific ferestrele existente...")
                self.state["daily_limit_hit"] = False
                self._save_state()

                # FIXED: Reconstruie»ôte progresul din fi»ôierele de pe disk
                self.sync_json_with_disk_files()

                # CLEANUP dubluri DUPƒÇ sincronizarea cu disk-ul
                self.cleanup_duplicate_issues()

                # NOUƒÇ ETAPƒÇ: CorecteazƒÉ issue-urile marcate gre»ôit ca complete
                print("\nüîß ETAPA CORECTARE: Verific issue-urile marcate gre»ôit ca complete")
                fixes_applied = self.fix_incorrectly_marked_complete_issues()

                if fixes_applied > 0:
                    print(f"‚úÖ Corectat {fixes_applied} issue-uri - acestea vor fi reluate")

                if self.check_daily_limit_in_all_windows(set_flag=False):
                    print("‚ö† Am gƒÉsit ferestre cu limita deschise din sesiuni anterioare.")
                    print("üîÑ Le-am √Ænchis »ôi re√Æncerc procesarea...")

                # FIXED: ETAPA 0: ProceseazƒÉ MAI √éNT√ÇI issue-urile par»õiale (PRIORITATE ABSOLUTƒÇ)
                print(f"\nüéØ ETAPA 0: PRIORITATE ABSOLUTƒÇ - Procesez issue-urile par»õiale")
                if self.process_pending_partials_first():
                    print("‚úÖ Issue-urile par»õiale au fost procesate sau limita a fost atinsƒÉ.")
                    # OPRE»òTE aici dacƒÉ existƒÉ par»õiale - nu trece la colec»õii noi
                    if self.get_pending_partial_issues():
                        print("üîÑ √éncƒÉ mai existƒÉ issue-uri par»õiale - voi continua cu ele urmƒÉtoarea datƒÉ")
                        return True

                if self.remaining_quota() <= 0 or self.state.get("daily_limit_hit", False):
                    print("‚ö† Limita zilnicƒÉ atinsƒÉ dupƒÉ procesarea issue-urilor par»õiale.")
                    return True

                # ETAPA 1: ProceseazƒÉ colec»õia principalƒÉ (dacƒÉ nu e completƒÉ)
                # FIXED: VerificƒÉ √Æntotdeauna dacƒÉ colec»õia principalƒÉ e completƒÉ
                print(f"\nüìö ETAPA 1: Verific colec»õia principalƒÉ: {self.main_collection_url}")

                main_completed = self.run_collection(self.main_collection_url)

                if self.state.get("daily_limit_hit", False):
                    print("‚ö† Limita zilnicƒÉ atinsƒÉ √Æn colec»õia principalƒÉ.")
                    return True

                if main_completed:
                    print("‚úÖ Colec»õia principalƒÉ este completƒÉ!")
                    self.state["main_collection_completed"] = True
                    self._save_state()
                else:
                    print("üîÑ Colec»õia principalƒÉ nu este completƒÉ √ÆncƒÉ - continuez cu ea.")
                    self.state["main_collection_completed"] = False  # RESETEAZƒÇ dacƒÉ nu e completƒÉ
                    self._save_state()
                    return True

                # ETAPA 2: ProceseazƒÉ colec»õiile adi»õionale
                if self.remaining_quota() > 0 and not self.state.get("daily_limit_hit", False):
                    print(f"\nüìö ETAPA 2: Procesez colec»õiile adi»õionale")
                    self.run_additional_collections()

                print("‚úÖ Toate opera»õiunile au fost ini»õiate.")
                self._finalize_session()
                return True

            except KeyboardInterrupt:
                print("\n\n‚ö† Interven»õie manualƒÉ: √Æntrerupt.")
                return False
            except Exception as e:
                print(f"\n‚ùå Eroare nea»ôteptatƒÉ: {e}")
                return False
            finally:
                if not self.attached_existing and self.driver:
                    try:
                        self.driver.quit()
                    except Exception:
                        pass

    def _finalize_session(self):
        if self.driver:
            if self.attached_existing:
                print("üîñ Am pƒÉstrat sesiunea Chrome existentƒÉ deschisƒÉ (nu fac quit).")
            else:
                print("üö™ √énchid browserul.")
                try:
                    self.driver.quit()
                except Exception:
                    pass


def main():
    """
    MAIN FUNCTION CORECTATƒÇ - FOCUSEAZƒÇ PE StudiiSiCercetariMecanicaSiAplicata
    Nu mai sare la alte colec»õii p√¢nƒÉ nu terminƒÉ cu aceasta complet!
    """

    log_file = setup_logging()  # ADƒÇUGAT - PRIMA LINIE


    print("üöÄ PORNIRE SCRIPT - ANALIZA INI»öIALƒÇ")
    print("=" * 70)

    # PASUL 1: CreeazƒÉ downloader temporar pentru analiza stƒÉrii
    temp_downloader = ChromePDFDownloader("temp", download_dir="D:\\", batch_size=50)

    # PASUL 2: AnalizeazƒÉ starea curentƒÉ
    print("üîç ANALIZA STƒÇRII CURENTE:")
    current_state = temp_downloader.state

    main_completed = current_state.get("main_collection_completed", False)
    current_index = current_state.get("current_additional_collection_index", 0)
    total_issues = len(current_state.get("downloaded_issues", []))

    print(f"   üìä Total issues √Æn state: {total_issues}")
    print(f"   üèÅ Main collection completed: {main_completed}")
    print(f"   üî¢ Current additional index: {current_index}")

    # PASUL 3: VerificƒÉ issue-urile par»õiale (PRIORITATE ABSOLUTƒÇ)
    print(f"\nüéØ VERIFICARE ISSUE-URI PAR»öIALE:")
    pending_partials = temp_downloader.get_pending_partial_issues()

    if pending_partials:
        print(f"üö® GƒÇSITE {len(pending_partials)} ISSUE-URI PAR»öIALE!")
        print(f"üî• PRIORITATE ABSOLUTƒÇ - acestea trebuie continuate:")

        for item in pending_partials:
            url = item.get("url", "")
            progress = item.get("last_successful_segment_end", 0)
            total = item.get("total_pages", 0)
            title = item.get("title", "")
            print(f"   üîÑ {title}")
            print(f"      üìç {url}")
            print(f"      üéØ CONTINUƒÇ de la pagina {progress + 1} (progres: {progress}/{total})")

        print(f"\n‚úÖ VA PROCESA AUTOMAT issue-urile par»õiale primul!")
    else:
        print(f"‚úÖ Nu existƒÉ issue-uri par»õiale de procesat")

    # PASUL 4: AnalizeazƒÉ progresul StudiiSiCercetariMecanicaSiAplicata
    print(f"\nüìö ANALIZA COLEC»öIEI StudiiSiCercetariMecanicaSiAplicata:")

    # Lista completƒÉ a anilor disponibili din HTML (1954-1992, minus 1964)
    expected_years = []
    for year in range(1954, 1993):  # 1954-1992
        if year != 1964:  # 1964 nu existƒÉ √Æn colec»õie
            expected_years.append(year)

    # VerificƒÉ care ani au fost descƒÉrca»õi
    downloaded_years = []
    partial_years = []

    for item in current_state.get("downloaded_issues", []):
        url = item.get("url", "")
        if "StudiiSiCercetariMecanicaSiAplicata" in url:
            # Extrage anul din URL
            year_match = re.search(r'StudiiSiCercetariMecanicaSiAplicata_(\d{4})', url)
            if year_match:
                year = int(year_match.group(1))
                if item.get("completed_at"):
                    downloaded_years.append(year)
                else:
                    partial_years.append(year)

    downloaded_years.sort()
    partial_years.sort()
    missing_years = [year for year in expected_years if year not in downloaded_years and year not in partial_years]

    print(f"   üìÖ Ani disponibili: {len(expected_years)} (1954-1992, minus 1964)")
    print(f"   ‚úÖ Ani descƒÉrca»õi: {len(downloaded_years)} - {downloaded_years}")
    print(f"   üîÑ Ani par»õiali: {len(partial_years)} - {partial_years}")
    print(f"   ‚ùå Ani lipsƒÉ: {len(missing_years)} - {missing_years[:10]}{'...' if len(missing_years) > 10 else ''}")

    # PASUL 5: DeterminƒÉ strategia
    total_remaining = len(partial_years) + len(missing_years)

    if total_remaining > 0:
        print(f"\nüéØ STRATEGIA DE PROCESARE:")
        print(f"   üî• RƒÇM√ÇN {total_remaining} ani de procesat din StudiiSiCercetariMecanicaSiAplicata")
        print(f"   üö´ NU se trece la alte colec»õii p√¢nƒÉ nu se terminƒÉ aceasta!")
        print(f"   üìà Progres: {len(downloaded_years)}/{len(expected_years)} ani completa»õi ({len(downloaded_years)/len(expected_years)*100:.1f}%)")
    else:
        print(f"\n‚úÖ StudiiSiCercetariMecanicaSiAplicata este COMPLET!")
        print(f"   üéØ Va trece la urmƒÉtoarea colec»õie din ADDITIONAL_COLLECTIONS")

    # PASUL 6: ReseteazƒÉ starea pentru a continua corect cu StudiiSiCercetariMecanicaSiAplicata
    if total_remaining > 0:
        print(f"\nüîß RESETEZ STAREA pentru a continua cu StudiiSiCercetariMecanicaSiAplicata:")

        # ReseteazƒÉ flag-urile gre»ôite
        if main_completed:
            print(f"   üîÑ Resetez main_collection_completed: True ‚Üí False")
            temp_downloader.state["main_collection_completed"] = False

        if current_index > 1:  # StudiiSiCercetariMecanicaSiAplicata e pe index 1
            print(f"   üîÑ Resetez current_additional_collection_index: {current_index} ‚Üí 1")
            temp_downloader.state["current_additional_collection_index"] = 1

        temp_downloader._save_state()
        print(f"   ‚úÖ Starea resetatƒÉ pentru a continua cu StudiiSiCercetariMecanicaSiAplicata")

    # PASUL 7: SeteazƒÉ URL-ul colec»õiei principale
    main_collection_url = "https://adt.arcanum.com/ro/collection/StudiiSiCercetariMecanicaSiAplicata/"

    print(f"\nüöÄ √éNCEPE PROCESAREA:")
    print(f"üìç URL principal: {main_collection_url}")
    print(f"üìÅ Director descƒÉrcare: D:\\")
    print(f"üì¶ Batch size: 50 pagini per segment")

    if pending_partials:
        print(f"‚ö° Va √Æncepe cu {len(pending_partials)} issue-uri par»õiale")
    if missing_years:
        print(f"üìÖ Va continua cu anii lipsƒÉ: {missing_years[:5]}{'...' if len(missing_years) > 5 else ''}")

    print("=" * 70)

    # PASUL 8: CreeazƒÉ downloader-ul principal »ôi porne»ôte procesarea
    try:
        downloader = ChromePDFDownloader(
            main_collection_url=main_collection_url,
            download_dir="D:\\",
            batch_size=50
        )

        print("üéØ √éNCEPE EXECU»öIA PRINCIPALƒÇ...")
        success = downloader.run()

        if success:
            print("\n‚úÖ EXECU»öIE FINALIZATƒÇ CU SUCCES!")
        else:
            print("\n‚ö† EXECU»öIE FINALIZATƒÇ CU PROBLEME!")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n\n‚ö† OPRIRE MANUALƒÇ - Progresul a fost salvat")
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå EROARE FATALƒÇ √Æn main(): {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"‚ùå Eroare fatalƒÉ √Æn __main__: {e}")
        sys.exit(1)