#!/usr/bin/env python3
"""
Automatizare descÄƒrcare PDF-uri din Arcanum (FIXED VERSION cu SORTARE CRONOLOGICÄ‚):
- FIXED: ScaneazÄƒ corect toate fiÈ™ierele existente de pe disk
- FIXED: PÄƒstreazÄƒ progresul parÈ›ial Ã®ntre zile
- FIXED: ProceseazÄƒ È™i combinÄƒ corect TOATE PDF-urile pentru fiecare issue
- FIXED: Resume logic corect pentru issue-urile parÈ›iale
- FIXED: DetecteazÄƒ corect prefix-urile pentru fiÈ™iere
- FIXED: VerificÄƒ corect issue-urile complete pentru skip URLs
- FIXED: EliminÄƒ dublurile automat
- FIXED: DetecteazÄƒ mai bine numÄƒrul total de pagini
- FIXED: Sortare cronologicÄƒ corectÄƒ Ã®n downloaded_issues
"""

import time
import os
import sys
import re
import json
import shutil
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException, ElementClickInterceptedException

import logging
import sys

def setup_logging():
    """ConfigureazÄƒ logging Ã®n timp real"""
    log_dir = r"E:\Carte\BB\17 - Site Leadership\alte\Ionel Balauta\Aryeht\Task 1 - Traduce tot site-ul\Doar Google Web\Andreea\Meditatii\2023\++Arcanum Download + Chrome\Ruleaza cand sunt plecat 3\Logs"
    os.makedirs(log_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"arcanum_download_{timestamp}.log")

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[
            logging.FileHandler(log_file, mode='w', encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )

    for handler in logging.getLogger().handlers:
        if isinstance(handler, logging.FileHandler):
            handler.stream.reconfigure(line_buffering=True)

    print(f"ğŸ“ LOGGING ACTIVAT: {log_file}")
    return log_file

# ColecÈ›iile adiÈ›ionale (procesate DUPÄ‚ colecÈ›ia principalÄƒ din main())
ADDITIONAL_COLLECTIONS = [
    "https://adt.arcanum.com/ro/collection/Convietuirea/",
    "https://adt.arcanum.com/ro/collection/StudiiSiCercetariMecanicaSiAplicata/",
    "https://adt.arcanum.com/ro/collection/CalitateFiabilitat/",
    "https://adt.arcanum.com/ro/collection/Metrologie/",
    "https://adt.arcanum.com/ro/collection/MetrologiaAplicata/",
    "https://adt.arcanum.com/ro/collection/AnaleleUnivBucuresti_GeologieGeografie/",
    "https://adt.arcanum.com/ro/collection/BuletinulStiintificTechnicInstitutuluiPolitehnicTimisoara_MatematicaFizicaMecanica/",
    "https://adt.arcanum.com/ro/collection/AlmanahAteneu/",
    "https://adt.arcanum.com/hu/collection/AMatematikaTanitasa/",
    "https://adt.arcanum.com/hu/collection/AFizikaTanitasa/",
    "https://adt.arcanum.com/hu/collection/AKemiaTanitasa/",
    "https://adt.arcanum.com/ro/collection/StudiiSiCercetariMatematice/",
    "https://adt.arcanum.com/ro/collection/AnaleleUnivBucuresti_MatematicaMecanicaFizica/",
    "https://adt.arcanum.com/ro/collection/RevistaMatematicaDinTimisoara/",
    "https://adt.arcanum.com/ro/collection/BuletInstPolitehIasi_1/",
    "https://adt.arcanum.com/ro/collection/Energetica/",
    "https://adt.arcanum.com/ro/collection/CulturaFizicaSiSport/",
    "https://adt.arcanum.com/ro/collection/Almanahul_Satelor/",
    "https://adt.arcanum.com/ro/collection/SteauaRomaniei/",
    "https://adt.arcanum.com/ro/collection/RevistaDeFolclor/",
    "https://adt.arcanum.com/ro/collection/AlmanahBTT/",
    "https://adt.arcanum.com/ro/collection/MinePetrolGaze/",
    "https://adt.arcanum.com/ro/collection/BuletinulInstitutuluiPolitehnicBucuresti_Mecanica/",
    "https://adt.arcanum.com/ro/collection/RevistaMinelor/",
    "https://adt.arcanum.com/ro/collection/StudiiSiCercetariDeGeologie/",
    "https://adt.arcanum.com/ro/collection/RevistaIndustriaAlimentare/",
    "https://adt.arcanum.com/ro/collection/IndustriaLemnului/",
    "https://adt.arcanum.com/ro/collection/Electricitatea/",
    "https://adt.arcanum.com/ro/collection/SzatmariMuzeumKiadvanyai_Evkonyv_ADT/",
    "https://adt.arcanum.com/ro/collection/ConstructiaDeMasini/",
    "https://adt.arcanum.com/ro/collection/SufletNou/",
    "https://adt.arcanum.com/ro/collection/Rondul/",
    "https://adt.arcanum.com/ro/collection/RomaniaMilitara/",
    "https://adt.arcanum.com/ro/collection/Magazin/",
    "https://adt.arcanum.com/ro/collection/TribunaSibiului/",
    "https://adt.arcanum.com/ro/collection/Metalurgia/",
    "https://adt.arcanum.com/ro/collection/StudiiSiCercetariDeMetalurgie/",
    "https://adt.arcanum.com/ro/collection/BuletinulInstitutuluiPolitehnicBucuresti_ChimieMetalurgie/",
    "https://adt.arcanum.com/ro/collection/RomaniaMuncitoare/",
    "https://adt.arcanum.com/ro/collection/Cronica/",
    "https://adt.arcanum.com/ro/collection/Marisia_ADT/",
    "https://adt.arcanum.com/ro/collection/RevistaDeEtnografieSiFolclor/",
    "https://adt.arcanum.com/ro/collection/RevistaMuzeelor/",
    "https://adt.arcanum.com/ro/collection/GazetaDeDuminica/",
    "https://adt.arcanum.com/ro/collection/BuletInstPolitehIasi_0/",
    "https://adt.arcanum.com/ro/collection/JurnalulNational/",
    "https://adt.arcanum.com/ro/collection/UnireaBlajPoporului/",
    "https://adt.arcanum.com/ro/collection/TranssylvaniaNostra/",
    "https://adt.arcanum.com/ro/collection/CuvantulPoporului/",
    "https://adt.arcanum.com/ro/collection/Agrarul/",
    "https://adt.arcanum.com/ro/collection/Radiofonia/",
    "https://adt.arcanum.com/ro/collection/CurierulDeIasi/",
    "https://adt.arcanum.com/ro/collection/CurierulFoaiaIntereselorGenerale/",
    "https://adt.arcanum.com/ro/collection/EconomiaNationala/",
    "https://adt.arcanum.com/ro/collection/Constitutionalul/",
    "https://adt.arcanum.com/ro/collection/Semnalul/",
    "https://adt.arcanum.com/ro/collection/Rampa/",
    "https://adt.arcanum.com/ro/collection/ViataRomaneasca/",
    "https://adt.arcanum.com/ro/collection/SteauaRosie/",
    "https://adt.arcanum.com/ro/collection/Almanah_ScinteiaTineretului/",
    "https://adt.arcanum.com/ro/collection/EvenimentulZilei/",
    "https://adt.arcanum.com/ro/collection/CurierulFoaiaIntereselorGenerale/",
    "https://adt.arcanum.com/hu/collection/IdegenNyelvekTanitasa/",
    "https://adt.arcanum.com/hu/collection/AzEnekZeneTanitasa/",
    "https://adt.arcanum.com/hu/collection/ABiologiaTanitasa/",
    "https://adt.arcanum.com/hu/collection/ErtekezesekEmlekezesek/",
    "https://adt.arcanum.com/hu/collection/Books_SorozatonKivul/",
    "https://adt.arcanum.com/hu/collection/Books_22_OrvoslasTermeszetrajz/",
    "https://adt.arcanum.com/hu/collection/DomolkiKonyvek/"
]

# Skip URLs statice (hardcoded)
STATIC_SKIP_URLS = {
    "https://adt.arcanum.com/ro/view/Convietuirea_1997-1998"
}

DAILY_LIMIT = 105
STATE_FILENAME = "state.json"
SKIP_URLS_FILENAME = "skip_urls.json"


class ChromePDFDownloader:
    def __init__(self, main_collection_url, download_dir=None, batch_size=50, timeout=15):
        self.main_collection_url = main_collection_url
        self.batch_size = batch_size
        self.timeout = timeout
        self.download_dir = download_dir or os.getcwd()
        self.driver = None
        self.wait = None
        self.attached_existing = False
        self.state_path = os.path.join(self.download_dir, STATE_FILENAME)
        self.skip_urls_path = os.path.join(self.download_dir, SKIP_URLS_FILENAME)
        self.current_issue_url = None
        self.dynamic_skip_urls = set()
        self._load_skip_urls()
        self._load_state()
        self.fix_existing_json()

    def _load_skip_urls(self):
        """ÃncarcÄƒ skip URLs din fiÈ™ierul separat"""
        self.dynamic_skip_urls = set(STATIC_SKIP_URLS)  # Ãncepe cu cele statice

        if os.path.exists(self.skip_urls_path):
            try:
                with open(self.skip_urls_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    completed_urls = data.get("completed_urls", [])
                    completed_collections = data.get("completed_collections", [])

                    self.dynamic_skip_urls.update(url.rstrip('/') for url in completed_urls)
                    self.dynamic_skip_urls.update(url.rstrip('/') for url in completed_collections)

                    print(f"ğŸ“‹ ÃncÄƒrcat {len(completed_urls)} URL-uri complet descÄƒrcate din {SKIP_URLS_FILENAME}")
                    print(f"ğŸ“‹ ÃncÄƒrcat {len(completed_collections)} colecÈ›ii complet procesate din {SKIP_URLS_FILENAME}")
            except Exception as e:
                print(f"âš  Eroare la citirea {SKIP_URLS_FILENAME}: {e}")

        print(f"ğŸš« Total URL-uri de skip: {len(self.dynamic_skip_urls)}")

    def _save_skip_urls(self):
        """FIXED: VerificÄƒ corect dacÄƒ un issue este complet - FOLOSEÈ˜TE last_successful_segment_end!"""
        try:
            completed_urls = []
            for item in self.state.get("downloaded_issues", []):
                # VERIFICARE CORECTÄ‚: foloseÈ™te last_successful_segment_end, NU pages!
                completed_at = item.get("completed_at")
                total_pages = item.get("total_pages")
                last_segment = item.get("last_successful_segment_end", 0)
                pages = item.get("pages", 0)  # Pentru debug

                # CONDIÈšIE FIXATÄ‚: verificÄƒ progresul REAL (last_segment), nu pages!
                if (completed_at and  # Marcat ca terminat
                    total_pages and  # Are total_pages setat
                    total_pages > 0 and  # Total valid
                    last_segment >= total_pages):  # Progresul REAL este complet

                    completed_urls.append(item["url"])
                    print(f"âœ… Issue complet pentru skip: {item['url']} ({last_segment}/{total_pages})")

                    # DEBUG: AfiÈ™eazÄƒ discrepanÈ›ele
                    if pages != last_segment:
                        print(f"   âš  DISCREPANÈšÄ‚: pages={pages}, last_segment={last_segment}")
                else:
                    # DEBUG: AfiÈ™eazÄƒ de ce nu e considerat complet
                    if item.get("url"):  # Doar dacÄƒ are URL valid
                        print(f"ğŸ”„ Issue incomplet: {item.get('url', 'NO_URL')}")
                        print(f"   completed_at: {bool(completed_at)}")
                        print(f"   total_pages: {total_pages}")
                        print(f"   last_segment: {last_segment}")
                        print(f"   pages: {pages}")

                        # VerificÄƒ fiecare condiÈ›ie individual
                        if not completed_at:
                            print(f"   â†’ LipseÈ™te completed_at")
                        elif not total_pages or total_pages <= 0:
                            print(f"   â†’ total_pages invalid")
                        elif last_segment < total_pages:
                            print(f"   â†’ Progres incomplet: {last_segment}/{total_pages}")

            # AdaugÄƒ È™i cele statice
            all_completed = list(STATIC_SKIP_URLS) + completed_urls

            # PÄƒstreazÄƒ È™i colecÈ›iile complete dacÄƒ existÄƒ
            existing_data = {}
            if os.path.exists(self.skip_urls_path):
                with open(self.skip_urls_path, "r", encoding="utf-8") as f:
                    existing_data = json.load(f)

            data = {
                "last_updated": datetime.now().isoformat(),
                "completed_urls": sorted(list(set(all_completed))),
                "completed_collections": existing_data.get("completed_collections", [])
            }

            with open(self.skip_urls_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            print(f"ğŸ’¾ Salvat {len(data['completed_urls'])} URL-uri CORECT VERIFICATE Ã®n {SKIP_URLS_FILENAME}")

            # RAPORT FINAL pentru debugging
            print(f"ğŸ“‹ ISSUES COMPLETE Ã®n skip_urls:")
            for url in sorted(completed_urls):
                year = url.split('_')[-1] if '_' in url else 'UNKNOWN'
                print(f"   âœ… {year}")

        except Exception as e:
            print(f"âš  Eroare la salvarea {SKIP_URLS_FILENAME}: {e}")

    def _safe_folder_name(self, name: str) -> str:
        return re.sub(r'[<>:"/\\|?*]', '_', name).strip()

    def _decode_unicode_escapes(self, obj):
        """DecodificÄƒ secvenÈ›ele unicode din JSON"""
        if isinstance(obj, dict):
            return {key: self._decode_unicode_escapes(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._decode_unicode_escapes(item) for item in obj]
        elif isinstance(obj, str):
            # DecodificÄƒ secvenÈ›ele unicode ca \u0103, \u0219
            try:
                return obj.encode('utf-8').decode('unicode_escape').encode('latin-1').decode('utf-8') if '\\u' in obj else obj
            except:
                return obj
        else:
            return obj

    def is_issue_complete_by_end_page(self, end_page):
        """DeterminÄƒ dacÄƒ un issue e complet pe baza ultimei pagini"""
        return not ((end_page + 1) % 50 == 0 or (end_page + 1) % 100 == 0)

    def extract_issue_id_from_filename(self, filename):
        """FIXED: Extrage ID-ul issue-ului din numele fiÈ™ierului (fÄƒrÄƒ timestamp)"""
        # CautÄƒ pattern-ul: PrefixIssue-TIMESTAMP__pages
        match = re.search(r'([^-]+(?:-[^-]+)*)-\d+__pages', filename)
        if match:
            return match.group(1)
        return None

    def extract_issue_url_from_filename(self, filename):
        """FIXED: Extrage URL-ul issue-ului din numele fiÈ™ierului"""
        issue_id = self.extract_issue_id_from_filename(filename)
        if not issue_id:
            return None

        if "Convietuirea" in issue_id:
            return f"https://adt.arcanum.com/ro/view/{issue_id}"
        elif "GazetaMatematica" in issue_id:
            return f"https://adt.arcanum.com/en/view/{issue_id}"
        else:
            return f"https://adt.arcanum.com/ro/view/{issue_id}"

    def get_all_pdf_segments_for_issue(self, issue_url):
        """FIXED: ScaneazÄƒ toate fiÈ™ierele PDF pentru un issue specific"""
        issue_id = issue_url.rstrip('/').split('/')[-1]
        segments = []

        try:
            for filename in os.listdir(self.download_dir):
                if not filename.lower().endswith('.pdf'):
                    continue

                file_issue_id = self.extract_issue_id_from_filename(filename)
                if file_issue_id == issue_id:
                    # Extrage intervalul de pagini
                    match = re.search(r'__pages(\d+)-(\d+)\.pdf', filename)
                    if match:
                        start_page = int(match.group(1))
                        end_page = int(match.group(2))
                        segments.append({
                            'filename': filename,
                            'start': start_page,
                            'end': end_page,
                            'path': os.path.join(self.download_dir, filename)
                        })

        except Exception as e:
            print(f"âš  Eroare la scanarea fiÈ™ierelor pentru {issue_url}: {e}")

        # SorteazÄƒ dupÄƒ pagina de Ã®nceput
        segments.sort(key=lambda x: x['start'])
        return segments

    def get_existing_pdf_segments(self, issue_url):
        """FIXED: ScaneazÄƒ toate segmentele existente È™i returneazÄƒ ultima paginÄƒ"""
        segments = self.get_all_pdf_segments_for_issue(issue_url)

        if not segments:
            return 0

        # GÄƒseÈ™te cea mai mare paginÄƒ finalÄƒ
        max_page = max(seg['end'] for seg in segments)

        print(f"ğŸ“Š FiÈ™iere PDF existente pentru {issue_url}:")
        for seg in segments:
            print(f"   ğŸ“„ {seg['filename']} (pagini {seg['start']}-{seg['end']})")

        return max_page

    def reconstruct_all_issues_from_disk(self):
        """FIXED: ReconstruieÈ™te complet progresul din fiÈ™ierele de pe disk"""
        print("ğŸ” SCANEZ COMPLET toate fiÈ™ierele PDF de pe disk...")

        # GrupeazÄƒ fiÈ™ierele dupÄƒ issue ID
        issues_on_disk = {}

        try:
            for filename in os.listdir(self.download_dir):
                if not filename.lower().endswith('.pdf'):
                    continue

                issue_id = self.extract_issue_id_from_filename(filename)
                if not issue_id:
                    continue

                # Extrage intervalul de pagini
                match = re.search(r'__pages(\d+)-(\d+)\.pdf', filename)
                if not match:
                    continue

                start_page = int(match.group(1))
                end_page = int(match.group(2))

                if issue_id not in issues_on_disk:
                    issues_on_disk[issue_id] = {
                        'segments': [],
                        'max_page': 0,
                        'url': self.extract_issue_url_from_filename(filename)
                    }

                issues_on_disk[issue_id]['segments'].append({
                    'filename': filename,
                    'start': start_page,
                    'end': end_page
                })

                if end_page > issues_on_disk[issue_id]['max_page']:
                    issues_on_disk[issue_id]['max_page'] = end_page

        except Exception as e:
            print(f"âš  Eroare la scanarea disk-ului: {e}")
            return {}

        # AfiÈ™eazÄƒ rezultatele scanÄƒrii
        print(f"ğŸ“Š GÄƒsite {len(issues_on_disk)} issue-uri pe disk:")
        for issue_id, data in issues_on_disk.items():
            segments_count = len(data['segments'])
            max_page = data['max_page']
            url = data['url']

            print(f"   ğŸ“ {issue_id}: {segments_count} segmente, max pagina {max_page}")
            print(f"      ğŸ”— URL: {url}")

            # AfiÈ™eazÄƒ segmentele sortate
            data['segments'].sort(key=lambda x: x['start'])
            for seg in data['segments'][:3]:  # Primele 3
                print(f"      ğŸ“„ {seg['filename']} ({seg['start']}-{seg['end']})")
            if segments_count > 3:
                print(f"      ğŸ“„ ... È™i Ã®ncÄƒ {segments_count - 3} segmente")

        return issues_on_disk

    def sync_json_with_disk_files(self):
        """SAFE: ÃmbogÄƒÈ›eÈ™te informaÈ›iile din JSON cu cele de pe disk, ZERO pierderi + SORTARE CRONOLOGICÄ‚ CORECTÄ‚"""
        print("ğŸ”„ MERGE SAFE - combinez informaÈ›iile din JSON cu cele de pe disk...")

        # PASUL 1: ScaneazÄƒ complet disk-ul
        issues_on_disk = self.reconstruct_all_issues_from_disk()

        # PASUL 2: PÄ‚STREAZÄ‚ TOATE issue-urile existente din JSON (ZERO pierderi)
        existing_issues_by_url = {}
        for item in self.state.get("downloaded_issues", []):
            url = item.get("url", "").rstrip('/')
            existing_issues_by_url[url] = item.copy()  # DEEP COPY pentru siguranÈ›Äƒ

        print(f"ğŸ“‹ PÄ‚STREZ {len(existing_issues_by_url)} issue-uri din JSON existent")

        # PASUL 3: MERGE cu datele de pe disk (doar Ã®mbogÄƒÈ›eÈ™te, nu È™terge)
        enriched_count = 0
        new_from_disk_count = 0

        for issue_id, disk_data in issues_on_disk.items():
            url = disk_data['url']
            if not url:
                continue

            max_page = disk_data['max_page']
            segments_count = len(disk_data['segments'])
            is_complete = self.is_issue_complete_by_end_page(max_page)

            if url in existing_issues_by_url:
                # ÃMBOGÄ‚ÈšEÈ˜TE issue-ul existent (doar dacÄƒ progresul e mai mare)
                existing_issue = existing_issues_by_url[url]
                current_progress = existing_issue.get("last_successful_segment_end", 0)

                if max_page > current_progress:
                    # ÃMBOGÄ‚ÈšEÈ˜TE doar cÃ¢mpurile necesare, pÄƒstreazÄƒ restul
                    existing_issue["last_successful_segment_end"] = max_page
                    if not existing_issue.get("total_pages"):
                        existing_issue["total_pages"] = max_page
                    enriched_count += 1
                    print(f"ğŸ”„ ÃMBOGÄ‚ÈšIT: {url} - progres {current_progress} â†’ {max_page}")

                # MarcheazÄƒ ca complet DOAR dacÄƒ nu era deja marcat
                if is_complete and not existing_issue.get("completed_at"):
                    existing_issue["completed_at"] = datetime.now().isoformat(timespec="seconds")
                    existing_issue["pages"] = max_page
                    existing_issue["total_pages"] = max_page
                    print(f"âœ… MARCAT ca complet: {url} ({max_page} pagini)")

            else:
                # Issue complet nou gÄƒsit doar pe disk - ADAUGÄ‚
                new_issue = {
                    "url": url,
                    "title": issue_id.replace("-", " ").replace("_", " "),
                    "subtitle": "",
                    "pages": max_page if is_complete else 0,
                    "completed_at": datetime.now().isoformat(timespec="seconds") if is_complete else "",
                    "last_successful_segment_end": max_page,
                    "total_pages": max_page if is_complete else None
                }
                existing_issues_by_url[url] = new_issue
                new_from_disk_count += 1
                print(f"â• ADÄ‚UGAT nou din disk: {url} ({max_page} pagini, {segments_count} segmente)")

        # PASUL 4: ReconstruieÈ™te lista finalÄƒ (TOATE issue-urile pÄƒstrate)
        all_issues_list = list(existing_issues_by_url.values())

        # PASUL 5: SORTARE CRONOLOGICÄ‚ CORECTÄ‚
        partial_issues = []
        complete_issues = []

        for issue in all_issues_list:
            is_partial = (issue.get("last_successful_segment_end", 0) > 0 and
                         not issue.get("completed_at") and
                         issue.get("total_pages") and
                         issue.get("last_successful_segment_end", 0) < issue.get("total_pages", 0))

            if is_partial:
                partial_issues.append(issue)
                print(f"ğŸ”„ Issue parÈ›ial: {issue['url']} ({issue.get('last_successful_segment_end', 0)}/{issue.get('total_pages', 0)} pagini)")
            else:
                complete_issues.append(issue)

        # SORTARE CRONOLOGICÄ‚ PENTRU COMPLETE ISSUES
        # SorteazÄƒ issue-urile complete dupÄƒ completed_at (cel mai recent primul)
        def sort_key_for_complete(issue):
            completed_at = issue.get("completed_at", "")
            if completed_at:
                try:
                    # ConverteÈ™te la datetime pentru sortare corectÄƒ
                    return datetime.fromisoformat(completed_at.replace('Z', '+00:00'))
                except:
                    return datetime.min
            else:
                # Issue-urile fÄƒrÄƒ completed_at merg la sfÃ¢rÈ™it
                return datetime.min

        # SorteazÄƒ: parÈ›iale dupÄƒ progres (desc), complete dupÄƒ data (desc - cel mai recent primul)
        partial_issues.sort(key=lambda x: x.get("last_successful_segment_end", 0), reverse=True)
        complete_issues.sort(key=sort_key_for_complete, reverse=True)  # Cel mai recent primul

        print(f"\nğŸ“Š SORTARE CRONOLOGICÄ‚ APLICATÄ‚:")
        print(f"   ğŸ”„ Issue-uri parÈ›iale: {len(partial_issues)} (sortate dupÄƒ progres)")

        if complete_issues:
            print(f"   âœ… Issue-uri complete: {len(complete_issues)} (sortate cronologic)")
            print(f"      ğŸ“… Cel mai recent: {complete_issues[0].get('completed_at', 'N/A')}")
            print(f"      ğŸ“… Cel mai vechi: {complete_issues[-1].get('completed_at', 'N/A')}")

            # AfiÈ™eazÄƒ primele 5 pentru verificare
            print(f"      ğŸ” Ordinea cronologicÄƒ (primele 5):")
            for i, issue in enumerate(complete_issues[:5]):
                url = issue.get('url', '').split('/')[-1]
                completed_at = issue.get('completed_at', 'N/A')
                print(f"         {i+1}. {url} - {completed_at}")

        # PASUL 6: ActualizeazÄƒ starea SAFE (pÄƒstreazÄƒ tot ce nu modificÄƒm)
        original_count = self.state.get("count", 0)
        final_issues = partial_issues + complete_issues  # ParÈ›iale primul, apoi complete cronologic
        actual_complete_count = len([i for i in final_issues if i.get("completed_at")])

        # PÄ‚STREAZÄ‚ toate cÃ¢mpurile existente, actualizeazÄƒ doar ce e necesar
        self.state["downloaded_issues"] = final_issues
        self.state["count"] = max(original_count, actual_complete_count)  # Nu scade niciodatÄƒ

        self._save_state_safe()

        print(f"âœ… MERGE COMPLET cu SORTARE CRONOLOGICÄ‚ CORECTÄ‚ - ZERO pierderi:")
        print(f"   ğŸ“Š Total issues: {len(final_issues)} (Ã®nainte: {len(existing_issues_by_url) - new_from_disk_count})")
        print(f"   ğŸ”„ ÃmbogÄƒÈ›ite: {enriched_count}")
        print(f"   â• AdÄƒugate din disk: {new_from_disk_count}")
        print(f"   ğŸ”„ ParÈ›iale: {len(partial_issues)}")
        print(f"   âœ… Complete: {len(complete_issues)}")
        print(f"   ğŸ¯ Count pÄƒstrat/actualizat: {original_count} â†’ {self.state['count']}")

        if partial_issues:
            print("ğŸ¯ Issue-urile parÈ›iale vor fi procesate primele!")

        print("ğŸ“… Issue-urile complete sunt acum sortate cronologic (cel mai recent primul)!")

    def cleanup_duplicate_issues(self):
        """NOUÄ‚ FUNCÈšIE: EliminÄƒ dublurile din state.json"""
        print("ğŸ§¹ CURÄ‚ÈšARE: Verific È™i elimin dublurile din state.json...")

        issues = self.state.get("downloaded_issues", [])
        if not issues:
            return

        # GrupeazÄƒ dupÄƒ URL normalizat
        url_groups = {}
        for i, item in enumerate(issues):
            url = item.get("url", "").rstrip('/').lower()
            if not url:
                continue

            if url not in url_groups:
                url_groups[url] = []
            url_groups[url].append((i, item))

        # GÄƒseÈ™te È™i rezolvÄƒ dublurile
        duplicates_found = 0
        clean_issues = []
        processed_urls = set()

        for original_url, group in url_groups.items():
            if len(group) > 1:
                duplicates_found += 1
                print(f"ğŸ” DUBLURÄ‚ gÄƒsitÄƒ pentru {original_url}: {len(group)} intrÄƒri")

                # GÄƒseÈ™te cea mai completÄƒ versiune
                best_item = None
                best_score = -1

                for idx, item in group:
                    score = 0
                    if item.get("completed_at"): score += 100
                    if item.get("total_pages"): score += 50
                    if item.get("title"): score += 10
                    if item.get("last_successful_segment_end", 0) > 0: score += 20

                    print(f"   ğŸ“Š Index {idx}: score {score}, completed: {bool(item.get('completed_at'))}")

                    if score > best_score:
                        best_score = score
                        best_item = item

                print(f"   âœ… PÄƒstrez cea mai completÄƒ versiune (score: {best_score})")
                clean_issues.append(best_item)
            else:
                # Nu e dublurÄƒ, pÄƒstreazÄƒ-l
                clean_issues.append(group[0][1])

            processed_urls.add(original_url)

        if duplicates_found > 0:
            print(f"ğŸ§¹ ELIMINAT {duplicates_found} dubluri din {len(issues)} issues")
            print(f"ğŸ“Š RÄƒmas cu {len(clean_issues)} issues unice")

            self.state["downloaded_issues"] = clean_issues
            self._save_state_safe()
        else:
            print("âœ… Nu am gÄƒsit dubluri Ã®n state.json")

    def is_issue_really_complete(self, item):
            """HELPER: VerificÄƒ dacÄƒ un issue este REAL complet (nu doar marcat ca atare)"""
            completed_at = item.get("completed_at")
            last_segment = item.get("last_successful_segment_end", 0)
            total_pages = item.get("total_pages")
            pages = item.get("pages", 0)

            # Un issue este REAL complet dacÄƒ:
            # 1. Are completed_at setat È˜I
            # 2. Are progresul complet (last_segment >= total_pages) È˜I
            # 3. Are pages > 0 (nu e marcat greÈ™it)
            return (
                completed_at and
                total_pages and
                total_pages > 0 and
                last_segment >= total_pages and
                pages > 0
            )

    def fix_incorrectly_marked_complete_issues(self):
            """NOUÄ‚ FUNCÈšIE: CorecteazÄƒ issue-urile marcate greÈ™it ca complete"""
            print("ğŸ”§ CORECTEZ issue-urile marcate GREÈ˜IT ca complete...")

            fixes_applied = 0

            for item in self.state.get("downloaded_issues", []):
                completed_at = item.get("completed_at")
                last_segment = item.get("last_successful_segment_end", 0)
                total_pages = item.get("total_pages")
                pages = item.get("pages", 0)
                url = item.get("url", "")

                # DetecteazÄƒ issue-uri marcate greÈ™it ca complete
                if (completed_at and
                    pages == 0 and
                    total_pages and
                    last_segment < total_pages):

                    print(f"ğŸš¨ CORECTEZ issue marcat GREÈ˜IT ca complet: {url}")
                    print(f"   Ãnainte: completed_at={completed_at}, pages={pages}")
                    print(f"   Progres real: {last_segment}/{total_pages}")

                    # È˜terge completed_at pentru a-l face parÈ›ial din nou
                    item["completed_at"] = ""
                    item["pages"] = 0  # AsigurÄƒ-te cÄƒ pages rÄƒmÃ¢ne 0

                    fixes_applied += 1
                    print(f"   DupÄƒ: completed_at='', pages=0 (va fi reluat)")

            if fixes_applied > 0:
                print(f"ğŸ”§ CORECTAT {fixes_applied} issue-uri marcate greÈ™it ca complete")
                self._save_state_safe()

                # ActualizeazÄƒ È™i skip URLs
                self._save_skip_urls()
            else:
                print("âœ… Nu am gÄƒsit issue-uri marcate greÈ™it ca complete")

            return fixes_applied

    def get_pending_partial_issues(self):
            """FIXED: ReturneazÄƒ issue-urile parÈ›iale care trebuie continuate (inclusiv cele cu completed_at setat greÈ™it)"""
            pending_partials = []

            for item in self.state.get("downloaded_issues", []):
                url = item.get("url", "").rstrip('/')
                last_segment = item.get("last_successful_segment_end", 0)
                total_pages = item.get("total_pages")
                completed_at = item.get("completed_at", "")
                pages = item.get("pages", 0)

                # Skip URL-urile complet descÄƒrcate
                if url in self.dynamic_skip_urls:
                    continue

                # LOGICÄ‚ CORECTATÄ‚: Un issue este parÈ›ial dacÄƒ:
                # 1. Are progres (last_segment > 0)
                # 2. Are total_pages setat
                # 3. Progresul nu este complet (last_segment < total_pages)
                # 4. SAU are completed_at setat dar pages = 0 (marcat greÈ™it ca complet)

                is_partial = False

                # Caz 1: Issue cu progres dar necomplet
                if (last_segment > 0 and
                    total_pages and
                    last_segment < total_pages):
                    is_partial = True

                # Caz 2: Issue marcat ca complet dar cu pages = 0 (eroare de marcare)
                elif (completed_at and
                      pages == 0 and
                      total_pages and
                      last_segment < total_pages):
                    is_partial = True
                    print(f"ğŸš¨ DETECTAT issue marcat GREÈ˜IT ca complet: {url}")
                    print(f"   completed_at: {completed_at}, pages: {pages}")
                    print(f"   progres real: {last_segment}/{total_pages}")

                if is_partial:
                    pending_partials.append(item)
                    print(f"ğŸ”„ Issue parÈ›ial gÄƒsit: {url} (pagini {last_segment}/{total_pages})")

            return pending_partials

    def _normalize_downloaded_issues(self, raw):
        normalized = []
        for item in raw:
            if isinstance(item, str):
                normalized.append({
                    "url": item.rstrip('/'),
                    "title": "",
                    "subtitle": "",
                    "pages": 0,
                    "completed_at": "",
                    "last_successful_segment_end": 0,
                    "total_pages": None
                })
            elif isinstance(item, dict):
                normalized.append({
                    "url": item.get("url", "").rstrip('/'),
                    "title": item.get("title", ""),
                    "subtitle": item.get("subtitle", ""),
                    "pages": item.get("pages", 0),
                    "completed_at": item.get("completed_at", ""),
                    "last_successful_segment_end": item.get("last_successful_segment_end", 0),
                    "total_pages": item.get("total_pages")
                })
        return normalized

    def _load_state(self):
        """ULTRA SAFE: Nu È™terge NICIODATÄ‚ datele existente"""
        today = datetime.now().strftime("%Y-%m-%d")

        if os.path.exists(self.state_path):
            try:
                with open(self.state_path, "r", encoding="utf-8") as f:
                    loaded = json.load(f)
                    loaded = self._decode_unicode_escapes(loaded)

                # PÄ‚STREAZÄ‚ TOATE issue-urile existente - ZERO È˜TERS
                existing_issues = self._normalize_downloaded_issues(loaded.get("downloaded_issues", []))

                print(f"ğŸ“‹ ÃNCÄ‚RCAT {len(existing_issues)} issue-uri din state.json")

                # GÄƒseÈ™te issue-urile parÈ›iale
                partial_issues = []
                for issue in existing_issues:
                    last_segment = issue.get("last_successful_segment_end", 0)
                    total_pages = issue.get("total_pages")
                    completed_at = issue.get("completed_at", "")

                    if (last_segment > 0 and not completed_at and total_pages and last_segment < total_pages):
                        partial_issues.append(issue)
                        print(f"ğŸ”„ PARÈšIAL: {issue['url']} - {last_segment}/{total_pages} pagini")

                complete_count = len([i for i in existing_issues if i.get("completed_at")])

                # PÄ‚STREAZÄ‚ TOT - doar actualizeazÄƒ data
                self.state = {
                    "date": today,
                    "count": loaded.get("count", complete_count),
                    "downloaded_issues": existing_issues,  # TOATE PÄ‚STRATE
                    "pages_downloaded": loaded.get("pages_downloaded", 0),
                    "recent_links": loaded.get("recent_links", []),
                    "daily_limit_hit": False,
                    "main_collection_completed": loaded.get("main_collection_completed", False),
                    "current_additional_collection_index": loaded.get("current_additional_collection_index", 0)
                }

                print(f"âœ… PÄ‚STRAT TOT: {complete_count} complete, {len(partial_issues)} parÈ›iale")

            except Exception as e:
                print(f"âŒ JSON CORRUPT: {e}")
                print(f"ğŸ› ï¸ RECUPEREZ din backup sau disk...")

                # ÃncearcÄƒ backup
                backup_path = self.state_path + ".backup"
                if os.path.exists(backup_path):
                    print(f"ğŸ”„ Restabilesc din backup...")
                    shutil.copy2(backup_path, self.state_path)
                    return self._load_state()  # Recursiv cu backup

                # Altfel Ã®ncepe gol dar SCANEAZÄ‚ DISK-UL
                print(f"ğŸ” SCANEZ DISK-UL pentru recuperare...")
                self.state = {
                    "date": today,
                    "count": 0,
                    "downloaded_issues": [],
                    "pages_downloaded": 0,
                    "recent_links": [],
                    "daily_limit_hit": False,
                    "main_collection_completed": False,
                    "current_additional_collection_index": 0
                }
        else:
            print(f"ğŸ“„ Nu existÄƒ state.json")
            self.state = {
                "date": today,
                "count": 0,
                "downloaded_issues": [],
                "pages_downloaded": 0,
                "recent_links": [],
                "daily_limit_hit": False,
                "main_collection_completed": False,
                "current_additional_collection_index": 0
            }

        self._save_state()

    def _save_state_safe(self):
        """SAFE: SalveazÄƒ starea doar dacÄƒ existÄƒ modificÄƒri, pÄƒstreazÄƒ backup"""
        try:
            # CreeazÄƒ backup Ã®nainte de salvare
            if os.path.exists(self.state_path):
                backup_path = self.state_path + ".backup"
                shutil.copy2(self.state_path, backup_path)

            with open(self.state_path, "w", encoding="utf-8") as f:
                json.dump(self.state, f, indent=2, ensure_ascii=False)

        except Exception as e:
            print(f"âš  Nu am putut salva state-ul: {e}")
            # ÃncearcÄƒ sÄƒ restabileascÄƒ din backup
            backup_path = self.state_path + ".backup"
            if os.path.exists(backup_path):
                print(f"ğŸ”„ Ãncerc sÄƒ restabilesc din backup...")
                try:
                    shutil.copy2(backup_path, self.state_path)
                    print(f"âœ… State restabilit din backup")
                except:
                    print(f"âŒ Nu am putut restabili din backup")

    def _save_state(self):
        """WRAPPER: FoloseÈ™te salvarea safe"""
        self._save_state_safe()

    def fix_existing_json(self):
        """FuncÈ›ie temporarÄƒ pentru a repara caracterele din JSON existent"""
        if os.path.exists(self.state_path):
            with open(self.state_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            data = self._decode_unicode_escapes(data)

            with open(self.state_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            print("âœ… JSON reparat cu caractere romÃ¢neÈ™ti")

    def remaining_quota(self):
        return max(0, DAILY_LIMIT - self.state.get("count", 0))

    def _update_partial_issue_progress(self, issue_url, last_successful_segment_end, total_pages=None, title=None, subtitle=None):
        """FIXED: Previne dublurile - verificÄƒ È™i dupÄƒ title dacÄƒ URL-ul nu se potriveÈ™te"""
        normalized = issue_url.rstrip('/')
        updated = False

        # STEP 1: CautÄƒ dupÄƒ URL exact
        for i, item in enumerate(self.state.setdefault("downloaded_issues", [])):
            if item["url"] == normalized:
                # ACTUALIZEAZÄ‚ issue-ul existent
                if last_successful_segment_end > item.get("last_successful_segment_end", 0):
                    item["last_successful_segment_end"] = last_successful_segment_end

                if total_pages is not None and not item.get("total_pages"):
                    item["total_pages"] = total_pages

                if title and not item.get("title"):
                    item["title"] = title

                if subtitle and not item.get("subtitle"):
                    item["subtitle"] = subtitle

                # MutÄƒ la Ã®nceput pentru prioritate
                updated_item = self.state["downloaded_issues"].pop(i)
                self.state["downloaded_issues"].insert(0, updated_item)
                updated = True
                print(f"ğŸ”„ ACTUALIZAT progres pentru: {normalized} â†’ {last_successful_segment_end} pagini")
                break

        # STEP 2: DacÄƒ nu gÄƒseÈ™ti dupÄƒ URL, cautÄƒ dupÄƒ title (prevenire dubluri)
        if not updated and title:
            for i, item in enumerate(self.state["downloaded_issues"]):
                if item.get("title") == title and not item["url"].startswith("http"):
                    # GÄ‚SIT dublu cu title ca URL - È™terge-l!
                    print(f"ğŸ—‘ï¸ È˜TERG DUBLU GREÈ˜IT: {item['url']} (era title Ã®n loc de URL)")
                    self.state["downloaded_issues"].pop(i)
                    break

        # STEP 3: Doar dacÄƒ nu existÄƒ deloc, creeazÄƒ nou
        if not updated:
            # VALIDEAZÄ‚ cÄƒ URL-ul e corect
            if not normalized.startswith("https://"):
                print(f"âŒ URL INVALID: {normalized} - nu creez issue nou!")
                return

            new_issue = {
                "url": normalized,
                "title": title or "",
                "subtitle": subtitle or "",
                "pages": 0,
                "completed_at": "",
                "last_successful_segment_end": last_successful_segment_end,
                "total_pages": total_pages
            }
            self.state["downloaded_issues"].insert(0, new_issue)
            print(f"â• ADÄ‚UGAT issue nou Ã®n progres: {normalized}")

        self._save_state_safe()
        print(f"ğŸ’¾ Progres salvat SAFE: {normalized} - pagini {last_successful_segment_end}/{total_pages or '?'}")

    def mark_issue_done(self, issue_url, pages_count, title=None, subtitle=None, total_pages=None):
        """ULTRA SAFE: VerificÄƒri stricte Ã®nainte de a marca ca terminat"""
        normalized = issue_url.rstrip('/')
        now_iso = datetime.now().isoformat(timespec="seconds")

        # ===== VERIFICÄ‚RI ULTRA SAFE ÃNAINTE DE A MARCA CA TERMINAT =====

        print(f"ğŸ”’ VERIFICÄ‚RI ULTRA SAFE pentru marcarea ca terminat: {normalized}")

        # VERIFICARE 1: total_pages trebuie sÄƒ fie rezonabil
        if total_pages is None or total_pages <= 0:
            print(f"âŒ BLOCARE SAFETY: total_pages invalid ({total_pages}) pentru {normalized}")
            print(f"ğŸ›¡ï¸ NU MARCHEZ ca terminat fÄƒrÄƒ total_pages valid!")
            print(f"ğŸ”„ MarcheazÄƒ ca parÈ›ial cu progres {pages_count}")

            # MarcheazÄƒ ca parÈ›ial, NU ca terminat
            self._update_partial_issue_progress(
                normalized, pages_count, total_pages=None, title=title, subtitle=subtitle
            )
            return

        # VERIFICARE 2: pages_count trebuie sÄƒ fie aproape de total_pages
        completion_percentage = (pages_count / total_pages) * 100

        if completion_percentage < 95:  # Trebuie sÄƒ fie cel puÈ›in 95% complet
            print(f"âŒ BLOCARE SAFETY: Progres insuficient pentru {normalized}")
            print(f"ğŸ“Š Progres: {pages_count}/{total_pages} ({completion_percentage:.1f}%)")
            print(f"ğŸ›¡ï¸ Trebuie cel puÈ›in 95% pentru a marca ca terminat!")
            print(f"ğŸ”„ MarcheazÄƒ ca parÈ›ial Ã®n loc de terminat")

            # MarcheazÄƒ ca parÈ›ial, NU ca terminat
            self._update_partial_issue_progress(
                normalized, pages_count, total_pages=total_pages, title=title, subtitle=subtitle
            )
            return

        # VERIFICARE 3: DetecteazÄƒ batch size suspicious
        if pages_count < 100 and total_pages > 500:
            print(f"âŒ BLOCARE SAFETY: Progres suspect de mic pentru {normalized}")
            print(f"ğŸ“Š {pages_count} pagini par sÄƒ fie doar primul batch din {total_pages}")
            print(f"ğŸ›¡ï¸ Probabil s-a oprit prematur, NU marchez ca terminat")

            # MarcheazÄƒ ca parÈ›ial
            self._update_partial_issue_progress(
                normalized, pages_count, total_pages=total_pages, title=title, subtitle=subtitle
            )
            return

        # VERIFICARE 4: VerificÄƒ dacÄƒ pages_count pare sÄƒ fie doar primul segment
        if total_pages >= 1000 and pages_count < 100:
            print(f"âŒ BLOCARE SAFETY: {pages_count} pagini din {total_pages} pare primul segment")
            print(f"ğŸ›¡ï¸ NU marchez issues mari ca terminate cu progres atÃ¢t de mic")

            # MarcheazÄƒ ca parÈ›ial
            self._update_partial_issue_progress(
                normalized, pages_count, total_pages=total_pages, title=title, subtitle=subtitle
            )
            return

        # ===== TOATE VERIFICÄ‚RILE AU TRECUT - SAFE SÄ‚ MARCHEZ CA TERMINAT =====

        print(f"âœ… TOATE VERIFICÄ‚RILE ULTRA SAFE trecute pentru {normalized}")
        print(f"ğŸ“Š Progres: {pages_count}/{total_pages} ({completion_percentage:.1f}%)")
        print(f"ğŸ¯ Marchez ca TERMINAT")

        # ContinuÄƒ cu logica originalÄƒ de marcare ca terminat...
        existing = None
        existing_index = -1

        # CÄ‚UTARE ÃMBUNÄ‚TÄ‚ÈšITÄ‚: Ã®ncearcÄƒ mai multe variante de URL
        search_variants = [
            normalized,
            normalized + '/',
            normalized.replace('https://', 'http://'),
            normalized.replace('http://', 'https://')
        ]

        for i, item in enumerate(self.state.setdefault("downloaded_issues", [])):
            item_url = item.get("url", "").rstrip('/')
            if item_url in search_variants or normalized in [item_url, item_url + '/']:
                existing = item
                existing_index = i
                print(f"ğŸ” GÄ‚SIT issue existent la index {i}: {item_url}")
                break

        # CreeazÄƒ record-ul de completare
        completion_data = {
            "pages": pages_count,
            "completed_at": now_iso,
            "last_successful_segment_end": pages_count,
            "total_pages": total_pages  # SETEAZÄ‚ ÃNTOTDEAUNA!
        }

        # AdaugÄƒ title/subtitle doar dacÄƒ nu existÄƒ sau sunt goale
        if title:
            completion_data["title"] = title
        if subtitle:
            completion_data["subtitle"] = subtitle

        if existing:
            # ÃMBOGÄ‚ÈšEÈ˜TE issue-ul existent
            for key, value in completion_data.items():
                if key in ["title", "subtitle"]:
                    if not existing.get(key):
                        existing[key] = value
                else:
                    existing[key] = value

            # SCOATE din poziÈ›ia curentÄƒ
            updated_issue = self.state["downloaded_issues"].pop(existing_index)
            print(f"âœ… ACTUALIZAT È™i SCOS din poziÈ›ia {existing_index}: {normalized}")
        else:
            # CreeazÄƒ issue nou complet
            updated_issue = {
                "url": normalized,
                "title": title or "",
                "subtitle": subtitle or "",
                **completion_data
            }
            print(f"â• CREAT issue nou: {normalized}")

        # INSEREAZÄ‚ ÃN POZIÈšIA CRONOLOGICÄ‚ CORECTÄ‚
        # GÄƒseÈ™te primul issue cu completed_at mai vechi decÃ¢t cel curent
        insert_position = 0

        # Sari peste issue-urile parÈ›iale (care sunt mereu primele)
        while (insert_position < len(self.state["downloaded_issues"]) and
               not self.state["downloaded_issues"][insert_position].get("completed_at")):
            insert_position += 1

        # GÄƒseÈ™te poziÈ›ia corectÄƒ Ã®ntre issue-urile complete (sortate cronologic descendent)
        while insert_position < len(self.state["downloaded_issues"]):
            other_completed_at = self.state["downloaded_issues"][insert_position].get("completed_at", "")
            if other_completed_at and other_completed_at < now_iso:
                break
            insert_position += 1

        # InsereazÄƒ Ã®n poziÈ›ia cronologicÄƒ corectÄƒ
        self.state["downloaded_issues"].insert(insert_position, updated_issue)
        print(f"ğŸ“… INSERAT Ã®n poziÈ›ia CRONOLOGICÄ‚ {insert_position} (dupÄƒ issue-urile parÈ›iale È™i Ã®n ordine de completed_at)")

        # ActualizeazÄƒ contoarele SAFE
        completed_count = len([i for i in self.state["downloaded_issues"] if i.get("completed_at")])
        self.state["count"] = max(self.state.get("count", 0), completed_count)

        # ActualizeazÄƒ pages_downloaded SAFE
        current_pages = self.state.get("pages_downloaded", 0)
        self.state["pages_downloaded"] = current_pages + pages_count

        # AdaugÄƒ Ã®n recent_links (pÄƒstreazÄƒ max 10)
        recent_entry = {
            "url": normalized,
            "title": (existing and existing.get("title")) or title or "",
            "subtitle": (existing and existing.get("subtitle")) or subtitle or "",
            "pages": pages_count,
            "timestamp": now_iso
        }
        recent_links = self.state.setdefault("recent_links", [])
        recent_links.insert(0, recent_entry)
        self.state["recent_links"] = recent_links[:10]

        # ReseteazÄƒ flag-ul de limitÄƒ
        self.state["daily_limit_hit"] = False

        # AdaugÄƒ Ã®n skip URLs
        self.dynamic_skip_urls.add(normalized)

        self._save_state_safe()
        self._save_skip_urls()

        print(f"âœ… Issue marcat ca terminat cu SORTARE CRONOLOGICÄ‚ CORECTÄ‚: {normalized}")
        print(f"ğŸ“Š Detalii: {pages_count} pagini, total_pages: {total_pages}")
        print(f"ğŸ“Š Total complet: {self.state['count']}, Total pagini: {self.state['pages_downloaded']}")
        print(f"ğŸ“… Plasat Ã®n poziÈ›ia cronologicÄƒ {insert_position} din {len(self.state['downloaded_issues'])}")

    def mark_collection_complete(self, collection_url):
        """MarcheazÄƒ o colecÈ›ie ca fiind complet procesatÄƒ Ã®n skip_urls.json"""
        try:
            normalized_collection = collection_url.rstrip('/')

            # AdaugÄƒ Ã®n dynamic skip URLs
            self.dynamic_skip_urls.add(normalized_collection)

            # SalveazÄƒ Ã®n skip_urls.json cu un marker special pentru colecÈ›ii
            skip_data = {}
            if os.path.exists(self.skip_urls_path):
                with open(self.skip_urls_path, "r", encoding="utf-8") as f:
                    skip_data = json.load(f)

            completed_collections = skip_data.get("completed_collections", [])
            if normalized_collection not in completed_collections:
                completed_collections.append(normalized_collection)
                skip_data["completed_collections"] = completed_collections
                skip_data["last_updated"] = datetime.now().isoformat()

                with open(self.skip_urls_path, "w", encoding="utf-8") as f:
                    json.dump(skip_data, f, indent=2, ensure_ascii=False)

                print(f"âœ… ColecÈ›ia marcatÄƒ ca completÄƒ: {normalized_collection}")
        except Exception as e:
            print(f"âš  Eroare la marcarea colecÈ›iei complete: {e}")

    def setup_chrome_driver(self):
        try:
            print("ğŸ”§ IniÈ›ializare WebDriver â€“ Ã®ncerc conectare la instanÈ›a Chrome existentÄƒ via remote debugging...")
            chrome_options = Options()
            chrome_options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")
            prefs = {
                "download.default_directory": os.path.abspath(self.download_dir),
                "download.prompt_for_download": False,
                "download.directory_upgrade": True,
                "safebrowsing.enabled": True,
            }
            chrome_options.add_experimental_option("prefs", prefs)
            try:
                self.driver = webdriver.Chrome(options=chrome_options)
                self.wait = WebDriverWait(self.driver, self.timeout)
                self.attached_existing = True
                print("âœ… Conectat la instanÈ›a Chrome existentÄƒ cu succes.")
                return True
            except WebDriverException as e:
                print(f"âš  Conexiune la Chrome existent eÈ™uat ({e}); pornesc o instanÈ›Äƒ nouÄƒ.")
                chrome_options = Options()
                chrome_options.add_experimental_option("prefs", prefs)
                chrome_options.add_argument("--no-sandbox")
                chrome_options.add_argument("--disable-dev-shm-usage")
                chrome_options.add_argument("--disable-gpu")
                chrome_options.add_argument("--window-size=1920,1080")
                chrome_options.add_argument("--incognito")
                self.driver = webdriver.Chrome(options=chrome_options)
                self.wait = WebDriverWait(self.driver, self.timeout)
                self.attached_existing = False
                print("âœ… Chrome nou pornit cu succes.")
                return True
        except WebDriverException as e:
            print(f"âŒ Eroare la iniÈ›ializarea WebDriver-ului: {e}")
            return False

    def navigate_to_page(self, url):
        try:
            # VERIFICÄ‚ ÃNTÃ‚I DACÄ‚ BROWSER-UL MAI EXISTÄ‚
            try:
                _ = self.driver.current_url
            except:
                print("âš  Browser Ã®nchis, Ã®ncerc reconectare...")
                if not self.setup_chrome_driver():
                    print("âŒ Nu pot reconecta browser-ul")
                    return False

            print(f"ğŸŒ Navighez cÄƒtre: {url}")
            self.driver.get(url)
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'body')))
            print("âœ… Pagina Ã®ncÄƒrcatÄƒ.")
            return True
        except Exception as e:
            print(f"âŒ Eroare la navigare sau Ã®ncÄƒrcare: {e}")
            # ÃNCEARCÄ‚ O RECONECTARE CA ULTIM RESORT
            try:
                print("ğŸ”„ Ãncerc reconectare de urgenÈ›Äƒ...")
                if self.setup_chrome_driver():
                    self.driver.get(url)
                    self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'body')))
                    print("âœ… Reconectat È™i navigat cu succes!")
                    return True
            except:
                pass
            return False

    def get_issue_metadata(self):
        title = ""
        subtitle = ""
        try:
            breadcrumb = self.driver.find_element(By.CSS_SELECTOR, "li.breadcrumb-item.active")
            try:
                sub_elem = breadcrumb.find_element(By.CSS_SELECTOR, "#pdfview-pdfcontents span")
                subtitle = sub_elem.text.strip()
            except Exception:
                subtitle = ""
            raw = breadcrumb.text.strip()
            if subtitle and subtitle in raw:
                title = raw.replace(subtitle, "").strip()
            else:
                title = raw
        except Exception:
            pass
        return title, subtitle

    def get_total_pages(self, max_attempts=5, delay_between=1.0):
        """FIXED: DetecteazÄƒ corect numÄƒrul total de pagini"""
        for attempt in range(1, max_attempts + 1):
            try:
                # Metoda 1: CautÄƒ pattern-ul "numÄƒr / total" Ã®n mai multe locuri
                page_patterns = [
                    r'(\d+)\s*/\s*(\d+)',  # "249 / 1565"
                    r'/\s*(\d+)',          # "/ 1565"
                    r'of\s+(\d+)',         # "of 1565"
                ]

                # CautÄƒ Ã®n toate textele de pe paginÄƒ
                all_texts = self.driver.find_elements(By.XPATH, "//*[contains(text(), '/') or contains(text(), 'of')]")

                for el in all_texts:
                    text = el.text.strip()
                    for pattern in page_patterns:
                        matches = re.findall(pattern, text)
                        if matches:
                            if pattern == page_patterns[0]:  # "numÄƒr / total"
                                current, total = matches[0]
                                total = int(total)
                                print(f"âœ… TOTAL PAGINI detectat din '{text}': {total} (curent: {current})")
                                return total
                            else:  # "/ total" sau "of total"
                                total = int(matches[0])
                                print(f"âœ… TOTAL PAGINI detectat din '{text}': {total}")
                                return total

                # Metoda 2: JavaScript pentru cÄƒutare mai profundÄƒ
                js_result = self.driver.execute_script(r"""
                    const patterns = [
                        /(\d+)\s*\/\s*(\d+)/g,
                        /\/\s*(\d+)/g,
                        /of\s+(\d+)/g
                    ];

                    const walker = document.createTreeWalker(document.body, NodeFilter.SHOW_TEXT);
                    while(walker.nextNode()) {
                        const text = walker.currentNode.nodeValue;
                        for (let pattern of patterns) {
                            const matches = [...text.matchAll(pattern)];
                            if (matches.length > 0) {
                                const match = matches[0];
                                if (match.length === 3) {  // "numÄƒr / total"
                                    return {text: text, total: parseInt(match[2]), current: parseInt(match[1])};
                                } else {  // "/ total"
                                    return {text: text, total: parseInt(match[1])};
                                }
                            }
                        }
                    }
                    return null;
                """)

                if js_result:
                    total = js_result['total']
                    current = js_result.get('current', 0)
                    text = js_result['text']
                    print(f"âœ… TOTAL PAGINI detectat prin JS din '{text}': {total} (curent: {current})")
                    return total

                print(f"âš  ({attempt}) Nu am gÄƒsit Ã®ncÄƒ numÄƒrul total de pagini, reÃ®ncerc Ã®n {delay_between}s...")
                time.sleep(delay_between)

            except Exception as e:
                print(f"âš  ({attempt}) Eroare Ã®n get_total_pages: {e}")
                time.sleep(delay_between)

        print("âŒ Nu s-a reuÈ™it extragerea numÄƒrului total de pagini dupÄƒ multiple Ã®ncercÄƒri.")
        return 0

    def open_save_popup(self):
        try:
            try:
                self.wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR, 'div.MuiDialog-container')))
            except Exception:
                self.driver.switch_to.active_element.send_keys(Keys.ESCAPE)
                time.sleep(0.5)
            svg = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'svg[data-testid="SaveAltIcon"]')))
            button = svg
            if svg.tag_name.lower() == "svg":
                try:
                    button = svg.find_element(By.XPATH, "./ancestor::button")
                except Exception:
                    pass
            for attempt in range(1, 4):
                try:
                    button.click()
                    return True
                except ElementClickInterceptedException:
                    print(f"âš  click interceptat (Ã®ncercarea {attempt}), trimit ESC È™i reiau...")
                    self.driver.switch_to.active_element.send_keys(Keys.ESCAPE)
                    time.sleep(1)
                    continue
            print("âŒ Nu am reuÈ™it sÄƒ dau click pe butonul de deschidere a popup-ului dupÄƒ retry-uri.")
            return False
        except Exception as e:
            print(f"âŒ Nu am reuÈ™it sÄƒ deschid popup-ul de salvare: {e}")
            return False

    def fill_and_save_range(self, start, end):
        try:
            first_input = self.wait.until(EC.presence_of_element_located((By.ID, "first page")))
            print("â³ AÈ™tept 2s Ã®nainte de a completa primul input...")
            time.sleep(2)
            first_input.send_keys(Keys.CONTROL + "a")
            first_input.send_keys(str(start))
            print(f"âœï¸ Am introdus primul numÄƒr: {start}")
            print("â³ AÈ™tept 2s Ã®nainte de a completa al doilea input...")
            time.sleep(2)
            last_input = self.wait.until(EC.presence_of_element_located((By.ID, "last page")))
            last_input.send_keys(Keys.CONTROL + "a")
            last_input.send_keys(str(end))
            print(f"âœï¸ Am introdus al doilea numÄƒr: {end}")
            print("â³ AÈ™tept 2s Ã®nainte de a apÄƒsa butonul de salvare...")
            time.sleep(2)
            save_btn = self.wait.until(EC.element_to_be_clickable((
                By.XPATH,
                '//button[.//text()[contains(normalize-space(.), "SalvaÈ›i") or contains(normalize-space(.), "Save")]]'
            )))
            save_btn.click()
            print(f"âœ… Segmentul {start}-{end} salvat.")
            return True
        except Exception as e:
            print(f"âŒ Eroare la completarea/salvarea intervalului {start}-{end}: {e}")
            return False

    def check_daily_limit_in_all_windows(self, set_flag=True):
        """VerificÄƒ mesajul de limitÄƒ zilnicÄƒ Ã®n toate ferestrele deschise"""
        current_window = self.driver.current_window_handle
        limit_reached = False

        try:
            all_handles = self.driver.window_handles
            for handle in all_handles:
                try:
                    self.driver.switch_to.window(handle)
                    body_text = self.driver.find_element(By.TAG_NAME, "body").text

                    if ("Daily download limit reached" in body_text or
                        "Terms and conditions" in body_text):
                        print(f"âš  Limita zilnicÄƒ detectatÄƒ Ã®n fereastra: {handle}")
                        limit_reached = True

                        if handle != current_window and len(all_handles) > 1:
                            print(f"ğŸ—™ Ãnchid fereastra cu limita zilnicÄƒ: {handle}")
                            self.driver.close()
                        break

                except Exception as e:
                    continue

            try:
                if current_window in self.driver.window_handles:
                    self.driver.switch_to.window(current_window)
                elif self.driver.window_handles:
                    self.driver.switch_to.window(self.driver.window_handles[0])
            except Exception:
                pass

        except Exception as e:
            print(f"âš  Eroare la verificarea ferestrelor: {e}")

        if limit_reached and set_flag:
            self.state["daily_limit_hit"] = True
            self._save_state()

        return limit_reached

    def check_for_daily_limit_popup(self):
        """VerificÄƒ dacÄƒ s-a deschis o filÄƒ nouÄƒ cu mesajul de limitÄƒ zilnicÄƒ dupÄƒ descÄƒrcare"""
        try:
            current_handles = set(self.driver.window_handles)

            # VerificÄƒ toate filele deschise pentru mesajul de limitÄƒ
            for handle in current_handles:
                try:
                    self.driver.switch_to.window(handle)
                    body_text = self.driver.find_element(By.TAG_NAME, "body").text.strip()

                    if "Daily download limit reached" in body_text:
                        print(f"ğŸ›‘ LIMITÄ‚ ZILNICÄ‚ DETECTATÄ‚ Ã®n filÄƒ nouÄƒ: {handle}")
                        print(f"ğŸ“„ ConÈ›inut filÄƒ: {body_text}")

                        # Ãnchide fila cu limita
                        self.driver.close()

                        # Revine la prima filÄƒ disponibilÄƒ
                        if self.driver.window_handles:
                            self.driver.switch_to.window(self.driver.window_handles[0])

                        # SeteazÄƒ flag-ul È™i opreÈ™te procesarea
                        self.state["daily_limit_hit"] = True
                        self._save_state()
                        return True

                except Exception as e:
                    continue

            return False

        except Exception as e:
            print(f"âš  Eroare la verificarea popup-ului de limitÄƒ: {e}")
            return False

    def save_page_range(self, start, end, retries=1):
            """FIXED: VerificÄƒ URL-ul Ã®nainte de fiecare descÄƒrcare + verificÄƒ limita zilnicÄƒ"""
            for attempt in range(1, retries + 2):
                print(f"ğŸ”„ Ãncep segmentul {start}-{end}, Ã®ncercarea {attempt}")

                # VERIFICARE CRITICÄ‚: Suntem pe documentul corect?
                try:
                    current_url = self.driver.current_url
                    if self.current_issue_url not in current_url:
                        print(f"ğŸš¨ EROARE: Browser-ul a navigat la URL greÈ™it!")
                        print(f"   AÈ™teptat: {self.current_issue_url}")
                        print(f"   Actual: {current_url}")
                        print(f"ğŸ”„ Renavigez la documentul corect...")

                        if not self.navigate_to_page(self.current_issue_url):
                            print(f"âŒ Nu pot renaviga la {self.current_issue_url}")
                            return False

                        time.sleep(3)  # AÈ™teaptÄƒ Ã®ncÄƒrcarea completÄƒ
                        print(f"âœ… Renavigat cu succes la documentul corect")
                except Exception as e:
                    print(f"âš  Eroare la verificarea URL-ului: {e}")

                if not self.open_save_popup():
                    print(f"âš  EÈ™ec la deschiderea popup-ului pentru {start}-{end}")
                    time.sleep(1)
                    continue

                success = self.fill_and_save_range(start, end)
                if success:
                    print("â³ AÈ™tept 5s pentru finalizarea descÄƒrcÄƒrii segmentului...")
                    time.sleep(5)

                    # VERIFICÄ‚ LIMITA ZILNICÄ‚ IMEDIAT DUPÄ‚ DESCÄ‚RCARE
                    if self.check_for_daily_limit_popup():
                        print(f"ğŸ›‘ OPRIRE INSTANT - LimitÄƒ zilnicÄƒ detectatÄƒ dupÄƒ segmentul {start}-{end}")
                        return False

                    print(f"âœ… Segmentul {start}-{end} descÄƒrcat cu succes")
                    return True
                else:
                    print(f"âš  Retry pentru segmentul {start}-{end}")
                    time.sleep(1)
            print(f"âŒ RenunÈ› la segmentul {start}-{end} dupÄƒ {retries+1} Ã®ncercÄƒri.")
            return False

    def save_all_pages_in_batches(self, resume_from=1):
        """FIXED: OpreÈ™te instant la detectarea limitei zilnice È™i NU marcheazÄƒ ca terminat dacÄƒ e incomplet"""
        total = self.get_total_pages()
        if total <= 0:
            print("âš  Nu am obÈ›inut numÄƒrul total de pagini; nu pot continua.")
            return 0, False

        print(f"ğŸ¯ TOTAL PAGINI DETECTAT: {total}")

        # Verificare de siguranÈ›Äƒ pentru total_pages suspect
        if total < 50:
            print(f"âš  ATENÈšIE: total_pages pare suspect de mic ({total})")
            print(f"ğŸ”„ Ãncerc din nou detecÈ›ia...")
            time.sleep(3)
            total_retry = self.get_total_pages()
            if total_retry > total:
                total = total_retry
                print(f"âœ… Total actualizat la: {total}")

        bs = self.batch_size
        segments = []

        if resume_from == 1:
            first_end = min(bs - 1, total)
            if first_end >= 1:
                segments.append((1, first_end))
            current = bs
        else:
            current = resume_from

        while current <= total:
            end = min(current + bs - 1, total)
            segments.append((current, end))
            current += bs

        last_successful_page = resume_from - 1

        print(f"ğŸ¯ FOCUSEZ PE ACEST ISSUE: Voi descÄƒrca {len(segments)} segmente fÄƒrÄƒ Ã®ntreruperi")

        for (start, end) in segments:
            print(f"ğŸ“¦ Procesez segmentul {start}-{end}")
            result = self.save_page_range(start, end, retries=1)

            if not result:
                # VERIFICÄ‚ DACÄ‚ EÈ˜ECUL E DIN CAUZA LIMITEI ZILNICE
                if self.state.get("daily_limit_hit", False):
                    print(f"ğŸ›‘ OPRIRE INSTANT - LimitÄƒ zilnicÄƒ atinsÄƒ la segmentul {start}-{end}")
                    return last_successful_page, True  # ReturneazÄƒ TRUE pentru limitÄƒ zilnicÄƒ

                print(f"âŒ EÈ™ec persistent la segmentul {start}-{end}")
                print(f"ğŸ“Š PROGRES PARÈšIAL: {last_successful_page}/{total} pagini")
                print(f"ğŸ”„ Issue-ul va rÄƒmÃ¢ne PARÈšIAL pentru continuare ulterioarÄƒ")
                return last_successful_page, False  # EÈ™ec - nu e terminat

            last_successful_page = end
            self._update_partial_issue_progress(self.current_issue_url, end, total_pages=total)
            print(f"âœ… Progres real salvat: pagini pÃ¢nÄƒ la {end}")
            time.sleep(1)

        print("ğŸ¯ Toate segmentele au fost procesate pentru acest issue.")
        return total, False

    def extract_issue_links_from_collection(self):
        try:
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.list-group')))
            anchors = self.driver.find_elements(By.CSS_SELECTOR, 'li.list-group-item a[href^="/view/"], li.list-group-item a[href^="/en/view/"], li.list-group-item a[href^="/ro/view/"]')
            links = []
            for a in anchors:
                href = a.get_attribute("href")
                if href and '/view/' in href:
                    normalized = href.split('?')[0].rstrip('/')
                    links.append(normalized)
            unique = []
            seen = set()
            for l in links:
                if l not in seen:
                    seen.add(l)
                    unique.append(l)
            print(f"ğŸ”— Am gÄƒsit {len(unique)} linkuri de issue Ã®n colecÈ›ie.")
            return unique
        except Exception as e:
            print(f"âŒ Eroare la extragerea linkurilor din colecÈ›ie: {e}")
            return []

    def extract_page_range_from_filename(self, filename):
        """Extrage range-ul de pagini din numele fiÈ™ierului pentru sortare corectÄƒ"""
        match = re.search(r'__pages(\d+)-(\d+)\.pdf', filename)
        if match:
            start_page = int(match.group(1))
            end_page = int(match.group(2))
            return (start_page, end_page)
        return (0, 0)

    def copy_and_combine_issue_pdfs(self, issue_url: str, issue_title: str):
        """
        FIXED: MUTÄ‚ fiÈ™ierele Ã®n folder È™i le combinÄƒ (nu mai pÄƒstreazÄƒ pe D:)
        ADDED: Face backup Ã®n g:Temporare Ã®nainte de procesare
        """
        issue_id = issue_url.rstrip('/').split('/')[-1]
        folder_name = self._safe_folder_name(issue_title or issue_id)
        dest_dir = os.path.join(self.download_dir, folder_name)
        os.makedirs(dest_dir, exist_ok=True)

        # DIRECTORUL DE BACKUP
        backup_base_dir = r"g:\Temporare"
        backup_dir = os.path.join(backup_base_dir, folder_name)

        print(f"ğŸ“ Procesez PDF-urile pentru '{issue_title}' cu ID '{issue_id}'")

        # â³ AÈ˜TEAPTÄ‚ CA TOATE FIÈ˜IERELE SÄ‚ FIE COMPLET DESCÄ‚RCATE
        print("â³ AÈ™tept 10 secunde ca toate fiÈ™ierele sÄƒ se termine de descÄƒrcat...")
        time.sleep(10)

        # PASUL 1: GÄƒseÈ™te TOATE fiÈ™ierele pentru acest issue
        all_segments = self.get_all_pdf_segments_for_issue(issue_url)

        if not all_segments:
            print(f"â„¹ï¸ Nu am gÄƒsit fiÈ™iere PDF pentru '{issue_title}' cu ID '{issue_id}'.")
            return

        print(f"ğŸ” Am gÄƒsit {len(all_segments)} fiÈ™iere PDF pentru '{issue_id}':")
        for seg in all_segments:
            print(f"   ğŸ“„ {seg['filename']} (pagini {seg['start']}-{seg['end']})")

        # PASUL 1.5: CREEAZÄ‚ BACKUP-UL ÃNAINTE DE PROCESARE
        print(f"ğŸ’¾ Creez backup Ã®n: {backup_dir}")
        try:
            os.makedirs(backup_dir, exist_ok=True)
            backup_success = True
            backup_size_total = 0

            for seg in all_segments:
                src = seg['path']
                backup_dst = os.path.join(backup_dir, seg['filename'])

                try:
                    shutil.copy2(src, backup_dst)  # copy2 pÄƒstreazÄƒ È™i metadata
                    file_size = os.path.getsize(backup_dst)
                    backup_size_total += file_size
                    print(f"ğŸ’¾ BACKUP: {seg['filename']} â†’ g:\\Temporare\\{folder_name}\\")
                except Exception as e:
                    print(f"âš  EROARE backup pentru {seg['filename']}: {e}")
                    backup_success = False

            backup_size_mb = backup_size_total / (1024 * 1024)
            if backup_success:
                print(f"âœ… BACKUP COMPLET: {len(all_segments)} fiÈ™iere ({backup_size_mb:.2f} MB) Ã®n {backup_dir}")
            else:
                print(f"âš  BACKUP PARÈšIAL: Unele fiÈ™iere nu au putut fi copiate Ã®n backup")

        except Exception as e:
            print(f"âŒ EROARE la crearea backup-ului: {e}")
            print(f"ğŸ›¡ï¸ OPRESC PROCESAREA pentru siguranÈ›Äƒ - fiÈ™ierele rÄƒmÃ¢n pe D:\\")
            return

        # PASUL 2: MUTÄ‚ (nu copiazÄƒ) TOATE fiÈ™ierele Ã®n folder (DOAR DUPÄ‚ backup SUCCESS)
        moved_files = []
        for seg in all_segments:
            src = seg['path']
            dst = os.path.join(dest_dir, seg['filename'])
            try:
                shutil.move(src, dst)  # MOVE Ã®n loc de COPY
                moved_files.append(dst)
                print(f"ğŸ“„ MUTAT: {seg['filename']} â†’ {folder_name}/")
            except Exception as e:
                print(f"âš  Nu am reuÈ™it sÄƒ mut {seg['filename']}: {e}")

        if not moved_files:
            print(f"âŒ Nu am reuÈ™it sÄƒ mut niciun fiÈ™ier pentru '{issue_title}'.")
            return

        print(f"ğŸ“ Toate {len(moved_files)} PDF-urile pentru '{issue_title}' au fost MUTATE Ã®n '{dest_dir}'.")
        print(f"ğŸ’¾ BACKUP SIGUR gÄƒsit Ã®n: {backup_dir}")

        # PASUL 3: CombinÄƒ PDF-urile Ã®n ordinea corectÄƒ
        output_file = os.path.join(dest_dir, f"{folder_name}.pdf")

        try:
            if len(moved_files) > 1:
                print(f"ğŸ”— Combinez {len(moved_files)} fiÈ™iere PDF Ã®n ordinea corectÄƒ...")

                # SorteazÄƒ fiÈ™ierele dupÄƒ range-ul de pagini
                files_with_ranges = []
                for file_path in moved_files:
                    filename = os.path.basename(file_path)
                    start_page, end_page = self.extract_page_range_from_filename(filename)
                    files_with_ranges.append((start_page, end_page, file_path))

                # SorteazÄƒ dupÄƒ pagina de Ã®nceput
                files_with_ranges.sort(key=lambda x: x[0])
                sorted_files = [x[2] for x in files_with_ranges]

                # AfiÈ™eazÄƒ ordinea de combinare
                print("ğŸ“‹ Ordinea de combinare:")
                for start, end, path in files_with_ranges:
                    filename = os.path.basename(path)
                    print(f"   ğŸ“„ {filename} (pagini {start}-{end})")

                from PyPDF2 import PdfMerger
                merger = PdfMerger()

                for pdf_path in sorted_files:
                    try:
                        merger.append(pdf_path)
                        filename = os.path.basename(pdf_path)
                        print(f"   âœ… AdÄƒugat Ã®n ordine: {filename}")
                    except Exception as e:
                        print(f"   âš  Eroare la adÄƒugarea {pdf_path}: {e}")

                # Scrie fiÈ™ierul combinat
                merger.write(output_file)
                merger.close()

                # VerificÄƒ cÄƒ fiÈ™ierul combinat a fost creat cu succes
                if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
                    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)
                    print(f"âœ… FiÈ™ierul combinat creat cu succes: {file_size_mb:.2f} MB")

                    # È˜TERGE SEGMENTELE DIN FOLDER (nu mai sunt copii, sunt originalele mutate)
                    deleted_count = 0
                    total_deleted_size = 0

                    for file_to_delete in moved_files:
                        try:
                            file_size = os.path.getsize(file_to_delete)
                            os.remove(file_to_delete)
                            deleted_count += 1
                            total_deleted_size += file_size
                            print(f"   ğŸ—‘ï¸ È˜ters segment: {os.path.basename(file_to_delete)}")
                        except Exception as e:
                            print(f"   âš  Nu am putut È™terge {file_to_delete}: {e}")

                    deleted_size_mb = total_deleted_size / (1024 * 1024)
                    print(f"ğŸ‰ FINALIZAT: PÄƒstrat doar fiÈ™ierul combinat '{os.path.basename(output_file)}'")
                    print(f"ğŸ—‘ï¸ È˜terse {deleted_count} segmente originale ({deleted_size_mb:.2f} MB)")
                    print(f"ğŸ’¾ BACKUP SIGUR: Segmentele originale pÄƒstrate Ã®n {backup_dir}")

                else:
                    print(f"âŒ EROARE: FiÈ™ierul combinat nu a fost creat corect!")
                    print(f"ğŸ›¡ï¸ SIGURANÈšÄ‚: PÄƒstrez segmentele pentru siguranÈ›Äƒ")
                    print(f"ğŸ’¾ BACKUP DISPONIBIL: {backup_dir}")

            elif len(moved_files) == 1:
                # Un singur fiÈ™ier - doar redenumeÈ™te
                original_file = moved_files[0]
                original_size_mb = os.path.getsize(original_file) / (1024 * 1024)

                try:
                    os.replace(original_file, output_file)
                    print(f"âœ… FiÈ™ierul redenumit Ã®n: {os.path.basename(output_file)} ({original_size_mb:.2f} MB)")
                    print(f"ğŸ’¾ BACKUP SIGUR: Originalul pÄƒstrat Ã®n {backup_dir}")
                except Exception as e:
                    print(f"âš  Nu am putut redenumi {original_file}: {e}")

            else:
                print(f"â„¹ï¸ Nu existÄƒ fiÈ™iere PDF de combinat Ã®n '{dest_dir}'.")

        except Exception as e:
            print(f"âŒ EROARE la combinarea PDF-urilor: {e}")
            print(f"ğŸ›¡ï¸ SIGURANÈšÄ‚: PÄƒstrez segmentele din cauza erorii")
            print(f"ğŸ’¾ BACKUP DISPONIBIL: {backup_dir}")
            return

        # PASUL 4: Raport final
        try:
            if os.path.exists(output_file):
                final_size_mb = os.path.getsize(output_file) / (1024 * 1024)

                print(f"\nğŸ“‹ RAPORT FINAL pentru '{issue_title}':")
                print(f"   ğŸ“ Folder destinaÈ›ie: {dest_dir}")
                print(f"   ğŸ“„ FiÈ™ier final: {os.path.basename(output_file)} ({final_size_mb:.2f} MB)")
                print(f"   ğŸ” Combinat din {len(all_segments)} segmente")
                print(f"   ğŸ’¾ BACKUP SIGUR: {backup_dir} ({backup_size_mb:.2f} MB)")
                print(f"   âœ… STATUS: SUCCES - fiÈ™ier complet creat, backup realizat, segmente È™terse de pe D:\\")
            else:
                print(f"âš  Nu s-a putut crea fiÈ™ierul final pentru '{issue_title}'")
                print(f"ğŸ’¾ BACKUP DISPONIBIL: {backup_dir}")
        except Exception as e:
            print(f"âš  Eroare la raportul final: {e}")

        print(f"=" * 60)

    def find_next_issue_in_collection_order(self, collection_links, last_completed_url):
        """
        FIXED: GÄƒseÈ™te urmÄƒtorul issue de procesat Ã®n ordinea din HTML, nu primul din listÄƒ
        """
        if not last_completed_url:
            # DacÄƒ nu avem istoric, Ã®ncepe cu primul din listÄƒ
            return collection_links[0] if collection_links else None

        try:
            last_index = collection_links.index(last_completed_url.rstrip('/'))
            # ReturneazÄƒ urmÄƒtorul din listÄƒ dupÄƒ cel completat
            next_index = last_index + 1
            if next_index < len(collection_links):
                next_url = collection_links[next_index]
                print(f"ğŸ¯ UrmÄƒtorul issue dupÄƒ '{last_completed_url}' este: '{next_url}'")
                return next_url
            else:
                print(f"âœ… Toate issue-urile din colecÈ›ie au fost procesate!")
                return None
        except ValueError:
            # DacÄƒ last_completed_url nu e Ã®n lista curentÄƒ, Ã®ncepe cu primul
            print(f"âš  URL-ul '{last_completed_url}' nu e Ã®n colecÈ›ia curentÄƒ, Ã®ncep cu primul din listÄƒ")
            return collection_links[0] if collection_links else None

    def get_last_completed_issue_from_collection(self, collection_links):
            """FIXED: GÄƒseÈ™te ultimul issue REAL complet descÄƒrcat din colecÈ›ia curentÄƒ"""
            for item in self.state.get("downloaded_issues", []):
                url = item.get("url", "").rstrip('/')
                if url in [link.rstrip('/') for link in collection_links]:

                    # VERIFICARE CORECTÄ‚: Issue-ul trebuie sÄƒ fie REAL complet
                    if self.is_issue_really_complete(item):
                        print(f"ğŸ Ultimul issue REAL complet din colecÈ›ie: {url}")
                        return url
                    elif item.get("completed_at"):
                        last_segment = item.get("last_successful_segment_end", 0)
                        total_pages = item.get("total_pages")
                        pages = item.get("pages", 0)
                        print(f"âš  Issue marcat ca complet dar INCOMPLET: {url} ({last_segment}/{total_pages}, pages: {pages})")

            print("ğŸ†• Niciun issue REAL complet gÄƒsit Ã®n colecÈ›ia curentÄƒ")
            return None

    def open_new_tab_and_download(self, url):
        """FIXED: Se focuseazÄƒ pe un singur issue pÃ¢nÄƒ la final cu verificÄƒri ultra-safe"""
        normalized_url = url.rstrip('/')

        # DOAR verificÄƒrile esenÈ›iale la Ã®nceput
        if normalized_url in self.dynamic_skip_urls:
            print(f"â­ï¸ Sar peste {url} (Ã®n skip list).")
            return False

        already_done = any(
            item.get("url") == normalized_url and item.get("completed_at") and
            item.get("last_successful_segment_end", 0) >= (item.get("total_pages") or float('inf'))
            for item in self.state.get("downloaded_issues", [])
        )
        if already_done:
            print(f"â­ï¸ Sar peste {url} (deja descÄƒrcat complet).")
            return False

        print(f"\nğŸ¯ ÃNCEP FOCUSAREA PE: {url}")
        print("=" * 60)

        try:
            if not self.attached_existing:
                self.ensure_alive_fallback()

            # Deschide fila nouÄƒ
            prev_handles = set(self.driver.window_handles)
            self.driver.execute_script("window.open('');")
            new_handles = set(self.driver.window_handles)
            diff = new_handles - prev_handles
            new_handle = diff.pop() if diff else self.driver.window_handles[-1]
            self.driver.switch_to.window(new_handle)

            if not self.navigate_to_page(url):
                print(f"âŒ Nu am putut naviga la {url}")
                return False

            time.sleep(2)

            # VerificÄƒ DOAR o datÄƒ la Ã®nceput pentru limitÄƒ
            if self.check_daily_limit_in_all_windows(set_flag=False):
                print("âš  PaginÄƒ cu limitÄƒ zilnicÄƒ detectatÄƒ - opresc aici.")
                self.state["daily_limit_hit"] = True
                self._save_state()
                return False

            print("âœ… Pagina e OK, Ã®ncep extragerea metadatelor...")
            title, subtitle = self.get_issue_metadata()

            # FIXED: ScaneazÄƒ corect fiÈ™ierele existente pentru acest issue specific
            existing_pages = self.get_existing_pdf_segments(url)
            print(f"ğŸ“Š Pagini existente pe disk: {existing_pages}")

            resume_from = 1
            json_progress = 0
            for item in self.state.get("downloaded_issues", []):
                if item.get("url") == normalized_url:
                    json_progress = item.get("last_successful_segment_end", 0)
                    total_pages = item.get("total_pages")
                    if json_progress and total_pages and json_progress >= total_pages:
                        resume_from = None
                    break

            actual_progress = max(json_progress, existing_pages)
            if actual_progress > 0 and resume_from is not None:
                resume_from = actual_progress + 1
                print(f"ğŸ“„ Reiau de la pagina {resume_from} (JSON: {json_progress}, Disk: {existing_pages})")

            if resume_from is None:
                print(f"â­ï¸ Issue-ul {url} este deja complet.")
                return False

            self.current_issue_url = normalized_url

            # FIXED: ObÈ›ine total_pages È™i actualizeazÄƒ progresul
            total_pages = self.get_total_pages()
            if total_pages > 0:
                self._update_partial_issue_progress(normalized_url, actual_progress, total_pages=total_pages, title=title, subtitle=subtitle)
            else:
                print("âš  Nu am putut obÈ›ine numÄƒrul total de pagini!")

            print(f"ğŸ”’ INTRÃ‚ND ÃN MODUL FOCUS - nu mai fac alte verificÄƒri pÃ¢nÄƒ nu termin!")

            # ==================== DESCÄ‚RCAREA PROPRIU-ZISÄ‚ ====================
            print(f"ğŸ“¥ ÃNCEPE DESCÄ‚RCAREA pentru {url}...")
            pages_done, limit_hit = self.save_all_pages_in_batches(resume_from=resume_from)

            print(f"ğŸ“Š REZULTAT DESCÄ‚RCARE:")
            print(f"   ğŸ“„ Pagini descÄƒrcate: {pages_done}")
            print(f"   ğŸ“„ Total necesar: {total_pages}")
            print(f"   ğŸ›‘ LimitÄƒ zilnicÄƒ: {limit_hit}")

            if pages_done == 0 and not limit_hit:
                print(f"âš  DescÄƒrcarea pentru {url} a eÈ™uat complet.")
                return False

            if limit_hit:
                print(f"âš  LimitÄƒ zilnicÄƒ atinsÄƒ Ã®n timpul descÄƒrcÄƒrii pentru {url}; progresul parÈ›ial a fost salvat.")
                return False

            # ==================== VERIFICÄ‚RI ULTRA SAFE ÃNAINTE DE FINALIZARE ====================

            print(f"ğŸ” VERIFICÄ‚RI FINALE ULTRA SAFE pentru {url}...")
            print(f"ğŸ“Š Rezultat descÄƒrcare: {pages_done} pagini din {total_pages}")

            # VerificÄƒ dacÄƒ total_pages a fost detectat corect
            if total_pages <= 0:
                print(f"âŒ OPRIRE SAFETY: total_pages nu a fost detectat corect ({total_pages})")
                print(f"ğŸ›¡ï¸ NU marchez ca terminat fÄƒrÄƒ total_pages valid")
                print(f"ğŸ”„ PÄƒstrez ca parÈ›ial cu progres {pages_done}")

                self._update_partial_issue_progress(
                    normalized_url, pages_done, total_pages=None, title=title, subtitle=subtitle
                )
                return True  # Succes parÈ›ial

            # VERIFICARE CRITICÄ‚: Progresul trebuie sÄƒ fie aproape complet
            completion_percent = (pages_done / total_pages) * 100
            print(f"ğŸ“Š Completitudine calculatÄƒ: {completion_percent:.1f}%")

            if completion_percent < 95:  # Cel puÈ›in 95%
                print(f"âŒ BLOCARE SAFETY: Progres insuficient pentru marcare ca terminat")
                print(f"ğŸ“Š Progres: {pages_done}/{total_pages} ({completion_percent:.1f}%)")
                print(f"ğŸ›¡ï¸ Trebuie cel puÈ›in 95% pentru a marca ca terminat!")
                print(f"ğŸ”„ PÄƒstrez ca PARÈšIAL pentru continuare ulterioarÄƒ")

                # MarcheazÄƒ ca parÈ›ial, NU ca terminat
                self._update_partial_issue_progress(
                    normalized_url, pages_done, total_pages=total_pages, title=title, subtitle=subtitle
                )

                print(f"ğŸ’¾ Issue {url} pÄƒstrat ca parÈ›ial: {pages_done}/{total_pages}")
                print(f"ğŸ”„ Va fi continuat automat la urmÄƒtoarea rulare")
                return True  # Succes parÈ›ial - va continua mai tÃ¢rziu

            # VERIFICARE SUPLIMENTARÄ‚: Issues mari cu progres mic
            if total_pages >= 500 and pages_done < 200:
                print(f"âŒ BLOCARE SPECIALÄ‚: Issue mare cu progres suspect de mic")
                print(f"ğŸ“Š {pages_done} pagini din {total_pages} pare eÈ™ec de descÄƒrcare!")
                print(f"ğŸ›¡ï¸ Probabil eroare tehnicÄƒ sau limitÄƒ - NU marchez terminat")

                self._update_partial_issue_progress(
                    normalized_url, pages_done, total_pages=total_pages, title=title, subtitle=subtitle
                )
                return True  # Succes parÈ›ial

            # ===== TOATE VERIFICÄ‚RILE AU TRECUT - SAFE SÄ‚ MARCHEZ CA TERMINAT =====

            print(f"âœ… TOATE VERIFICÄ‚RILE ULTRA SAFE au trecut pentru {url}")
            print(f"ğŸ¯ Progres: {pages_done}/{total_pages} ({completion_percent:.1f}%)")
            print(f"ğŸ¯ Marchez ca TERMINAT COMPLET")

            # PAUZÄ‚ CRITICÄ‚ 1: AÈ™teaptÄƒ ca toate fiÈ™ierele sÄƒ fie complet scrise pe disk
            print("â³ SINCRONIZARE: AÈ™tept 30 secunde ca toate fiÈ™ierele sÄƒ fie complet salvate pe disk...")
            time.sleep(30)

            # FIXED: MarcheazÄƒ ca terminat cu total_pages corect
            self.mark_issue_done(url, pages_done, title=title, subtitle=subtitle, total_pages=total_pages)
            print(f"âœ… Issue marcat ca terminat Ã®n JSON: {url} ({pages_done} pagini)")

            # PAUZÄ‚ CRITICÄ‚ 2: AÈ™teaptÄƒ ca JSON sÄƒ fie salvat complet
            print("â³ SINCRONIZARE: AÈ™tept 5 secunde pentru sincronizarea JSON...")
            time.sleep(5)

            # ==================== PROCESAREA PDF-URILOR ====================
            print(f"ğŸ”„ ÃNCEPE PROCESAREA PDF-URILOR pentru {url}...")

            # VerificÄƒ din nou cÄƒ toate fiÈ™ierele sunt pe disk
            final_segments = self.get_all_pdf_segments_for_issue(url)
            print(f"ğŸ” VERIFICARE FINALÄ‚: Am gÄƒsit {len(final_segments)} fiÈ™iere PDF pentru acest issue")

            if len(final_segments) == 0:
                print(f"âš  PROBLEMÄ‚: Nu am gÄƒsit fiÈ™iere PDF pentru {url}!")
                return False

            # CopiazÄƒ È™i combinÄƒ PDF-urile
            self.copy_and_combine_issue_pdfs(url, title or normalized_url)

            # PAUZÄ‚ CRITICÄ‚ 3: AÈ™teaptÄƒ ca procesarea PDF sÄƒ fie completÄƒ
            print("â³ SINCRONIZARE: AÈ™tept 15 secunde dupÄƒ procesarea PDF-urilor...")
            time.sleep(15)

            # ==================== FINALIZARE COMPLETÄ‚ ====================
            print("=" * 60)
            print(f"ğŸ‰ FOCUSAREA COMPLETÄ‚ PE {url} FINALIZATÄ‚ CU SUCCES!")
            print(f"ğŸ“Š REZULTAT: {pages_done} pagini descÄƒrcate È™i procesate")
            print("=" * 60)

            # PAUZÄ‚ FINALÄ‚: Ãnainte sÄƒ treacÄƒ la urmÄƒtorul issue
            print("â³ PAUZÄ‚ FINALÄ‚: 10 secunde Ã®nainte de urmÄƒtorul issue...")
            time.sleep(10)

            return True

        except WebDriverException as e:
            print(f"âŒ Eroare WebDriver pentru {url}: {e}")
            return False
        except Exception as e:
            print(f"âŒ Eroare Ã®n open_new_tab_and_download pentru {url}: {e}")
            return False
        finally:
            try:
                # NU ÃNCHIDE DACÄ‚ E ULTIMA FEREASTRÄ‚
                if len(self.driver.window_handles) > 1:
                    self.driver.close()
                    self.driver.switch_to.window(self.driver.window_handles[0])
                else:
                    # Doar revine la prima fereastrÄƒ fÄƒrÄƒ sÄƒ Ã®nchidÄƒ
                    if self.driver.window_handles:
                        self.driver.switch_to.window(self.driver.window_handles[0])
            except Exception as e:
                print(f"âš  Eroare Ã®n finally: {e}")
                pass

    def ensure_alive_fallback(self):
        try:
            _ = self.driver.title
        except Exception as e:
            print(f"âš  Conexiune WebDriver moartÄƒ ({e}), pornesc instanÈ›Äƒ nouÄƒ ca fallback.")
            prefs = {
                "download.default_directory": os.path.abspath(self.download_dir),
                "download.prompt_for_download": False,
                "download.directory_upgrade": True,
                "safebrowsing.enabled": True,
            }
            chrome_options = Options()
            chrome_options.add_experimental_option("prefs", prefs)
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--incognito")
            self.driver = webdriver.Chrome(options=chrome_options)
            self.wait = WebDriverWait(self.driver, self.timeout)
            self.attached_existing = False

    def run_collection(self, collection_url):
        """FIXED: ProceseazÄƒ UN SINGUR issue pe rÃ¢nd, fÄƒrÄƒ sÄƒ sarÄƒ la altele"""
        print(f"ğŸŒ Ãncep procesarea colecÈ›iei: {collection_url}")
        if not self.driver:
            print("âŒ Driver neiniÈ›ializat.")
            return False
        if not self.navigate_to_page(collection_url):
            return False

        # VerificÄƒ limita DOAR la Ã®nceput
        if self.state.get("daily_limit_hit", False):
            print("âš  Nu mai pot continua din cauza limitei zilnice setate anterior.")
            return True

        if self.remaining_quota() <= 0:
            print(f"âš  Limita zilnicÄƒ de {DAILY_LIMIT} issue-uri atinsÄƒ.")
            return True

        issue_links = self.extract_issue_links_from_collection()
        if not issue_links:
            print("âš  Nu s-au gÄƒsit issue-uri Ã®n colecÈ›ie.")
            return False

        # FIXED: GÄƒseÈ™te ultimul issue complet din colecÈ›ie È™i continuÄƒ cu urmÄƒtorul
        last_completed = self.get_last_completed_issue_from_collection(issue_links)
        next_to_process = self.find_next_issue_in_collection_order(issue_links, last_completed)

        if not next_to_process:
            print("âœ… Toate issue-urile din aceastÄƒ colecÈ›ie sunt complete!")
            return True

        # ProceseazÄƒ issue-urile Ã®ncepÃ¢nd cu urmÄƒtorul dupÄƒ cel completat
        start_index = issue_links.index(next_to_process)
        pending_links = issue_links[start_index:]

        # FiltreazÄƒ doar cele care nu sunt Ã®n skip list sau complete
        actual_pending = []
        for link in pending_links:
            normalized = link.rstrip('/')
            if normalized in self.dynamic_skip_urls:
                continue
            exists = next((i for i in self.state.get("downloaded_issues", []) if i.get("url") == normalized), None)
            if not exists or not exists.get("completed_at"):
                actual_pending.append(link)

        print(f"ğŸ“Š Procesez {len(actual_pending)} issue-uri din colecÈ›ia curentÄƒ (Ã®ncepÃ¢nd cu {next_to_process})")

        # PROCESEAZÄ‚ CÃ‚TE UN ISSUE PE RÃ‚ND - fÄƒrÄƒ sÄƒ sarÄƒ la altele
        processed_any = False
        for i, link in enumerate(actual_pending):
            print(f"\nğŸ”¢ ISSUE {i+1}/{len(actual_pending)}: {link}")

            # VerificÄƒ cota DOAR Ã®nainte sÄƒ Ã®nceapÄƒ un issue nou
            if self.remaining_quota() <= 0:
                print(f"âš  Limita zilnicÄƒ de {DAILY_LIMIT} issue-uri atinsÄƒ Ã®nainte de a Ã®ncepe acest issue.")
                break

            if self.state.get("daily_limit_hit", False):
                print("âš  Flag daily_limit_hit setat - opresc procesarea.")
                break

            # FOCUSEAZÄ‚ PE UN SINGUR ISSUE
            print(f"ğŸ¯ MÄƒ focusez EXCLUSIV pe: {link}")
            result = self.open_new_tab_and_download(link)

            if result:
                processed_any = True
                print(f"âœ… Issue-ul {link} procesat cu succes!")
            else:
                print(f"âš  Issue-ul {link} nu a fost procesat.")

            # VerificÄƒ din nou cota dupÄƒ procesare
            if self.remaining_quota() <= 0 or self.state.get("daily_limit_hit", False):
                print("âš  Limita zilnicÄƒ atinsÄƒ dupÄƒ procesarea acestui issue.")
                break

            # PauzÄƒ Ã®ntre issue-uri
            if i < len(actual_pending) - 1:  # Nu pune pauzÄƒ dupÄƒ ultimul
                print("â³ PauzÄƒ de 2s Ã®ntre issue-uri...")
                time.sleep(2)

        # ACUM VERIFICAREA E DUPÄ‚ BUCLA FOR, NU ÃNÄ‚UNTRUL EI!
        # FIXED: VerificÄƒ dacÄƒ TOATE issue-urile din aceastÄƒ colecÈ›ie sunt complete
        all_done = True
        total_issues = len(issue_links)
        completed_issues = 0
        pending_issues = []

        for link in issue_links:
            normalized = link.rstrip('/')

            # VerificÄƒ Ã®n skip URLs (complet descÄƒrcate)
            if normalized in self.dynamic_skip_urls:
                completed_issues += 1
                continue

            # VerificÄƒ Ã®n state.json
            exists = next((i for i in self.state.get("downloaded_issues", []) if i.get("url") == normalized), None)

            if exists and exists.get("completed_at") and \
               exists.get("total_pages") and \
               exists.get("last_successful_segment_end", 0) >= exists.get("total_pages", 0):
                completed_issues += 1
            else:
                all_done = False
                pending_issues.append(link)

        print(f"ğŸ“Š VERIFICARE COLECÈšIE: {completed_issues}/{total_issues} issue-uri complete")
        if pending_issues:
            print(f"ğŸ”„ Issue-uri rÄƒmase: {len(pending_issues)}")
            for idx, link in enumerate(pending_issues[:5]):  # AfiÈ™eazÄƒ primele 5
                print(f"   {idx+1}. {link}")
            if len(pending_issues) > 5:
                print(f"   ... È™i Ã®ncÄƒ {len(pending_issues) - 5} issue-uri")

        return all_done

    def process_pending_partials_first(self):
        """FIXED: ProceseazÄƒ mai Ã®ntÃ¢i issue-urile parÈ›iale, indiferent de colecÈ›ie"""
        pending_partials = self.get_pending_partial_issues()

        if not pending_partials:
            print("âœ… Nu existÄƒ issue-uri parÈ›iale de procesat.")
            return True

        print(f"\nğŸ¯ PRIORITATE: Procesez {len(pending_partials)} issue-uri parÈ›iale:")
        for item in pending_partials:
            url = item.get("url")
            progress = item.get("last_successful_segment_end", 0)
            total = item.get("total_pages", 0)
            print(f"   ğŸ”„ {url} - pagini {progress}/{total}")

        # ProceseazÄƒ issue-urile parÈ›iale
        processed_any = False
        for item in pending_partials:
            if self.remaining_quota() <= 0 or self.state.get("daily_limit_hit", False):
                print(f"âš  Limita zilnicÄƒ atinsÄƒ Ã®n timpul issue-urilor parÈ›iale.")
                break

            url = item.get("url")
            result = self.open_new_tab_and_download(url)
            if result:
                processed_any = True
            time.sleep(1)

        return processed_any

    def run_additional_collections(self):
        """FIXED: ProceseazÄƒ colecÈ›iile adiÈ›ionale Ã®n ordinea corectÄƒ"""
        start_index = self.state.get("current_additional_collection_index", 0)

        # VERIFICÄ‚ cÄƒ indexul e valid
        if start_index >= len(ADDITIONAL_COLLECTIONS):
            print("âœ… TOATE colecÈ›iile adiÈ›ionale au fost procesate!")
            return True

        print(f"ğŸ”„ Continuez cu colecÈ›iile adiÈ›ionale de la indexul {start_index}")

        for i in range(start_index, len(ADDITIONAL_COLLECTIONS)):
            collection_url = ADDITIONAL_COLLECTIONS[i]

            print(f"\nğŸ“š COLECÈšIA {i+1}/{len(ADDITIONAL_COLLECTIONS)}: {collection_url}")

            # ADAUGÄ‚ ACEASTÄ‚ VERIFICARE
            if collection_url.rstrip('/') in self.dynamic_skip_urls:
                print(f"â­ï¸ Sar peste colecÈ›ia {i+1}/{len(ADDITIONAL_COLLECTIONS)} (deja completÄƒ): {collection_url}")
                self.state["current_additional_collection_index"] = i + 1
                self._save_state()
                continue

            if self.remaining_quota() <= 0 or self.state.get("daily_limit_hit", False):
                print(f"âš  Limita zilnicÄƒ atinsÄƒ, opresc procesarea colecÈ›iilor adiÈ›ionale.")
                break

            print(f"\nğŸ”„ Procesez colecÈ›ia adiÈ›ionalÄƒ {i+1}/{len(ADDITIONAL_COLLECTIONS)}: {collection_url}")

            self.state["current_additional_collection_index"] = i
            self._save_state()

            collection_completed = self.run_collection(collection_url)

            if collection_completed and not self.state.get("daily_limit_hit", False):
                print(f"âœ… ColecÈ›ia adiÈ›ionalÄƒ {i+1} completÄƒ, o marchez È™i trec la urmÄƒtoarea.")
                self.mark_collection_complete(collection_url)
                self.state["current_additional_collection_index"] = i + 1
                self._save_state()
                print(f"ğŸ”„ Continu cu urmÄƒtoarea colecÈ›ie adiÈ›ionalÄƒ...")
            elif self.state.get("daily_limit_hit", False):
                print("âš  Limita zilnicÄƒ atinsÄƒ - opresc procesarea.")
                break
            else:
                print(f"âš  ColecÈ›ia adiÈ›ionalÄƒ {i+1} nu este completÄƒ Ã®ncÄƒ - voi continua cu urmÄƒtoarea!")
                print(f"ğŸ”„ Continu cu urmÄƒtoarea colecÈ›ie adiÈ›ionalÄƒ...")

        current_index = self.state.get("current_additional_collection_index", 0)
        if current_index >= len(ADDITIONAL_COLLECTIONS):
            print("ğŸ‰ TOATE colecÈ›iile adiÈ›ionale au fost procesate!")
            print("ğŸ”„ Voi continua cu urmÄƒtoarea colecÈ›ie disponibilÄƒ...")
            return False  # Returnez False pentru a continua cu urmÄƒtoarea colecÈ›ie
        else:
            remaining = len(ADDITIONAL_COLLECTIONS) - current_index
            print(f"âš  Mai rÄƒmÃ¢n {remaining} colecÈ›ii adiÈ›ionale de procesat.")
            print("ğŸ”„ Voi continua cu urmÄƒtoarea colecÈ›ie...")
            return False  # Returnez False pentru a continua

    def run(self):
            print("ğŸ§ª Ãncep executarea Chrome PDF Downloader FIXED cu SORTARE CRONOLOGICÄ‚")
            print("=" * 60)

            try:
                if not self.setup_chrome_driver():
                    return False

                print("ğŸ”„ Resetez flag-ul de limitÄƒ zilnicÄƒ È™i verific ferestrele existente...")
                self.state["daily_limit_hit"] = False
                self._save_state()

                # FIXED: ReconstruieÈ™te progresul din fiÈ™ierele de pe disk
                self.sync_json_with_disk_files()

                # CLEANUP dubluri DUPÄ‚ sincronizarea cu disk-ul
                self.cleanup_duplicate_issues()

                # NOUÄ‚ ETAPÄ‚: CorecteazÄƒ issue-urile marcate greÈ™it ca complete
                print("\nğŸ”§ ETAPA CORECTARE: Verific issue-urile marcate greÈ™it ca complete")
                fixes_applied = self.fix_incorrectly_marked_complete_issues()

                if fixes_applied > 0:
                    print(f"âœ… Corectat {fixes_applied} issue-uri - acestea vor fi reluate")

                if self.check_daily_limit_in_all_windows(set_flag=False):
                    print("âš  Am gÄƒsit ferestre cu limita deschise din sesiuni anterioare.")
                    print("ğŸ”„ Le-am Ã®nchis È™i reÃ®ncerc procesarea...")

                # FIXED: ETAPA 0: ProceseazÄƒ MAI ÃNTÃ‚I issue-urile parÈ›iale (PRIORITATE ABSOLUTÄ‚)
                print(f"\nğŸ¯ ETAPA 0: PRIORITATE ABSOLUTÄ‚ - Procesez issue-urile parÈ›iale")
                if self.process_pending_partials_first():
                    print("âœ… Issue-urile parÈ›iale au fost procesate sau limita a fost atinsÄƒ.")
                    # OPREÈ˜TE aici dacÄƒ existÄƒ parÈ›iale - nu trece la colecÈ›ii noi
                    if self.get_pending_partial_issues():
                        print("ğŸ”„ ÃncÄƒ mai existÄƒ issue-uri parÈ›iale - voi continua cu ele urmÄƒtoarea datÄƒ")
                        return True

                if self.remaining_quota() <= 0 or self.state.get("daily_limit_hit", False):
                    print("âš  Limita zilnicÄƒ atinsÄƒ dupÄƒ procesarea issue-urilor parÈ›iale.")
                    return True

                # ETAPA 1: ProceseazÄƒ colecÈ›ia principalÄƒ (dacÄƒ nu e completÄƒ)
                # FIXED: VerificÄƒ Ã®ntotdeauna dacÄƒ colecÈ›ia principalÄƒ e completÄƒ
                print(f"\nğŸ“š ETAPA 1: Verific colecÈ›ia principalÄƒ: {self.main_collection_url}")

                main_completed = self.run_collection(self.main_collection_url)

                if self.state.get("daily_limit_hit", False):
                    print("âš  Limita zilnicÄƒ atinsÄƒ Ã®n colecÈ›ia principalÄƒ.")
                    return True

                if main_completed:
                    print("âœ… ColecÈ›ia principalÄƒ este completÄƒ!")
                    self.state["main_collection_completed"] = True
                    self._save_state()
                else:
                    print("ğŸ”„ ColecÈ›ia principalÄƒ nu este completÄƒ Ã®ncÄƒ - continuez cu ea.")
                    self.state["main_collection_completed"] = False  # RESETEAZÄ‚ dacÄƒ nu e completÄƒ
                    self._save_state()
                    return True

                # ETAPA 2: ProceseazÄƒ colecÈ›iile adiÈ›ionale
                if self.remaining_quota() > 0 and not self.state.get("daily_limit_hit", False):
                    print(f"\nğŸ“š ETAPA 2: Procesez colecÈ›iile adiÈ›ionale")
                    self.run_additional_collections()

                print("âœ… Toate operaÈ›iunile au fost iniÈ›iate.")
                self._finalize_session()
                return True

            except KeyboardInterrupt:
                print("\n\nâš  IntervenÈ›ie manualÄƒ: Ã®ntrerupt.")
                return False
            except Exception as e:
                print(f"\nâŒ Eroare neaÈ™teptatÄƒ: {e}")
                return False
            finally:
                if not self.attached_existing and self.driver:
                    try:
                        self.driver.quit()
                    except Exception:
                        pass

    def _finalize_session(self):
        if self.driver:
            if self.attached_existing:
                print("ğŸ”– Am pÄƒstrat sesiunea Chrome existentÄƒ deschisÄƒ (nu fac quit).")
            else:
                print("ğŸšª Ãnchid browserul.")
                try:
                    self.driver.quit()
                except Exception:
                    pass


def main():
    """
    MAIN FUNCTION CORECTATÄ‚ - FOCUSEAZÄ‚ PE StudiiSiCercetariMecanicaSiAplicata
    Nu mai sare la alte colecÈ›ii pÃ¢nÄƒ nu terminÄƒ cu aceasta complet!
    """

    log_file = setup_logging()  # ADÄ‚UGAT - PRIMA LINIE


    print("ğŸš€ PORNIRE SCRIPT - ANALIZA INIÈšIALÄ‚")
    print("=" * 70)

    # PASUL 1: CreeazÄƒ downloader temporar pentru analiza stÄƒrii
    temp_downloader = ChromePDFDownloader("temp", download_dir="D:\\", batch_size=50)

    # PASUL 2: AnalizeazÄƒ starea curentÄƒ
    print("ğŸ” ANALIZA STÄ‚RII CURENTE:")
    current_state = temp_downloader.state

    main_completed = current_state.get("main_collection_completed", False)
    current_index = current_state.get("current_additional_collection_index", 0)
    total_issues = len(current_state.get("downloaded_issues", []))

    print(f"   ğŸ“Š Total issues Ã®n state: {total_issues}")
    print(f"   ğŸ Main collection completed: {main_completed}")
    print(f"   ğŸ”¢ Current additional index: {current_index}")

    # PASUL 3: VerificÄƒ issue-urile parÈ›iale (PRIORITATE ABSOLUTÄ‚)
    print(f"\nğŸ¯ VERIFICARE ISSUE-URI PARÈšIALE:")
    pending_partials = temp_downloader.get_pending_partial_issues()

    if pending_partials:
        print(f"ğŸš¨ GÄ‚SITE {len(pending_partials)} ISSUE-URI PARÈšIALE!")
        print(f"ğŸ”¥ PRIORITATE ABSOLUTÄ‚ - acestea trebuie continuate:")

        for item in pending_partials:
            url = item.get("url", "")
            progress = item.get("last_successful_segment_end", 0)
            total = item.get("total_pages", 0)
            title = item.get("title", "")
            print(f"   ğŸ”„ {title}")
            print(f"      ğŸ“ {url}")
            print(f"      ğŸ¯ CONTINUÄ‚ de la pagina {progress + 1} (progres: {progress}/{total})")

        print(f"\nâœ… VA PROCESA AUTOMAT issue-urile parÈ›iale primul!")
    else:
        print(f"âœ… Nu existÄƒ issue-uri parÈ›iale de procesat")

    # PASUL 4: AnalizeazÄƒ progresul StudiiSiCercetariMecanicaSiAplicata
    print(f"\nğŸ“š ANALIZA COLECÈšIEI StudiiSiCercetariMecanicaSiAplicata:")

    # Lista completÄƒ a anilor disponibili din HTML (1954-1992, minus 1964)
    expected_years = []
    for year in range(1954, 1993):  # 1954-1992
        if year != 1964:  # 1964 nu existÄƒ Ã®n colecÈ›ie
            expected_years.append(year)

    # VerificÄƒ care ani au fost descÄƒrcaÈ›i
    downloaded_years = []
    partial_years = []

    for item in current_state.get("downloaded_issues", []):
        url = item.get("url", "")
        if "StudiiSiCercetariMecanicaSiAplicata" in url:
            # Extrage anul din URL
            year_match = re.search(r'StudiiSiCercetariMecanicaSiAplicata_(\d{4})', url)
            if year_match:
                year = int(year_match.group(1))
                if item.get("completed_at"):
                    downloaded_years.append(year)
                else:
                    partial_years.append(year)

    downloaded_years.sort()
    partial_years.sort()
    missing_years = [year for year in expected_years if year not in downloaded_years and year not in partial_years]

    print(f"   ğŸ“… Ani disponibili: {len(expected_years)} (1954-1992, minus 1964)")
    print(f"   âœ… Ani descÄƒrcaÈ›i: {len(downloaded_years)} - {downloaded_years}")
    print(f"   ğŸ”„ Ani parÈ›iali: {len(partial_years)} - {partial_years}")
    print(f"   âŒ Ani lipsÄƒ: {len(missing_years)} - {missing_years[:10]}{'...' if len(missing_years) > 10 else ''}")

    # PASUL 5: DeterminÄƒ strategia
    total_remaining = len(partial_years) + len(missing_years)

    if total_remaining > 0:
        print(f"\nğŸ¯ STRATEGIA DE PROCESARE:")
        print(f"   ğŸ”¥ RÄ‚MÃ‚N {total_remaining} ani de procesat din StudiiSiCercetariMecanicaSiAplicata")
        print(f"   ğŸš« NU se trece la alte colecÈ›ii pÃ¢nÄƒ nu se terminÄƒ aceasta!")
        print(f"   ğŸ“ˆ Progres: {len(downloaded_years)}/{len(expected_years)} ani completaÈ›i ({len(downloaded_years)/len(expected_years)*100:.1f}%)")
    else:
        print(f"\nâœ… StudiiSiCercetariMecanicaSiAplicata este COMPLET!")
        print(f"   ğŸ¯ Va trece la urmÄƒtoarea colecÈ›ie din ADDITIONAL_COLLECTIONS")

    # PASUL 6: ReseteazÄƒ starea pentru a continua corect cu StudiiSiCercetariMecanicaSiAplicata
    if total_remaining > 0:
        print(f"\nğŸ”§ RESETEZ STAREA pentru a continua cu StudiiSiCercetariMecanicaSiAplicata:")

        # ReseteazÄƒ flag-urile greÈ™ite
        if main_completed:
            print(f"   ğŸ”„ Resetez main_collection_completed: True â†’ False")
            temp_downloader.state["main_collection_completed"] = False

        if current_index > 1:  # StudiiSiCercetariMecanicaSiAplicata e pe index 1
            print(f"   ğŸ”„ Resetez current_additional_collection_index: {current_index} â†’ 1")
            temp_downloader.state["current_additional_collection_index"] = 1

        temp_downloader._save_state()
        print(f"   âœ… Starea resetatÄƒ pentru a continua cu StudiiSiCercetariMecanicaSiAplicata")

    # PASUL 7: SeteazÄƒ URL-ul colecÈ›iei principale
    main_collection_url = "https://adt.arcanum.com/ro/collection/StudiiSiCercetariMecanicaSiAplicata/"

    print(f"\nğŸš€ ÃNCEPE PROCESAREA:")
    print(f"ğŸ“ URL principal: {main_collection_url}")
    print(f"ğŸ“ Director descÄƒrcare: D:\\")
    print(f"ğŸ“¦ Batch size: 50 pagini per segment")

    if pending_partials:
        print(f"âš¡ Va Ã®ncepe cu {len(pending_partials)} issue-uri parÈ›iale")
    if missing_years:
        print(f"ğŸ“… Va continua cu anii lipsÄƒ: {missing_years[:5]}{'...' if len(missing_years) > 5 else ''}")

    print("=" * 70)

    # PASUL 8: CreeazÄƒ downloader-ul principal È™i porneÈ™te procesarea
    try:
        downloader = ChromePDFDownloader(
            main_collection_url=main_collection_url,
            download_dir="D:\\",
            batch_size=50
        )

        print("ğŸ¯ ÃNCEPE EXECUÈšIA PRINCIPALÄ‚...")
        success = downloader.run()

        if success:
            print("\nâœ… EXECUÈšIE FINALIZATÄ‚ CU SUCCES!")
        else:
            print("\nâš  EXECUÈšIE FINALIZATÄ‚ CU PROBLEME!")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n\nâš  OPRIRE MANUALÄ‚ - Progresul a fost salvat")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ EROARE FATALÄ‚ Ã®n main(): {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"âŒ Eroare fatalÄƒ Ã®n __main__: {e}")
        sys.exit(1)