#!/usr/bin/env python3
"""
Automatizare descÄƒrcare PDF-uri din Arcanum (FIXED VERSION):
- FIXED: ScaneazÄƒ corect toate fiÈ™ierele existente de pe disk
- FIXED: PÄƒstreazÄƒ progresul parÈ›ial Ã®ntre zile
- FIXED: ProceseazÄƒ È™i combinÄƒ corect TOATE PDF-urile pentru fiecare issue
- FIXED: Resume logic corect pentru issue-urile parÈ›iale
- FIXED: DetecteazÄƒ corect prefix-urile pentru fiÈ™iere
- FIXED: VerificÄƒ corect issue-urile complete pentru skip URLs
- FIXED: EliminÄƒ dublurile automat
- FIXED: DetecteazÄƒ mai bine numÄƒrul total de pagini
"""

import time
import os
import sys
import re
import json
import shutil
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException, ElementClickInterceptedException

# ColecÈ›iile adiÈ›ionale (procesate DUPÄ‚ colecÈ›ia principalÄƒ din main())
ADDITIONAL_COLLECTIONS = [
    "https://adt.arcanum.com/ro/collection/Convietuirea/",
    "https://adt.arcanum.com/ro/collection/StudiiSiCercetariMecanicaSiAplicata/",
    "https://adt.arcanum.com/ro/collection/CalitateFiabilitat/",
    "https://adt.arcanum.com/ro/collection/Metrologie/",
    "https://adt.arcanum.com/ro/collection/MetrologiaAplicata/",
    "https://adt.arcanum.com/ro/collection/AnaleleUnivBucuresti_GeologieGeografie/",
    "https://adt.arcanum.com/ro/collection/BuletinulStiintificTechnicInstitutuluiPolitehnicTimisoara_MatematicaFizicaMecanica/",
    "https://adt.arcanum.com/ro/collection/StudiiSiCercetariMatematice/",
    "https://adt.arcanum.com/ro/collection/AnaleleUnivBucuresti_MatematicaMecanicaFizica/",
    "https://adt.arcanum.com/ro/collection/RevistaMatematicaDinTimisoara/",
    "https://adt.arcanum.com/ro/collection/BuletInstPolitehIasi_1/",
    "https://adt.arcanum.com/ro/collection/Energetica/",
    "https://adt.arcanum.com/ro/collection/CulturaFizicaSiSport/",
    "https://adt.arcanum.com/ro/collection/Almanahul_Satelor/",
    "https://adt.arcanum.com/ro/collection/RevistaDeFolclor/",
    "https://adt.arcanum.com/ro/collection/AlmanahBTT/",
    "https://adt.arcanum.com/ro/collection/MinePetrolGaze/",
    "https://adt.arcanum.com/ro/collection/BuletinulInstitutuluiPolitehnicBucuresti_Mecanica/",
    "https://adt.arcanum.com/ro/collection/RevistaMinelor/",
    "https://adt.arcanum.com/ro/collection/StudiiSiCercetariDeGeologie/",
    "https://adt.arcanum.com/ro/collection/RevistaIndustriaAlimentare/",
    "https://adt.arcanum.com/ro/collection/IndustriaLemnului/",
    "https://adt.arcanum.com/ro/collection/Electricitatea/",
    "https://adt.arcanum.com/ro/collection/SzatmariMuzeumKiadvanyai_Evkonyv_ADT/",
    "https://adt.arcanum.com/ro/collection/ConstructiaDeMasini/",
    "https://adt.arcanum.com/ro/collection/SufletNou/",
    "https://adt.arcanum.com/ro/collection/Rondul/",
    "https://adt.arcanum.com/ro/collection/RomaniaMilitara/",
    "https://adt.arcanum.com/ro/collection/Magazin/",
    "https://adt.arcanum.com/ro/collection/TribunaSibiului/",
    "https://adt.arcanum.com/ro/collection/Metalurgia/",
    "https://adt.arcanum.com/ro/collection/StudiiSiCercetariDeMetalurgie/",
    "https://adt.arcanum.com/ro/collection/BuletinulInstitutuluiPolitehnicBucuresti_ChimieMetalurgie/",
    "https://adt.arcanum.com/ro/collection/RomaniaMuncitoare/",
    "https://adt.arcanum.com/ro/collection/Cronica/",
    "https://adt.arcanum.com/ro/collection/Marisia_ADT/",
    "https://adt.arcanum.com/ro/collection/RevistaDeEtnografieSiFolclor/",
    "https://adt.arcanum.com/ro/collection/RevistaMuzeelor/",
    "https://adt.arcanum.com/ro/collection/GazetaDeDuminica/",
    "https://adt.arcanum.com/ro/collection/BuletInstPolitehIasi_0/",
    "https://adt.arcanum.com/ro/collection/JurnalulNational/",
    "https://adt.arcanum.com/ro/collection/UnireaBlajPoporului/",
    "https://adt.arcanum.com/ro/collection/TranssylvaniaNostra/",
    "https://adt.arcanum.com/ro/collection/CuvantulPoporului/",
    "https://adt.arcanum.com/ro/collection/Agrarul/",
    "https://adt.arcanum.com/ro/collection/CurierulFoaiaIntereselorGenerale/",
    "https://adt.arcanum.com/ro/collection/EconomiaNationala/",
    "https://adt.arcanum.com/ro/collection/Constitutionalul/"
]

# Skip URLs statice (hardcoded)
STATIC_SKIP_URLS = {
    "https://adt.arcanum.com/ro/view/Convietuirea_1997-1998"
}

DAILY_LIMIT = 105
STATE_FILENAME = "state.json"
SKIP_URLS_FILENAME = "skip_urls.json"


class ChromePDFDownloader:
    def __init__(self, main_collection_url, download_dir=None, batch_size=50, timeout=15):
        self.main_collection_url = main_collection_url
        self.batch_size = batch_size
        self.timeout = timeout
        self.download_dir = download_dir or os.getcwd()
        self.driver = None
        self.wait = None
        self.attached_existing = False
        self.state_path = os.path.join(self.download_dir, STATE_FILENAME)
        self.skip_urls_path = os.path.join(self.download_dir, SKIP_URLS_FILENAME)
        self.current_issue_url = None
        self.dynamic_skip_urls = set()
        self._load_skip_urls()
        self._load_state()
        self.fix_existing_json()

    def _load_skip_urls(self):
        """ÃncarcÄƒ skip URLs din fiÈ™ierul separat"""
        self.dynamic_skip_urls = set(STATIC_SKIP_URLS)  # Ãncepe cu cele statice

        if os.path.exists(self.skip_urls_path):
            try:
                with open(self.skip_urls_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    completed_urls = data.get("completed_urls", [])
                    completed_collections = data.get("completed_collections", [])

                    self.dynamic_skip_urls.update(url.rstrip('/') for url in completed_urls)
                    self.dynamic_skip_urls.update(url.rstrip('/') for url in completed_collections)

                    print(f"ğŸ“‹ ÃncÄƒrcat {len(completed_urls)} URL-uri complet descÄƒrcate din {SKIP_URLS_FILENAME}")
                    print(f"ğŸ“‹ ÃncÄƒrcat {len(completed_collections)} colecÈ›ii complet procesate din {SKIP_URLS_FILENAME}")
            except Exception as e:
                print(f"âš  Eroare la citirea {SKIP_URLS_FILENAME}: {e}")

        print(f"ğŸš« Total URL-uri de skip: {len(self.dynamic_skip_urls)}")

    def _save_skip_urls(self):
        """FIXED: VerificÄƒ corect dacÄƒ un issue este complet"""
        try:
            completed_urls = []
            for item in self.state.get("downloaded_issues", []):
                # VERIFICARE CORECTÄ‚: issue-ul trebuie sÄƒ aibÄƒ toate cÃ¢mpurile necesare
                completed_at = item.get("completed_at")
                total_pages = item.get("total_pages")
                last_segment = item.get("last_successful_segment_end", 0)

                # CONDIÈšIE FIXATÄ‚: toate trebuie sÄƒ fie setate È™i progresul sÄƒ fie complet
                if (completed_at and  # Marcat ca terminat
                    total_pages and  # Are total_pages setat
                    total_pages > 0 and  # Total valid
                    last_segment >= total_pages):  # Progresul este complet

                    completed_urls.append(item["url"])
                    print(f"âœ… Issue complet pentru skip: {item['url']} ({last_segment}/{total_pages})")
                else:
                    # DEBUG: AfiÈ™eazÄƒ de ce nu e considerat complet
                    if item.get("url"):  # Doar dacÄƒ are URL valid
                        print(f"ğŸ”„ Issue incomplet: {item.get('url', 'NO_URL')}")
                        print(f"   completed_at: {bool(completed_at)}")
                        print(f"   total_pages: {total_pages}")
                        print(f"   last_segment: {last_segment}")

            # AdaugÄƒ È™i cele statice
            all_completed = list(STATIC_SKIP_URLS) + completed_urls

            # PÄƒstreazÄƒ È™i colecÈ›iile complete dacÄƒ existÄƒ
            existing_data = {}
            if os.path.exists(self.skip_urls_path):
                with open(self.skip_urls_path, "r", encoding="utf-8") as f:
                    existing_data = json.load(f)

            data = {
                "last_updated": datetime.now().isoformat(),
                "completed_urls": sorted(list(set(all_completed))),
                "completed_collections": existing_data.get("completed_collections", [])
            }

            with open(self.skip_urls_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            print(f"ğŸ’¾ Salvat {len(data['completed_urls'])} URL-uri CORECT VERIFICATE Ã®n {SKIP_URLS_FILENAME}")
        except Exception as e:
            print(f"âš  Eroare la salvarea {SKIP_URLS_FILENAME}: {e}")

    def _safe_folder_name(self, name: str) -> str:
        return re.sub(r'[<>:"/\\|?*]', '_', name).strip()

    def _decode_unicode_escapes(self, obj):
        """DecodificÄƒ secvenÈ›ele unicode din JSON"""
        if isinstance(obj, dict):
            return {key: self._decode_unicode_escapes(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._decode_unicode_escapes(item) for item in obj]
        elif isinstance(obj, str):
            # DecodificÄƒ secvenÈ›ele unicode ca \u0103, \u0219
            try:
                return obj.encode('utf-8').decode('unicode_escape').encode('latin-1').decode('utf-8') if '\\u' in obj else obj
            except:
                return obj
        else:
            return obj

    def is_issue_complete_by_end_page(self, end_page):
        """DeterminÄƒ dacÄƒ un issue e complet pe baza ultimei pagini"""
        return not ((end_page + 1) % 50 == 0 or (end_page + 1) % 100 == 0)

    def extract_issue_id_from_filename(self, filename):
        """FIXED: Extrage ID-ul issue-ului din numele fiÈ™ierului (fÄƒrÄƒ timestamp)"""
        # CautÄƒ pattern-ul: PrefixIssue-TIMESTAMP__pages
        match = re.search(r'([^-]+(?:-[^-]+)*)-\d+__pages', filename)
        if match:
            return match.group(1)
        return None

    def extract_issue_url_from_filename(self, filename):
        """FIXED: Extrage URL-ul issue-ului din numele fiÈ™ierului"""
        issue_id = self.extract_issue_id_from_filename(filename)
        if not issue_id:
            return None

        if "Convietuirea" in issue_id:
            return f"https://adt.arcanum.com/ro/view/{issue_id}"
        elif "GazetaMatematica" in issue_id:
            return f"https://adt.arcanum.com/en/view/{issue_id}"
        else:
            return f"https://adt.arcanum.com/ro/view/{issue_id}"

    def get_all_pdf_segments_for_issue(self, issue_url):
        """FIXED: ScaneazÄƒ toate fiÈ™ierele PDF pentru un issue specific"""
        issue_id = issue_url.rstrip('/').split('/')[-1]
        segments = []

        try:
            for filename in os.listdir(self.download_dir):
                if not filename.lower().endswith('.pdf'):
                    continue

                file_issue_id = self.extract_issue_id_from_filename(filename)
                if file_issue_id == issue_id:
                    # Extrage intervalul de pagini
                    match = re.search(r'__pages(\d+)-(\d+)\.pdf', filename)
                    if match:
                        start_page = int(match.group(1))
                        end_page = int(match.group(2))
                        segments.append({
                            'filename': filename,
                            'start': start_page,
                            'end': end_page,
                            'path': os.path.join(self.download_dir, filename)
                        })

        except Exception as e:
            print(f"âš  Eroare la scanarea fiÈ™ierelor pentru {issue_url}: {e}")

        # SorteazÄƒ dupÄƒ pagina de Ã®nceput
        segments.sort(key=lambda x: x['start'])
        return segments

    def get_existing_pdf_segments(self, issue_url):
        """FIXED: ScaneazÄƒ toate segmentele existente È™i returneazÄƒ ultima paginÄƒ"""
        segments = self.get_all_pdf_segments_for_issue(issue_url)

        if not segments:
            return 0

        # GÄƒseÈ™te cea mai mare paginÄƒ finalÄƒ
        max_page = max(seg['end'] for seg in segments)

        print(f"ğŸ“Š FiÈ™iere PDF existente pentru {issue_url}:")
        for seg in segments:
            print(f"   ğŸ“„ {seg['filename']} (pagini {seg['start']}-{seg['end']})")

        return max_page

    def reconstruct_all_issues_from_disk(self):
        """FIXED: ReconstruieÈ™te complet progresul din fiÈ™ierele de pe disk"""
        print("ğŸ” SCANEZ COMPLET toate fiÈ™ierele PDF de pe disk...")

        # GrupeazÄƒ fiÈ™ierele dupÄƒ issue ID
        issues_on_disk = {}

        try:
            for filename in os.listdir(self.download_dir):
                if not filename.lower().endswith('.pdf'):
                    continue

                issue_id = self.extract_issue_id_from_filename(filename)
                if not issue_id:
                    continue

                # Extrage intervalul de pagini
                match = re.search(r'__pages(\d+)-(\d+)\.pdf', filename)
                if not match:
                    continue

                start_page = int(match.group(1))
                end_page = int(match.group(2))

                if issue_id not in issues_on_disk:
                    issues_on_disk[issue_id] = {
                        'segments': [],
                        'max_page': 0,
                        'url': self.extract_issue_url_from_filename(filename)
                    }

                issues_on_disk[issue_id]['segments'].append({
                    'filename': filename,
                    'start': start_page,
                    'end': end_page
                })

                if end_page > issues_on_disk[issue_id]['max_page']:
                    issues_on_disk[issue_id]['max_page'] = end_page

        except Exception as e:
            print(f"âš  Eroare la scanarea disk-ului: {e}")
            return {}

        # AfiÈ™eazÄƒ rezultatele scanÄƒrii
        print(f"ğŸ“Š GÄƒsite {len(issues_on_disk)} issue-uri pe disk:")
        for issue_id, data in issues_on_disk.items():
            segments_count = len(data['segments'])
            max_page = data['max_page']
            url = data['url']

            print(f"   ğŸ“ {issue_id}: {segments_count} segmente, max pagina {max_page}")
            print(f"      ğŸ”— URL: {url}")

            # AfiÈ™eazÄƒ segmentele sortate
            data['segments'].sort(key=lambda x: x['start'])
            for seg in data['segments'][:3]:  # Primele 3
                print(f"      ğŸ“„ {seg['filename']} ({seg['start']}-{seg['end']})")
            if segments_count > 3:
                print(f"      ğŸ“„ ... È™i Ã®ncÄƒ {segments_count - 3} segmente")

        return issues_on_disk

    def sync_json_with_disk_files(self):
        """SAFE: ÃmbogÄƒÈ›eÈ™te informaÈ›iile din JSON cu cele de pe disk, ZERO pierderi"""
        print("ğŸ”„ MERGE SAFE - combinez informaÈ›iile din JSON cu cele de pe disk...")

        # PASUL 1: ScaneazÄƒ complet disk-ul
        issues_on_disk = self.reconstruct_all_issues_from_disk()

        # PASUL 2: PÄ‚STREAZÄ‚ TOATE issue-urile existente din JSON (ZERO pierderi)
        existing_issues_by_url = {}
        for item in self.state.get("downloaded_issues", []):
            url = item.get("url", "").rstrip('/')
            existing_issues_by_url[url] = item.copy()  # DEEP COPY pentru siguranÈ›Äƒ

        print(f"ğŸ“‹ PÄ‚STREZ {len(existing_issues_by_url)} issue-uri din JSON existent")

        # PASUL 3: MERGE cu datele de pe disk (doar Ã®mbogÄƒÈ›eÈ™te, nu È™terge)
        enriched_count = 0
        new_from_disk_count = 0

        for issue_id, disk_data in issues_on_disk.items():
            url = disk_data['url']
            if not url:
                continue

            max_page = disk_data['max_page']
            segments_count = len(disk_data['segments'])
            is_complete = self.is_issue_complete_by_end_page(max_page)

            if url in existing_issues_by_url:
                # ÃMBOGÄ‚ÈšEÈ˜TE issue-ul existent (doar dacÄƒ progresul e mai mare)
                existing_issue = existing_issues_by_url[url]
                current_progress = existing_issue.get("last_successful_segment_end", 0)

                if max_page > current_progress:
                    # ÃMBOGÄ‚ÈšEÈ˜TE doar cÃ¢mpurile necesare, pÄƒstreazÄƒ restul
                    existing_issue["last_successful_segment_end"] = max_page
                    if not existing_issue.get("total_pages"):
                        existing_issue["total_pages"] = max_page
                    enriched_count += 1
                    print(f"ğŸ”„ ÃMBOGÄ‚ÈšIT: {url} - progres {current_progress} â†’ {max_page}")

                # MarcheazÄƒ ca complet DOAR dacÄƒ nu era deja marcat
                if is_complete and not existing_issue.get("completed_at"):
                    existing_issue["completed_at"] = datetime.now().isoformat(timespec="seconds")
                    existing_issue["pages"] = max_page
                    existing_issue["total_pages"] = max_page
                    print(f"âœ… MARCAT ca complet: {url} ({max_page} pagini)")

            else:
                # Issue complet nou gÄƒsit doar pe disk - ADAUGÄ‚
                new_issue = {
                    "url": url,
                    "title": issue_id.replace("-", " ").replace("_", " "),
                    "subtitle": "",
                    "pages": max_page if is_complete else 0,
                    "completed_at": datetime.now().isoformat(timespec="seconds") if is_complete else "",
                    "last_successful_segment_end": max_page,
                    "total_pages": max_page if is_complete else None
                }
                existing_issues_by_url[url] = new_issue
                new_from_disk_count += 1
                print(f"â• ADÄ‚UGAT nou din disk: {url} ({max_page} pagini, {segments_count} segmente)")

        # PASUL 4: ReconstruieÈ™te lista finalÄƒ (TOATE issue-urile pÄƒstrate)
        all_issues_list = list(existing_issues_by_url.values())

        # PASUL 5: SorteazÄƒ: parÈ›iale primele, apoi complete
        partial_issues = []
        complete_issues = []

        for issue in all_issues_list:
            is_partial = (issue.get("last_successful_segment_end", 0) > 0 and
                         not issue.get("completed_at") and
                         issue.get("total_pages") and
                         issue.get("last_successful_segment_end", 0) < issue.get("total_pages", 0))

            if is_partial:
                partial_issues.append(issue)
                print(f"ğŸ”„ Issue parÈ›ial: {issue['url']} ({issue.get('last_successful_segment_end', 0)}/{issue.get('total_pages', 0)} pagini)")
            else:
                complete_issues.append(issue)

        # SorteazÄƒ: parÈ›iale dupÄƒ progres (desc), complete dupÄƒ URL
        partial_issues.sort(key=lambda x: x.get("last_successful_segment_end", 0), reverse=True)
        complete_issues.sort(key=lambda x: x.get("url", ""))

        # PASUL 6: ActualizeazÄƒ starea SAFE (pÄƒstreazÄƒ tot ce nu modificÄƒm)
        original_count = self.state.get("count", 0)
        final_issues = partial_issues + complete_issues
        actual_complete_count = len([i for i in final_issues if i.get("completed_at")])

        # PÄ‚STREAZÄ‚ toate cÃ¢mpurile existente, actualizeazÄƒ doar ce e necesar
        self.state["downloaded_issues"] = final_issues
        self.state["count"] = max(original_count, actual_complete_count)  # Nu scade niciodatÄƒ

        self._save_state_safe()

        print(f"âœ… MERGE COMPLET - ZERO pierderi:")
        print(f"   ğŸ“Š Total issues: {len(final_issues)} (Ã®nainte: {len(existing_issues_by_url) - new_from_disk_count})")
        print(f"   ğŸ”„ ÃmbogÄƒÈ›ite: {enriched_count}")
        print(f"   â• AdÄƒugate din disk: {new_from_disk_count}")
        print(f"   ğŸ”„ ParÈ›iale: {len(partial_issues)}")
        print(f"   âœ… Complete: {len(complete_issues)}")
        print(f"   ğŸ¯ Count pÄƒstrat/actualizat: {original_count} â†’ {self.state['count']}")

        if partial_issues:
            print("ğŸ¯ Issue-urile parÈ›iale vor fi procesate primele!")

    def cleanup_duplicate_issues(self):
        """NOUÄ‚ FUNCÈšIE: EliminÄƒ dublurile din state.json"""
        print("ğŸ§¹ CURÄ‚ÈšARE: Verific È™i elimin dublurile din state.json...")

        issues = self.state.get("downloaded_issues", [])
        if not issues:
            return

        # GrupeazÄƒ dupÄƒ URL normalizat
        url_groups = {}
        for i, item in enumerate(issues):
            url = item.get("url", "").rstrip('/').lower()
            if not url:
                continue

            if url not in url_groups:
                url_groups[url] = []
            url_groups[url].append((i, item))

        # GÄƒseÈ™te È™i rezolvÄƒ dublurile
        duplicates_found = 0
        clean_issues = []
        processed_urls = set()

        for original_url, group in url_groups.items():
            if len(group) > 1:
                duplicates_found += 1
                print(f"ğŸ” DUBLURÄ‚ gÄƒsitÄƒ pentru {original_url}: {len(group)} intrÄƒri")

                # GÄƒseÈ™te cea mai completÄƒ versiune
                best_item = None
                best_score = -1

                for idx, item in group:
                    score = 0
                    if item.get("completed_at"): score += 100
                    if item.get("total_pages"): score += 50
                    if item.get("title"): score += 10
                    if item.get("last_successful_segment_end", 0) > 0: score += 20

                    print(f"   ğŸ“Š Index {idx}: score {score}, completed: {bool(item.get('completed_at'))}")

                    if score > best_score:
                        best_score = score
                        best_item = item

                print(f"   âœ… PÄƒstrez cea mai completÄƒ versiune (score: {best_score})")
                clean_issues.append(best_item)
            else:
                # Nu e dublurÄƒ, pÄƒstreazÄƒ-l
                clean_issues.append(group[0][1])

            processed_urls.add(original_url)

        if duplicates_found > 0:
            print(f"ğŸ§¹ ELIMINAT {duplicates_found} dubluri din {len(issues)} issues")
            print(f"ğŸ“Š RÄƒmas cu {len(clean_issues)} issues unice")

            self.state["downloaded_issues"] = clean_issues
            self._save_state_safe()
        else:
            print("âœ… Nu am gÄƒsit dubluri Ã®n state.json")

    def get_pending_partial_issues(self):
        """FIXED: ReturneazÄƒ issue-urile parÈ›iale care trebuie continuate"""
        pending_partials = []

        for item in self.state.get("downloaded_issues", []):
            url = item.get("url", "").rstrip('/')
            last_segment = item.get("last_successful_segment_end", 0)
            total_pages = item.get("total_pages")
            completed_at = item.get("completed_at", "")

            # Skip URL-urile complet descÄƒrcate
            if url in self.dynamic_skip_urls:
                continue

            # Issue parÈ›ial: are progres, nu e complet, È™i mai sunt pagini de descÄƒrcat
            if (last_segment > 0 and
                not completed_at and
                total_pages and
                last_segment < total_pages):

                pending_partials.append(item)
                print(f"ğŸ”„ Issue parÈ›ial gÄƒsit: {url} (pagini {last_segment}/{total_pages})")

        return pending_partials

    def _normalize_downloaded_issues(self, raw):
        normalized = []
        for item in raw:
            if isinstance(item, str):
                normalized.append({
                    "url": item.rstrip('/'),
                    "title": "",
                    "subtitle": "",
                    "pages": 0,
                    "completed_at": "",
                    "last_successful_segment_end": 0,
                    "total_pages": None
                })
            elif isinstance(item, dict):
                normalized.append({
                    "url": item.get("url", "").rstrip('/'),
                    "title": item.get("title", ""),
                    "subtitle": item.get("subtitle", ""),
                    "pages": item.get("pages", 0),
                    "completed_at": item.get("completed_at", ""),
                    "last_successful_segment_end": item.get("last_successful_segment_end", 0),
                    "total_pages": item.get("total_pages")
                })
        return normalized

    def _load_state(self):
        """ULTRA SAFE: Nu È™terge NICIODATÄ‚ datele existente"""
        today = datetime.now().strftime("%Y-%m-%d")

        if os.path.exists(self.state_path):
            try:
                with open(self.state_path, "r", encoding="utf-8") as f:
                    loaded = json.load(f)
                    loaded = self._decode_unicode_escapes(loaded)

                # PÄ‚STREAZÄ‚ TOATE issue-urile existente - ZERO È˜TERS
                existing_issues = self._normalize_downloaded_issues(loaded.get("downloaded_issues", []))

                print(f"ğŸ“‹ ÃNCÄ‚RCAT {len(existing_issues)} issue-uri din state.json")

                # GÄƒseÈ™te issue-urile parÈ›iale
                partial_issues = []
                for issue in existing_issues:
                    last_segment = issue.get("last_successful_segment_end", 0)
                    total_pages = issue.get("total_pages")
                    completed_at = issue.get("completed_at", "")

                    if (last_segment > 0 and not completed_at and total_pages and last_segment < total_pages):
                        partial_issues.append(issue)
                        print(f"ğŸ”„ PARÈšIAL: {issue['url']} - {last_segment}/{total_pages} pagini")

                complete_count = len([i for i in existing_issues if i.get("completed_at")])

                # PÄ‚STREAZÄ‚ TOT - doar actualizeazÄƒ data
                self.state = {
                    "date": today,
                    "count": loaded.get("count", complete_count),
                    "downloaded_issues": existing_issues,  # TOATE PÄ‚STRATE
                    "pages_downloaded": loaded.get("pages_downloaded", 0),
                    "recent_links": loaded.get("recent_links", []),
                    "daily_limit_hit": False,
                    "main_collection_completed": loaded.get("main_collection_completed", False),
                    "current_additional_collection_index": loaded.get("current_additional_collection_index", 0)
                }

                print(f"âœ… PÄ‚STRAT TOT: {complete_count} complete, {len(partial_issues)} parÈ›iale")

            except Exception as e:
                print(f"âŒ JSON CORRUPT: {e}")
                print(f"ğŸ› ï¸ RECUPEREZ din backup sau disk...")

                # ÃncearcÄƒ backup
                backup_path = self.state_path + ".backup"
                if os.path.exists(backup_path):
                    print(f"ğŸ”„ Restabilesc din backup...")
                    shutil.copy2(backup_path, self.state_path)
                    return self._load_state()  # Recursiv cu backup

                # Altfel Ã®ncepe gol dar SCANEAZÄ‚ DISK-UL
                print(f"ğŸ” SCANEZ DISK-UL pentru recuperare...")
                self.state = {
                    "date": today,
                    "count": 0,
                    "downloaded_issues": [],
                    "pages_downloaded": 0,
                    "recent_links": [],
                    "daily_limit_hit": False,
                    "main_collection_completed": False,
                    "current_additional_collection_index": 0
                }
        else:
            print(f"ğŸ“„ Nu existÄƒ state.json")
            self.state = {
                "date": today,
                "count": 0,
                "downloaded_issues": [],
                "pages_downloaded": 0,
                "recent_links": [],
                "daily_limit_hit": False,
                "main_collection_completed": False,
                "current_additional_collection_index": 0
            }

        self._save_state()

    def _save_state_safe(self):
        """SAFE: SalveazÄƒ starea doar dacÄƒ existÄƒ modificÄƒri, pÄƒstreazÄƒ backup"""
        try:
            # CreeazÄƒ backup Ã®nainte de salvare
            if os.path.exists(self.state_path):
                backup_path = self.state_path + ".backup"
                shutil.copy2(self.state_path, backup_path)

            with open(self.state_path, "w", encoding="utf-8") as f:
                json.dump(self.state, f, indent=2, ensure_ascii=False)

        except Exception as e:
            print(f"âš  Nu am putut salva state-ul: {e}")
            # ÃncearcÄƒ sÄƒ restabileascÄƒ din backup
            backup_path = self.state_path + ".backup"
            if os.path.exists(backup_path):
                print(f"ğŸ”„ Ãncerc sÄƒ restabilesc din backup...")
                try:
                    shutil.copy2(backup_path, self.state_path)
                    print(f"âœ… State restabilit din backup")
                except:
                    print(f"âŒ Nu am putut restabili din backup")

    def _save_state(self):
        """WRAPPER: FoloseÈ™te salvarea safe"""
        self._save_state_safe()

    def fix_existing_json(self):
        """FuncÈ›ie temporarÄƒ pentru a repara caracterele din JSON existent"""
        if os.path.exists(self.state_path):
            with open(self.state_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            data = self._decode_unicode_escapes(data)

            with open(self.state_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            print("âœ… JSON reparat cu caractere romÃ¢neÈ™ti")

    def remaining_quota(self):
        return max(0, DAILY_LIMIT - self.state.get("count", 0))

    def _update_partial_issue_progress(self, issue_url, last_successful_segment_end, total_pages=None, title=None, subtitle=None):
        """FIXED: Previne dublurile - verificÄƒ È™i dupÄƒ title dacÄƒ URL-ul nu se potriveÈ™te"""
        normalized = issue_url.rstrip('/')
        updated = False

        # STEP 1: CautÄƒ dupÄƒ URL exact
        for i, item in enumerate(self.state.setdefault("downloaded_issues", [])):
            if item["url"] == normalized:
                # ACTUALIZEAZÄ‚ issue-ul existent
                if last_successful_segment_end > item.get("last_successful_segment_end", 0):
                    item["last_successful_segment_end"] = last_successful_segment_end

                if total_pages is not None and not item.get("total_pages"):
                    item["total_pages"] = total_pages

                if title and not item.get("title"):
                    item["title"] = title

                if subtitle and not item.get("subtitle"):
                    item["subtitle"] = subtitle

                # MutÄƒ la Ã®nceput pentru prioritate
                updated_item = self.state["downloaded_issues"].pop(i)
                self.state["downloaded_issues"].insert(0, updated_item)
                updated = True
                print(f"ğŸ”„ ACTUALIZAT progres pentru: {normalized} â†’ {last_successful_segment_end} pagini")
                break

        # STEP 2: DacÄƒ nu gÄƒseÈ™ti dupÄƒ URL, cautÄƒ dupÄƒ title (prevenire dubluri)
        if not updated and title:
            for i, item in enumerate(self.state["downloaded_issues"]):
                if item.get("title") == title and not item["url"].startswith("http"):
                    # GÄ‚SIT dublu cu title ca URL - È™terge-l!
                    print(f"ğŸ—‘ï¸ È˜TERG DUBLU GREÈ˜IT: {item['url']} (era title Ã®n loc de URL)")
                    self.state["downloaded_issues"].pop(i)
                    break

        # STEP 3: Doar dacÄƒ nu existÄƒ deloc, creeazÄƒ nou
        if not updated:
            # VALIDEAZÄ‚ cÄƒ URL-ul e corect
            if not normalized.startswith("https://"):
                print(f"âŒ URL INVALID: {normalized} - nu creez issue nou!")
                return

            new_issue = {
                "url": normalized,
                "title": title or "",
                "subtitle": subtitle or "",
                "pages": 0,
                "completed_at": "",
                "last_successful_segment_end": last_successful_segment_end,
                "total_pages": total_pages
            }
            self.state["downloaded_issues"].insert(0, new_issue)
            print(f"â• ADÄ‚UGAT issue nou Ã®n progres: {normalized}")

        self._save_state_safe()
        print(f"ğŸ’¾ Progres salvat SAFE: {normalized} - pagini {last_successful_segment_end}/{total_pages or '?'}")

    def mark_issue_done(self, issue_url, pages_count, title=None, subtitle=None, total_pages=None):
        """FIXED: SeteazÄƒ OBLIGATORIU total_pages corect È™i evitÄƒ dublurile"""
        normalized = issue_url.rstrip('/')
        now_iso = datetime.now().isoformat(timespec="seconds")

        # VERIFICARE OBLIGATORIE: total_pages trebuie sÄƒ fie setat corect
        if total_pages is None or total_pages <= 0:
            print(f"âš  EROARE: total_pages invalid ({total_pages}) pentru {normalized}")
            print(f"ğŸ”§ Setez total_pages = pages_count ({pages_count}) ca fallback")
            total_pages = pages_count

        # VERIFICARE LOGICÄ‚: pages_count nu poate fi mai mare decÃ¢t total_pages
        if pages_count > total_pages:
            print(f"âš  EROARE LOGICÄ‚: pages_count ({pages_count}) > total_pages ({total_pages})")
            print(f"ğŸ”§ Corectez total_pages = pages_count")
            total_pages = pages_count

        existing = None
        existing_index = -1

        # CÄ‚UTARE ÃMBUNÄ‚TÄ‚ÈšITÄ‚: Ã®ncearcÄƒ mai multe variante de URL
        search_variants = [
            normalized,
            normalized + '/',
            normalized.replace('https://', 'http://'),
            normalized.replace('http://', 'https://')
        ]

        for i, item in enumerate(self.state.setdefault("downloaded_issues", [])):
            item_url = item.get("url", "").rstrip('/')
            if item_url in search_variants or normalized in [item_url, item_url + '/']:
                existing = item
                existing_index = i
                print(f"ğŸ” GÄ‚SIT issue existent la index {i}: {item_url}")
                break

        # CreeazÄƒ record-ul de completare
        completion_data = {
            "pages": pages_count,
            "completed_at": now_iso,
            "last_successful_segment_end": pages_count,
            "total_pages": total_pages  # SETEAZÄ‚ ÃNTOTDEAUNA!
        }

        # AdaugÄƒ title/subtitle doar dacÄƒ nu existÄƒ sau sunt goale
        if title:
            completion_data["title"] = title
        if subtitle:
            completion_data["subtitle"] = subtitle

        if existing:
            # ÃMBOGÄ‚ÈšEÈ˜TE issue-ul existent
            for key, value in completion_data.items():
                if key in ["title", "subtitle"]:
                    if not existing.get(key):
                        existing[key] = value
                else:
                    existing[key] = value

            # NU muta la Ã®nceput - pÄƒstreazÄƒ poziÈ›ia cronologicÄƒ
            print(f"âœ… ACTUALIZAT issue existent (pÄƒstrat la poziÈ›ia {existing_index}): {normalized}")
        else:
            # CreeazÄƒ issue nou complet
            new_record = {
                "url": normalized,
                "title": title or "",
                "subtitle": subtitle or "",
                **completion_data
            }

            # ADAUGÄ‚ LA SFÃ‚RÈ˜IT pentru cronologie corectÄƒ
            self.state["downloaded_issues"].append(new_record)
            print(f"â• ADÄ‚UGAT issue nou la sfÃ¢rÈ™itul listei: {normalized}")

        # ActualizeazÄƒ contoarele SAFE
        completed_count = len([i for i in self.state["downloaded_issues"] if i.get("completed_at")])
        self.state["count"] = max(self.state.get("count", 0), completed_count)

        # ActualizeazÄƒ pages_downloaded SAFE
        current_pages = self.state.get("pages_downloaded", 0)
        self.state["pages_downloaded"] = current_pages + pages_count

        # AdaugÄƒ Ã®n recent_links (pÄƒstreazÄƒ max 10)
        recent_entry = {
            "url": normalized,
            "title": (existing and existing.get("title")) or title or "",
            "subtitle": (existing and existing.get("subtitle")) or subtitle or "",
            "pages": pages_count,
            "timestamp": now_iso
        }
        recent_links = self.state.setdefault("recent_links", [])
        recent_links.insert(0, recent_entry)
        self.state["recent_links"] = recent_links[:10]

        # ReseteazÄƒ flag-ul de limitÄƒ
        self.state["daily_limit_hit"] = False

        # AdaugÄƒ Ã®n skip URLs
        self.dynamic_skip_urls.add(normalized)

        self._save_state_safe()
        self._save_skip_urls()

        print(f"âœ… Issue marcat ca terminat FIXED: {normalized}")
        print(f"ğŸ“Š Detalii: {pages_count} pagini, total_pages: {total_pages}")
        print(f"ğŸ“Š Total complet: {self.state['count']}, Total pagini: {self.state['pages_downloaded']}")

    def mark_collection_complete(self, collection_url):
        """MarcheazÄƒ o colecÈ›ie ca fiind complet procesatÄƒ Ã®n skip_urls.json"""
        try:
            normalized_collection = collection_url.rstrip('/')

            # AdaugÄƒ Ã®n dynamic skip URLs
            self.dynamic_skip_urls.add(normalized_collection)

            # SalveazÄƒ Ã®n skip_urls.json cu un marker special pentru colecÈ›ii
            skip_data = {}
            if os.path.exists(self.skip_urls_path):
                with open(self.skip_urls_path, "r", encoding="utf-8") as f:
                    skip_data = json.load(f)

            completed_collections = skip_data.get("completed_collections", [])
            if normalized_collection not in completed_collections:
                completed_collections.append(normalized_collection)
                skip_data["completed_collections"] = completed_collections
                skip_data["last_updated"] = datetime.now().isoformat()

                with open(self.skip_urls_path, "w", encoding="utf-8") as f:
                    json.dump(skip_data, f, indent=2, ensure_ascii=False)

                print(f"âœ… ColecÈ›ia marcatÄƒ ca completÄƒ: {normalized_collection}")
        except Exception as e:
            print(f"âš  Eroare la marcarea colecÈ›iei complete: {e}")

    def setup_chrome_driver(self):
        try:
            print("ğŸ”§ IniÈ›ializare WebDriver â€“ Ã®ncerc conectare la instanÈ›a Chrome existentÄƒ via remote debugging...")
            chrome_options = Options()
            chrome_options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")
            prefs = {
                "download.default_directory": os.path.abspath(self.download_dir),
                "download.prompt_for_download": False,
                "download.directory_upgrade": True,
                "safebrowsing.enabled": True,
            }
            chrome_options.add_experimental_option("prefs", prefs)
            try:
                self.driver = webdriver.Chrome(options=chrome_options)
                self.wait = WebDriverWait(self.driver, self.timeout)
                self.attached_existing = True
                print("âœ… Conectat la instanÈ›a Chrome existentÄƒ cu succes.")
                return True
            except WebDriverException as e:
                print(f"âš  Conexiune la Chrome existent eÈ™uat ({e}); pornesc o instanÈ›Äƒ nouÄƒ.")
                chrome_options = Options()
                chrome_options.add_experimental_option("prefs", prefs)
                chrome_options.add_argument("--no-sandbox")
                chrome_options.add_argument("--disable-dev-shm-usage")
                chrome_options.add_argument("--disable-gpu")
                chrome_options.add_argument("--window-size=1920,1080")
                chrome_options.add_argument("--incognito")
                self.driver = webdriver.Chrome(options=chrome_options)
                self.wait = WebDriverWait(self.driver, self.timeout)
                self.attached_existing = False
                print("âœ… Chrome nou pornit cu succes.")
                return True
        except WebDriverException as e:
            print(f"âŒ Eroare la iniÈ›ializarea WebDriver-ului: {e}")
            return False

    def navigate_to_page(self, url):
        try:
            print(f"ğŸŒ Navighez cÄƒtre: {url}")
            self.driver.get(url)
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'body')))
            print("âœ… Pagina Ã®ncÄƒrcatÄƒ.")
            return True
        except Exception as e:
            print(f"âŒ Eroare la navigare sau Ã®ncÄƒrcare: {e}")
            return False

    def get_issue_metadata(self):
        title = ""
        subtitle = ""
        try:
            breadcrumb = self.driver.find_element(By.CSS_SELECTOR, "li.breadcrumb-item.active")
            try:
                sub_elem = breadcrumb.find_element(By.CSS_SELECTOR, "#pdfview-pdfcontents span")
                subtitle = sub_elem.text.strip()
            except Exception:
                subtitle = ""
            raw = breadcrumb.text.strip()
            if subtitle and subtitle in raw:
                title = raw.replace(subtitle, "").strip()
            else:
                title = raw
        except Exception:
            pass
        return title, subtitle

    def get_total_pages(self, max_attempts=5, delay_between=1.0):
        """FIXED: DetecteazÄƒ corect numÄƒrul total de pagini"""
        for attempt in range(1, max_attempts + 1):
            try:
                # Metoda 1: CautÄƒ pattern-ul "numÄƒr / total" Ã®n mai multe locuri
                page_patterns = [
                    r'(\d+)\s*/\s*(\d+)',  # "249 / 1565"
                    r'/\s*(\d+)',          # "/ 1565"
                    r'of\s+(\d+)',         # "of 1565"
                ]

                # CautÄƒ Ã®n toate textele de pe paginÄƒ
                all_texts = self.driver.find_elements(By.XPATH, "//*[contains(text(), '/') or contains(text(), 'of')]")

                for el in all_texts:
                    text = el.text.strip()
                    for pattern in page_patterns:
                        matches = re.findall(pattern, text)
                        if matches:
                            if pattern == page_patterns[0]:  # "numÄƒr / total"
                                current, total = matches[0]
                                total = int(total)
                                print(f"âœ… TOTAL PAGINI detectat din '{text}': {total} (curent: {current})")
                                return total
                            else:  # "/ total" sau "of total"
                                total = int(matches[0])
                                print(f"âœ… TOTAL PAGINI detectat din '{text}': {total}")
                                return total

                # Metoda 2: JavaScript pentru cÄƒutare mai profundÄƒ
                js_result = self.driver.execute_script(r"""
                    const patterns = [
                        /(\d+)\s*\/\s*(\d+)/g,
                        /\/\s*(\d+)/g,
                        /of\s+(\d+)/g
                    ];

                    const walker = document.createTreeWalker(document.body, NodeFilter.SHOW_TEXT);
                    while(walker.nextNode()) {
                        const text = walker.currentNode.nodeValue;
                        for (let pattern of patterns) {
                            const matches = [...text.matchAll(pattern)];
                            if (matches.length > 0) {
                                const match = matches[0];
                                if (match.length === 3) {  // "numÄƒr / total"
                                    return {text: text, total: parseInt(match[2]), current: parseInt(match[1])};
                                } else {  // "/ total"
                                    return {text: text, total: parseInt(match[1])};
                                }
                            }
                        }
                    }
                    return null;
                """)

                if js_result:
                    total = js_result['total']
                    current = js_result.get('current', 0)
                    text = js_result['text']
                    print(f"âœ… TOTAL PAGINI detectat prin JS din '{text}': {total} (curent: {current})")
                    return total

                print(f"âš  ({attempt}) Nu am gÄƒsit Ã®ncÄƒ numÄƒrul total de pagini, reÃ®ncerc Ã®n {delay_between}s...")
                time.sleep(delay_between)

            except Exception as e:
                print(f"âš  ({attempt}) Eroare Ã®n get_total_pages: {e}")
                time.sleep(delay_between)

        print("âŒ Nu s-a reuÈ™it extragerea numÄƒrului total de pagini dupÄƒ multiple Ã®ncercÄƒri.")
        return 0

    def open_save_popup(self):
        try:
            try:
                self.wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR, 'div.MuiDialog-container')))
            except Exception:
                self.driver.switch_to.active_element.send_keys(Keys.ESCAPE)
                time.sleep(0.5)
            svg = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'svg[data-testid="SaveAltIcon"]')))
            button = svg
            if svg.tag_name.lower() == "svg":
                try:
                    button = svg.find_element(By.XPATH, "./ancestor::button")
                except Exception:
                    pass
            for attempt in range(1, 4):
                try:
                    button.click()
                    return True
                except ElementClickInterceptedException:
                    print(f"âš  click interceptat (Ã®ncercarea {attempt}), trimit ESC È™i reiau...")
                    self.driver.switch_to.active_element.send_keys(Keys.ESCAPE)
                    time.sleep(1)
                    continue
            print("âŒ Nu am reuÈ™it sÄƒ dau click pe butonul de deschidere a popup-ului dupÄƒ retry-uri.")
            return False
        except Exception as e:
            print(f"âŒ Nu am reuÈ™it sÄƒ deschid popup-ul de salvare: {e}")
            return False

    def fill_and_save_range(self, start, end):
        try:
            first_input = self.wait.until(EC.presence_of_element_located((By.ID, "first page")))
            print("â³ AÈ™tept 2s Ã®nainte de a completa primul input...")
            time.sleep(2)
            first_input.send_keys(Keys.CONTROL + "a")
            first_input.send_keys(str(start))
            print(f"âœï¸ Am introdus primul numÄƒr: {start}")
            print("â³ AÈ™tept 2s Ã®nainte de a completa al doilea input...")
            time.sleep(2)
            last_input = self.wait.until(EC.presence_of_element_located((By.ID, "last page")))
            last_input.send_keys(Keys.CONTROL + "a")
            last_input.send_keys(str(end))
            print(f"âœï¸ Am introdus al doilea numÄƒr: {end}")
            print("â³ AÈ™tept 2s Ã®nainte de a apÄƒsa butonul de salvare...")
            time.sleep(2)
            save_btn = self.wait.until(EC.element_to_be_clickable((
                By.XPATH,
                '//button[.//text()[contains(normalize-space(.), "SalvaÈ›i") or contains(normalize-space(.), "Save")]]'
            )))
            save_btn.click()
            print(f"âœ… Segmentul {start}-{end} salvat.")
            return True
        except Exception as e:
            print(f"âŒ Eroare la completarea/salvarea intervalului {start}-{end}: {e}")
            return False

    def check_daily_limit_in_all_windows(self, set_flag=True):
        """VerificÄƒ mesajul de limitÄƒ zilnicÄƒ Ã®n toate ferestrele deschise"""
        current_window = self.driver.current_window_handle
        limit_reached = False

        try:
            all_handles = self.driver.window_handles
            for handle in all_handles:
                try:
                    self.driver.switch_to.window(handle)
                    body_text = self.driver.find_element(By.TAG_NAME, "body").text

                    if ("Daily download limit reached" in body_text or
                        "Terms and conditions" in body_text):
                        print(f"âš  Limita zilnicÄƒ detectatÄƒ Ã®n fereastra: {handle}")
                        limit_reached = True

                        if handle != current_window and len(all_handles) > 1:
                            print(f"ğŸ—™ Ãnchid fereastra cu limita zilnicÄƒ: {handle}")
                            self.driver.close()
                        break

                except Exception as e:
                    continue

            try:
                if current_window in self.driver.window_handles:
                    self.driver.switch_to.window(current_window)
                elif self.driver.window_handles:
                    self.driver.switch_to.window(self.driver.window_handles[0])
            except Exception:
                pass

        except Exception as e:
            print(f"âš  Eroare la verificarea ferestrelor: {e}")

        if limit_reached and set_flag:
            self.state["daily_limit_hit"] = True
            self._save_state()

        return limit_reached

    def check_for_daily_limit_popup(self):
        """VerificÄƒ dacÄƒ s-a deschis o filÄƒ nouÄƒ cu mesajul de limitÄƒ zilnicÄƒ dupÄƒ descÄƒrcare"""
        try:
            current_handles = set(self.driver.window_handles)

            # VerificÄƒ toate filele deschise pentru mesajul de limitÄƒ
            for handle in current_handles:
                try:
                    self.driver.switch_to.window(handle)
                    body_text = self.driver.find_element(By.TAG_NAME, "body").text.strip()

                    if "Daily download limit reached" in body_text:
                        print(f"ğŸ›‘ LIMITÄ‚ ZILNICÄ‚ DETECTATÄ‚ Ã®n filÄƒ nouÄƒ: {handle}")
                        print(f"ğŸ“„ ConÈ›inut filÄƒ: {body_text}")

                        # Ãnchide fila cu limita
                        self.driver.close()

                        # Revine la prima filÄƒ disponibilÄƒ
                        if self.driver.window_handles:
                            self.driver.switch_to.window(self.driver.window_handles[0])

                        # SeteazÄƒ flag-ul È™i opreÈ™te procesarea
                        self.state["daily_limit_hit"] = True
                        self._save_state()
                        return True

                except Exception as e:
                    continue

            return False

        except Exception as e:
            print(f"âš  Eroare la verificarea popup-ului de limitÄƒ: {e}")
            return False

    def save_page_range(self, start, end, retries=1):
        """FIXED: VerificÄƒ limita zilnicÄƒ dupÄƒ fiecare segment descÄƒrcat"""
        for attempt in range(1, retries + 2):
            print(f"ğŸ”„ Ãncep segmentul {start}-{end}, Ã®ncercarea {attempt}")

            if not self.open_save_popup():
                print(f"âš  EÈ™ec la deschiderea popup-ului pentru {start}-{end}")
                time.sleep(1)
                continue

            success = self.fill_and_save_range(start, end)
            if success:
                print("â³ AÈ™tept 5s pentru finalizarea descÄƒrcÄƒrii segmentului...")
                time.sleep(5)

                # VERIFICÄ‚ LIMITA ZILNICÄ‚ IMEDIAT DUPÄ‚ DESCÄ‚RCARE
                if self.check_for_daily_limit_popup():
                    print(f"ğŸ›‘ OPRIRE INSTANT - LimitÄƒ zilnicÄƒ detectatÄƒ dupÄƒ segmentul {start}-{end}")
                    return False

                print(f"âœ… Segmentul {start}-{end} descÄƒrcat cu succes")
                return True
            else:
                print(f"âš  Retry pentru segmentul {start}-{end}")
                time.sleep(1)
        print(f"âŒ RenunÈ› la segmentul {start}-{end} dupÄƒ {retries+1} Ã®ncercÄƒri.")
        return False

    def save_all_pages_in_batches(self, resume_from=1):
        """FIXED: OpreÈ™te instant la detectarea limitei zilnice"""
        total = self.get_total_pages()
        if total <= 0:
            print("âš  Nu am obÈ›inut numÄƒrul total de pagini; nu pot continua.")
            return 0, False

        bs = self.batch_size
        segments = []

        if resume_from == 1:
            first_end = min(bs - 1, total)
            if first_end >= 1:
                segments.append((1, first_end))
            current = bs
        else:
            current = resume_from

        while current <= total:
            end = min(current + bs - 1, total)
            segments.append((current, end))
            current += bs

        last_successful_page = resume_from - 1

        print(f"ğŸ¯ FOCUSEZ PE ACEST ISSUE: Voi descÄƒrca {len(segments)} segmente fÄƒrÄƒ Ã®ntreruperi")

        for (start, end) in segments:
            print(f"ğŸ“¦ Procesez segmentul {start}-{end}")
            result = self.save_page_range(start, end, retries=1)

            if not result:
                # VERIFICÄ‚ DACÄ‚ EÈ˜ECUL E DIN CAUZA LIMITEI ZILNICE
                if self.state.get("daily_limit_hit", False):
                    print(f"ğŸ›‘ OPRIRE INSTANT - LimitÄƒ zilnicÄƒ atinsÄƒ la segmentul {start}-{end}")
                    return last_successful_page, True  # ReturneazÄƒ TRUE pentru limitÄƒ zilnicÄƒ

                print(f"âŒ EÈ™ec persistent la segmentul {start}-{end}, opresc acest issue.")
                return last_successful_page, False

            last_successful_page = end
            self._update_partial_issue_progress(self.current_issue_url, end, total_pages=total)
            print(f"âœ… Progres real salvat: pagini pÃ¢nÄƒ la {end}")
            time.sleep(1)

        print("ğŸ¯ Toate segmentele au fost procesate pentru acest issue.")
        return total, False

    def extract_issue_links_from_collection(self):
        try:
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.list-group')))
            anchors = self.driver.find_elements(By.CSS_SELECTOR, 'li.list-group-item a[href^="/view/"], li.list-group-item a[href^="/en/view/"], li.list-group-item a[href^="/ro/view/"]')
            links = []
            for a in anchors:
                href = a.get_attribute("href")
                if href and '/view/' in href:
                    normalized = href.split('?')[0].rstrip('/')
                    links.append(normalized)
            unique = []
            seen = set()
            for l in links:
                if l not in seen:
                    seen.add(l)
                    unique.append(l)
            print(f"ğŸ”— Am gÄƒsit {len(unique)} linkuri de issue Ã®n colecÈ›ie.")
            return unique
        except Exception as e:
            print(f"âŒ Eroare la extragerea linkurilor din colecÈ›ie: {e}")
            return []

    def extract_page_range_from_filename(self, filename):
        """Extrage range-ul de pagini din numele fiÈ™ierului pentru sortare corectÄƒ"""
        match = re.search(r'__pages(\d+)-(\d+)\.pdf', filename)
        if match:
            start_page = int(match.group(1))
            end_page = int(match.group(2))
            return (start_page, end_page)
        return (0, 0)

    def copy_and_combine_issue_pdfs(self, issue_url: str, issue_title: str):
        """
        FIXED: GÄƒseÈ™te TOATE fiÈ™ierele pentru issue È™i le combinÄƒ corect
        """
        issue_id = issue_url.rstrip('/').split('/')[-1]
        folder_name = self._safe_folder_name(issue_title or issue_id)
        dest_dir = os.path.join(self.download_dir, folder_name)
        os.makedirs(dest_dir, exist_ok=True)

        print(f"ğŸ“ Procesez PDF-urile pentru '{issue_title}' cu ID '{issue_id}'")

        # â³ AÈ˜TEAPTÄ‚ CA TOATE FIÈ˜IERELE SÄ‚ FIE COMPLET DESCÄ‚RCATE
        print("â³ AÈ™tept 10 secunde ca toate fiÈ™ierele sÄƒ se termine de descÄƒrcat...")
        time.sleep(10)

        # PASUL 1: GÄƒseÈ™te TOATE fiÈ™ierele pentru acest issue
        all_segments = self.get_all_pdf_segments_for_issue(issue_url)

        if not all_segments:
            print(f"â„¹ï¸ Nu am gÄƒsit fiÈ™iere PDF pentru '{issue_title}' cu ID '{issue_id}'.")
            return

        print(f"ğŸ” Am gÄƒsit {len(all_segments)} fiÈ™iere PDF pentru '{issue_id}':")
        for seg in all_segments:
            print(f"   ğŸ“„ {seg['filename']} (pagini {seg['start']}-{seg['end']})")

        # PASUL 2: CopiazÄƒ TOATE fiÈ™ierele Ã®n folder
        copied_files = []
        for seg in all_segments:
            src = seg['path']
            dst = os.path.join(dest_dir, seg['filename'])
            try:
                shutil.copy2(src, dst)
                copied_files.append(dst)
                print(f"ğŸ“„ Copiat: {seg['filename']} â†’ {folder_name}/")
            except Exception as e:
                print(f"âš  Nu am reuÈ™it sÄƒ copiez {seg['filename']}: {e}")

        if not copied_files:
            print(f"âŒ Nu am reuÈ™it sÄƒ copiez niciun fiÈ™ier pentru '{issue_title}'.")
            return

        print(f"ğŸ“ Toate {len(copied_files)} PDF-urile pentru '{issue_title}' au fost copiate Ã®n '{dest_dir}'.")

        # PASUL 3: CombinÄƒ PDF-urile Ã®n ordinea corectÄƒ
        output_file = os.path.join(dest_dir, f"{folder_name}.pdf")

        try:
            if len(copied_files) > 1:
                print(f"ğŸ”— Combinez {len(copied_files)} fiÈ™iere PDF Ã®n ordinea corectÄƒ...")

                # SorteazÄƒ fiÈ™ierele dupÄƒ range-ul de pagini
                files_with_ranges = []
                for file_path in copied_files:
                    filename = os.path.basename(file_path)
                    start_page, end_page = self.extract_page_range_from_filename(filename)
                    files_with_ranges.append((start_page, end_page, file_path))

                # SorteazÄƒ dupÄƒ pagina de Ã®nceput
                files_with_ranges.sort(key=lambda x: x[0])
                sorted_files = [x[2] for x in files_with_ranges]

                # AfiÈ™eazÄƒ ordinea de combinare
                print("ğŸ“‹ Ordinea de combinare:")
                for start, end, path in files_with_ranges:
                    filename = os.path.basename(path)
                    print(f"   ğŸ“„ {filename} (pagini {start}-{end})")

                from PyPDF2 import PdfMerger
                merger = PdfMerger()

                for pdf_path in sorted_files:
                    try:
                        merger.append(pdf_path)
                        filename = os.path.basename(pdf_path)
                        print(f"   âœ… AdÄƒugat Ã®n ordine: {filename}")
                    except Exception as e:
                        print(f"   âš  Eroare la adÄƒugarea {pdf_path}: {e}")

                # Scrie fiÈ™ierul combinat
                merger.write(output_file)
                merger.close()

                # VerificÄƒ cÄƒ fiÈ™ierul combinat a fost creat cu succes
                if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
                    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)
                    print(f"âœ… FiÈ™ierul combinat creat cu succes: {file_size_mb:.2f} MB")

                    # È˜TERGE COPIILE DIN FOLDER
                    deleted_count = 0
                    total_deleted_size = 0

                    for file_to_delete in copied_files:
                        try:
                            file_size = os.path.getsize(file_to_delete)
                            os.remove(file_to_delete)
                            deleted_count += 1
                            total_deleted_size += file_size
                            print(f"   ğŸ—‘ï¸ È˜ters copia: {os.path.basename(file_to_delete)}")
                        except Exception as e:
                            print(f"   âš  Nu am putut È™terge copia {file_to_delete}: {e}")

                    deleted_size_mb = total_deleted_size / (1024 * 1024)
                    print(f"ğŸ‰ FINALIZAT: PÄƒstrat doar fiÈ™ierul combinat '{os.path.basename(output_file)}'")
                    print(f"ğŸ—‘ï¸ È˜terse {deleted_count} copii din folder ({deleted_size_mb:.2f} MB)")

                    # ğŸ”¥ PÄ‚STREAZÄ‚ ORIGINALELE PE D:\ - doar afiÈ™eazÄƒ ce s-ar È™terge
                    print(f"ğŸ’¾ INFORMAÈšIE: Pe D:\\ rÄƒmÃ¢n {len(all_segments)} fiÈ™iere originale pentru backup")
                    for seg in all_segments:
                        print(f"   ğŸ“„ Original: {seg['filename']}")

                else:
                    print(f"âŒ EROARE: FiÈ™ierul combinat nu a fost creat corect!")
                    print(f"ğŸ›¡ï¸ SIGURANÈšÄ‚: PÄƒstrez copiile pentru siguranÈ›Äƒ")

            elif len(copied_files) == 1:
                # Un singur fiÈ™ier - doar redenumeÈ™te copia
                original_file = copied_files[0]
                original_size_mb = os.path.getsize(original_file) / (1024 * 1024)

                try:
                    os.replace(original_file, output_file)
                    print(f"âœ… Copia redenumitÄƒ Ã®n: {os.path.basename(output_file)} ({original_size_mb:.2f} MB)")
                except Exception as e:
                    print(f"âš  Nu am putut redenumi copia {original_file}: {e}")

            else:
                print(f"â„¹ï¸ Nu existÄƒ fiÈ™iere PDF de combinat Ã®n '{dest_dir}'.")

        except Exception as e:
            print(f"âŒ EROARE la combinarea PDF-urilor: {e}")
            print(f"ğŸ›¡ï¸ SIGURANÈšÄ‚: PÄƒstrez copiile din cauza erorii")
            return

        # PASUL 4: Raport final
        try:
            if os.path.exists(output_file):
                final_size_mb = os.path.getsize(output_file) / (1024 * 1024)

                print(f"\nğŸ“‹ RAPORT FINAL pentru '{issue_title}':")
                print(f"   ğŸ“ Folder destinaÈ›ie: {dest_dir}")
                print(f"   ğŸ“„ FiÈ™ier final: {os.path.basename(output_file)} ({final_size_mb:.2f} MB)")
                print(f"   ğŸ” Combinat din {len(all_segments)} segmente originale")
                print(f"   ğŸ’¾ FiÈ™iere originale: PÄ‚STRATE pe D:\\ pentru backup")
                print(f"   âœ… STATUS: SUCCES - fiÈ™ier complet creat")
            else:
                print(f"âš  Nu s-a putut crea fiÈ™ierul final pentru '{issue_title}'")
        except Exception as e:
            print(f"âš  Eroare la raportul final: {e}")

        print(f"=" * 60)

    def find_next_issue_in_collection_order(self, collection_links, last_completed_url):
        """
        FIXED: GÄƒseÈ™te urmÄƒtorul issue de procesat Ã®n ordinea din HTML, nu primul din listÄƒ
        """
        if not last_completed_url:
            # DacÄƒ nu avem istoric, Ã®ncepe cu primul din listÄƒ
            return collection_links[0] if collection_links else None

        try:
            last_index = collection_links.index(last_completed_url.rstrip('/'))
            # ReturneazÄƒ urmÄƒtorul din listÄƒ dupÄƒ cel completat
            next_index = last_index + 1
            if next_index < len(collection_links):
                next_url = collection_links[next_index]
                print(f"ğŸ¯ UrmÄƒtorul issue dupÄƒ '{last_completed_url}' este: '{next_url}'")
                return next_url
            else:
                print(f"âœ… Toate issue-urile din colecÈ›ie au fost procesate!")
                return None
        except ValueError:
            # DacÄƒ last_completed_url nu e Ã®n lista curentÄƒ, Ã®ncepe cu primul
            print(f"âš  URL-ul '{last_completed_url}' nu e Ã®n colecÈ›ia curentÄƒ, Ã®ncep cu primul din listÄƒ")
            return collection_links[0] if collection_links else None

    def get_last_completed_issue_from_collection(self, collection_links):
        """
        GÄƒseÈ™te ultimul issue complet descÄƒrcat din colecÈ›ia curentÄƒ
        """
        for item in self.state.get("downloaded_issues", []):
            url = item.get("url", "").rstrip('/')
            if url in [link.rstrip('/') for link in collection_links]:
                if item.get("completed_at"):  # Issue complet
                    print(f"ğŸ Ultimul issue complet din colecÈ›ie: {url}")
                    return url

        print("ğŸ†• Niciun issue complet gÄƒsit Ã®n colecÈ›ia curentÄƒ")
        return None

    def open_new_tab_and_download(self, url):
        """FIXED: Se focuseazÄƒ pe un singur issue pÃ¢nÄƒ la final cu sincronizare completÄƒ"""
        normalized_url = url.rstrip('/')

        # DOAR verificÄƒrile esenÈ›iale la Ã®nceput
        if normalized_url in self.dynamic_skip_urls:
            print(f"â­ï¸ Sar peste {url} (Ã®n skip list).")
            return False

        already_done = any(
            item.get("url") == normalized_url and item.get("completed_at") and
            item.get("last_successful_segment_end", 0) >= (item.get("total_pages") or float('inf'))
            for item in self.state.get("downloaded_issues", [])
        )
        if already_done:
            print(f"â­ï¸ Sar peste {url} (deja descÄƒrcat complet).")
            return False

        print(f"\nğŸ¯ ÃNCEP FOCUSAREA PE: {url}")
        print("=" * 60)

        try:
            if not self.attached_existing:
                self.ensure_alive_fallback()

            # Deschide fila nouÄƒ
            prev_handles = set(self.driver.window_handles)
            self.driver.execute_script("window.open('');")
            new_handles = set(self.driver.window_handles)
            diff = new_handles - prev_handles
            new_handle = diff.pop() if diff else self.driver.window_handles[-1]
            self.driver.switch_to.window(new_handle)

            if not self.navigate_to_page(url):
                print(f"âŒ Nu am putut naviga la {url}")
                return False

            time.sleep(2)  # PauzÄƒ pentru Ã®ncÄƒrcarea completÄƒ

            # VerificÄƒ DOAR o datÄƒ la Ã®nceput pentru limitÄƒ
            if self.check_daily_limit_in_all_windows(set_flag=False):
                print("âš  PaginÄƒ cu limitÄƒ zilnicÄƒ detectatÄƒ - opresc aici.")
                self.state["daily_limit_hit"] = True
                self._save_state()
                return False

            print("âœ… Pagina e OK, Ã®ncep extragerea metadatelor...")
            title, subtitle = self.get_issue_metadata()

            # FIXED: ScaneazÄƒ corect fiÈ™ierele existente pentru acest issue specific
            existing_pages = self.get_existing_pdf_segments(url)
            print(f"ğŸ“Š Pagini existente pe disk: {existing_pages}")

            resume_from = 1
            json_progress = 0
            for item in self.state.get("downloaded_issues", []):
                if item.get("url") == normalized_url:
                    json_progress = item.get("last_successful_segment_end", 0)
                    total_pages = item.get("total_pages")
                    if json_progress and total_pages and json_progress >= total_pages:
                        resume_from = None
                    break

            actual_progress = max(json_progress, existing_pages)
            if actual_progress > 0 and resume_from is not None:
                resume_from = actual_progress + 1
                print(f"ğŸ“„ Reiau de la pagina {resume_from} (JSON: {json_progress}, Disk: {existing_pages})")

            if resume_from is None:
                print(f"â­ï¸ Issue-ul {url} este deja complet.")
                return False

            self.current_issue_url = normalized_url

            # FIXED: ObÈ›ine total_pages È™i actualizeazÄƒ progresul
            total_pages = self.get_total_pages()
            if total_pages > 0:
                self._update_partial_issue_progress(normalized_url, actual_progress, total_pages=total_pages, title=title, subtitle=subtitle)
            else:
                print("âš  Nu am putut obÈ›ine numÄƒrul total de pagini!")

            print(f"ğŸ”’ INTRÃ‚ND ÃN MODUL FOCUS - nu mai fac alte verificÄƒri pÃ¢nÄƒ nu termin!")

            # ==================== DESCÄ‚RCAREA PROPRIU-ZISÄ‚ ====================
            print(f"ğŸ“¥ ÃNCEPE DESCÄ‚RCAREA pentru {url}...")
            pages_done, limit_hit = self.save_all_pages_in_batches(resume_from=resume_from)

            if pages_done == 0 and not limit_hit:
                print(f"âš  DescÄƒrcarea pentru {url} a eÈ™uat complet.")
                return False

            if limit_hit:
                print(f"âš  LimitÄƒ zilnicÄƒ atinsÄƒ Ã®n timpul descÄƒrcÄƒrii pentru {url}; progresul parÈ›ial a fost salvat.")
                return False

            # ==================== DESCÄ‚RCAREA S-A TERMINAT ====================
            print(f"âœ… DESCÄ‚RCAREA COMPLETÄ‚ pentru {url} ({pages_done} pagini)")

            # PAUZÄ‚ CRITICÄ‚ 1: AÈ™teaptÄƒ ca toate fiÈ™ierele sÄƒ fie complet scrise pe disk
            print("â³ SINCRONIZARE: AÈ™tept 30 secunde ca toate fiÈ™ierele sÄƒ fie complet salvate pe disk...")
            time.sleep(30)

            # FIXED: MarcheazÄƒ ca terminat cu total_pages corect
            final_total = total_pages if total_pages > 0 else pages_done
            self.mark_issue_done(url, pages_done, title=title, subtitle=subtitle, total_pages=final_total)
            print(f"âœ… Issue marcat ca terminat Ã®n JSON: {url} ({pages_done} pagini)")

            # PAUZÄ‚ CRITICÄ‚ 2: AÈ™teaptÄƒ ca JSON sÄƒ fie salvat complet
            print("â³ SINCRONIZARE: AÈ™tept 5 secunde pentru sincronizarea JSON...")
            time.sleep(5)

            # ==================== PROCESAREA PDF-URILOR ====================
            print(f"ğŸ”„ ÃNCEPE PROCESAREA PDF-URILOR pentru {url}...")

            # VerificÄƒ din nou cÄƒ toate fiÈ™ierele sunt pe disk
            final_segments = self.get_all_pdf_segments_for_issue(url)
            print(f"ğŸ” VERIFICARE FINALÄ‚: Am gÄƒsit {len(final_segments)} fiÈ™iere PDF pentru acest issue")

            if len(final_segments) == 0:
                print(f"âš  PROBLEMÄ‚: Nu am gÄƒsit fiÈ™iere PDF pentru {url}!")
                return False

            # CopiazÄƒ È™i combinÄƒ PDF-urile
            self.copy_and_combine_issue_pdfs(url, title or normalized_url)

            # PAUZÄ‚ CRITICÄ‚ 3: AÈ™teaptÄƒ ca procesarea PDF sÄƒ fie completÄƒ
            print("â³ SINCRONIZARE: AÈ™tept 15 secunde dupÄƒ procesarea PDF-urilor...")
            time.sleep(15)

            # ==================== FINALIZARE COMPLETÄ‚ ====================
            print("=" * 60)
            print(f"ğŸ‰ FOCUSAREA COMPLETÄ‚ PE {url} FINALIZATÄ‚ CU SUCCES!")
            print(f"ğŸ“Š REZULTAT: {pages_done} pagini descÄƒrcate È™i procesate")
            print("=" * 60)

            # PAUZÄ‚ FINALÄ‚: Ãnainte sÄƒ treacÄƒ la urmÄƒtorul issue
            print("â³ PAUZÄ‚ FINALÄ‚: 10 secunde Ã®nainte de urmÄƒtorul issue...")
            time.sleep(10)

            return True

        except WebDriverException as e:
            print(f"âŒ Eroare WebDriver pentru {url}: {e}")
            return False
        except Exception as e:
            print(f"âŒ Eroare Ã®n open_new_tab_and_download pentru {url}: {e}")
            return False
        finally:
            try:
                self.driver.close()
                if self.driver.window_handles:
                    self.driver.switch_to.window(self.driver.window_handles[0])
            except Exception:
                pass

    def ensure_alive_fallback(self):
        try:
            _ = self.driver.title
        except Exception as e:
            print(f"âš  Conexiune WebDriver moartÄƒ ({e}), pornesc instanÈ›Äƒ nouÄƒ ca fallback.")
            prefs = {
                "download.default_directory": os.path.abspath(self.download_dir),
                "download.prompt_for_download": False,
                "download.directory_upgrade": True,
                "safebrowsing.enabled": True,
            }
            chrome_options = Options()
            chrome_options.add_experimental_option("prefs", prefs)
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--incognito")
            self.driver = webdriver.Chrome(options=chrome_options)
            self.wait = WebDriverWait(self.driver, self.timeout)
            self.attached_existing = False

    def run_collection(self, collection_url):
        """FIXED: ProceseazÄƒ UN SINGUR issue pe rÃ¢nd, fÄƒrÄƒ sÄƒ sarÄƒ la altele"""
        print(f"ğŸŒ Ãncep procesarea colecÈ›iei: {collection_url}")
        if not self.driver:
            print("âŒ Driver neiniÈ›ializat.")
            return False
        if not self.navigate_to_page(collection_url):
            return False

        # VerificÄƒ limita DOAR la Ã®nceput
        if self.state.get("daily_limit_hit", False):
            print("âš  Nu mai pot continua din cauza limitei zilnice setate anterior.")
            return True

        if self.remaining_quota() <= 0:
            print(f"âš  Limita zilnicÄƒ de {DAILY_LIMIT} issue-uri atinsÄƒ.")
            return True

        issue_links = self.extract_issue_links_from_collection()
        if not issue_links:
            print("âš  Nu s-au gÄƒsit issue-uri Ã®n colecÈ›ie.")
            return False

        # FIXED: GÄƒseÈ™te ultimul issue complet din colecÈ›ie È™i continuÄƒ cu urmÄƒtorul
        last_completed = self.get_last_completed_issue_from_collection(issue_links)
        next_to_process = self.find_next_issue_in_collection_order(issue_links, last_completed)

        if not next_to_process:
            print("âœ… Toate issue-urile din aceastÄƒ colecÈ›ie sunt complete!")
            return True

        # ProceseazÄƒ issue-urile Ã®ncepÃ¢nd cu urmÄƒtorul dupÄƒ cel completat
        start_index = issue_links.index(next_to_process)
        pending_links = issue_links[start_index:]

        # FiltreazÄƒ doar cele care nu sunt Ã®n skip list sau complete
        actual_pending = []
        for link in pending_links:
            normalized = link.rstrip('/')
            if normalized in self.dynamic_skip_urls:
                continue
            exists = next((i for i in self.state.get("downloaded_issues", []) if i.get("url") == normalized), None)
            if not exists or not exists.get("completed_at"):
                actual_pending.append(link)

        print(f"ğŸ“Š Procesez {len(actual_pending)} issue-uri din colecÈ›ia curentÄƒ (Ã®ncepÃ¢nd cu {next_to_process})")

        # PROCESEAZÄ‚ CÃ‚TE UN ISSUE PE RÃ‚ND - fÄƒrÄƒ sÄƒ sarÄƒ la altele
        processed_any = False
        for i, link in enumerate(actual_pending):
            print(f"\nğŸ”¢ ISSUE {i+1}/{len(actual_pending)}: {link}")

            # VerificÄƒ cota DOAR Ã®nainte sÄƒ Ã®nceapÄƒ un issue nou
            if self.remaining_quota() <= 0:
                print(f"âš  Limita zilnicÄƒ de {DAILY_LIMIT} issue-uri atinsÄƒ Ã®nainte de a Ã®ncepe acest issue.")
                break

            if self.state.get("daily_limit_hit", False):
                print("âš  Flag daily_limit_hit setat - opresc procesarea.")
                break

            # FOCUSEAZÄ‚ PE UN SINGUR ISSUE
            print(f"ğŸ¯ MÄƒ focusez EXCLUSIV pe: {link}")
            result = self.open_new_tab_and_download(link)

            if result:
                processed_any = True
                print(f"âœ… Issue-ul {link} procesat cu succes!")
            else:
                print(f"âš  Issue-ul {link} nu a fost procesat.")

            # VerificÄƒ din nou cota dupÄƒ procesare
            if self.remaining_quota() <= 0 or self.state.get("daily_limit_hit", False):
                print("âš  Limita zilnicÄƒ atinsÄƒ dupÄƒ procesarea acestui issue.")
                break

            # PauzÄƒ Ã®ntre issue-uri
            if i < len(actual_pending) - 1:  # Nu pune pauzÄƒ dupÄƒ ultimul
                print("â³ PauzÄƒ de 2s Ã®ntre issue-uri...")
                time.sleep(2)

            # FIXED: VerificÄƒ dacÄƒ TOATE issue-urile din aceastÄƒ colecÈ›ie sunt complete
            all_done = True
            total_issues = len(issue_links)
            completed_issues = 0
            pending_issues = []

            for link in issue_links:
                normalized = link.rstrip('/')

                # VerificÄƒ Ã®n skip URLs (complet descÄƒrcate)
                if normalized in self.dynamic_skip_urls:
                    completed_issues += 1
                    continue

                # VerificÄƒ Ã®n state.json
                exists = next((i for i in self.state.get("downloaded_issues", []) if i.get("url") == normalized), None)

                if exists and exists.get("completed_at") and \
                   exists.get("total_pages") and \
                   exists.get("last_successful_segment_end", 0) >= exists.get("total_pages", 0):
                    completed_issues += 1
                else:
                    all_done = False
                    pending_issues.append(link)

            print(f"ğŸ“Š VERIFICARE COLECÈšIE: {completed_issues}/{total_issues} issue-uri complete")
            if pending_issues:
                print(f"ğŸ”„ Issue-uri rÄƒmase: {len(pending_issues)}")
                for i, link in enumerate(pending_issues[:5]):  # AfiÈ™eazÄƒ primele 5
                    print(f"   {i+1}. {link}")
                if len(pending_issues) > 5:
                    print(f"   ... È™i Ã®ncÄƒ {len(pending_issues) - 5} issue-uri")

            return all_done

    def process_pending_partials_first(self):
        """FIXED: ProceseazÄƒ mai Ã®ntÃ¢i issue-urile parÈ›iale, indiferent de colecÈ›ie"""
        pending_partials = self.get_pending_partial_issues()

        if not pending_partials:
            print("âœ… Nu existÄƒ issue-uri parÈ›iale de procesat.")
            return True

        print(f"\nğŸ¯ PRIORITATE: Procesez {len(pending_partials)} issue-uri parÈ›iale:")
        for item in pending_partials:
            url = item.get("url")
            progress = item.get("last_successful_segment_end", 0)
            total = item.get("total_pages", 0)
            print(f"   ğŸ”„ {url} - pagini {progress}/{total}")

        # ProceseazÄƒ issue-urile parÈ›iale
        processed_any = False
        for item in pending_partials:
            if self.remaining_quota() <= 0 or self.state.get("daily_limit_hit", False):
                print(f"âš  Limita zilnicÄƒ atinsÄƒ Ã®n timpul issue-urilor parÈ›iale.")
                break

            url = item.get("url")
            result = self.open_new_tab_and_download(url)
            if result:
                processed_any = True
            time.sleep(1)

        return processed_any

    def run_additional_collections(self):
        """FIXED: ProceseazÄƒ colecÈ›iile adiÈ›ionale Ã®n ordinea corectÄƒ"""
        start_index = self.state.get("current_additional_collection_index", 0)

        # VERIFICÄ‚ cÄƒ indexul e valid
        if start_index >= len(ADDITIONAL_COLLECTIONS):
            print("âœ… TOATE colecÈ›iile adiÈ›ionale au fost procesate!")
            return True

        print(f"ğŸ”„ Continuez cu colecÈ›iile adiÈ›ionale de la indexul {start_index}")

        for i in range(start_index, len(ADDITIONAL_COLLECTIONS)):
            collection_url = ADDITIONAL_COLLECTIONS[i]

            print(f"\nğŸ“š COLECÈšIA {i+1}/{len(ADDITIONAL_COLLECTIONS)}: {collection_url}")

            # ADAUGÄ‚ ACEASTÄ‚ VERIFICARE
            if collection_url.rstrip('/') in self.dynamic_skip_urls:
                print(f"â­ï¸ Sar peste colecÈ›ia {i+1}/{len(ADDITIONAL_COLLECTIONS)} (deja completÄƒ): {collection_url}")
                self.state["current_additional_collection_index"] = i + 1
                self._save_state()
                continue

            if self.remaining_quota() <= 0 or self.state.get("daily_limit_hit", False):
                print(f"âš  Limita zilnicÄƒ atinsÄƒ, opresc procesarea colecÈ›iilor adiÈ›ionale.")
                break

            print(f"\nğŸ”„ Procesez colecÈ›ia adiÈ›ionalÄƒ {i+1}/{len(ADDITIONAL_COLLECTIONS)}: {collection_url}")

            self.state["current_additional_collection_index"] = i
            self._save_state()

            collection_completed = self.run_collection(collection_url)

            if collection_completed and not self.state.get("daily_limit_hit", False):
                print(f"âœ… ColecÈ›ia adiÈ›ionalÄƒ {i+1} completÄƒ, o marchez È™i trec la urmÄƒtoarea.")
                self.mark_collection_complete(collection_url)
                self.state["current_additional_collection_index"] = i + 1
                self._save_state()
                print(f"ğŸ”„ Continu cu urmÄƒtoarea colecÈ›ie adiÈ›ionalÄƒ...")
            elif self.state.get("daily_limit_hit", False):
                print("âš  Limita zilnicÄƒ atinsÄƒ - opresc procesarea.")
                break
            else:
                print(f"âš  ColecÈ›ia adiÈ›ionalÄƒ {i+1} nu este completÄƒ Ã®ncÄƒ - voi continua cu urmÄƒtoarea!")
                print(f"ğŸ”„ Continu cu urmÄƒtoarea colecÈ›ie adiÈ›ionalÄƒ...")

        current_index = self.state.get("current_additional_collection_index", 0)
        if current_index >= len(ADDITIONAL_COLLECTIONS):
            print("ğŸ‰ TOATE colecÈ›iile adiÈ›ionale au fost procesate!")
            print("ğŸ”„ Voi continua cu urmÄƒtoarea colecÈ›ie disponibilÄƒ...")
            return False  # Returnez False pentru a continua cu urmÄƒtoarea colecÈ›ie
        else:
            remaining = len(ADDITIONAL_COLLECTIONS) - current_index
            print(f"âš  Mai rÄƒmÃ¢n {remaining} colecÈ›ii adiÈ›ionale de procesat.")
            print("ğŸ”„ Voi continua cu urmÄƒtoarea colecÈ›ie...")
            return False  # Returnez False pentru a continua

    def run(self):
        print("ğŸ§ª Ãncep executarea Chrome PDF Downloader FIXED")
        print("=" * 60)

        try:
            if not self.setup_chrome_driver():
                return False

            print("ğŸ”„ Resetez flag-ul de limitÄƒ zilnicÄƒ È™i verific ferestrele existente...")
            self.state["daily_limit_hit"] = False
            self._save_state()

            # FIXED: ReconstruieÈ™te progresul din fiÈ™ierele de pe disk
            self.sync_json_with_disk_files()

            # CLEANUP dubluri DUPÄ‚ sincronizarea cu disk-ul
            self.cleanup_duplicate_issues()

            if self.check_daily_limit_in_all_windows(set_flag=False):
                print("âš  Am gÄƒsit ferestre cu limita deschise din sesiuni anterioare.")
                print("ğŸ”„ Le-am Ã®nchis È™i reÃ®ncerc procesarea...")

            # FIXED: ETAPA 0: ProceseazÄƒ MAI ÃNTÃ‚I issue-urile parÈ›iale (PRIORITATE ABSOLUTÄ‚)
            print(f"\nğŸ¯ ETAPA 0: PRIORITATE ABSOLUTÄ‚ - Procesez issue-urile parÈ›iale")
            if self.process_pending_partials_first():
                print("âœ… Issue-urile parÈ›iale au fost procesate sau limita a fost atinsÄƒ.")
                # OPREÈ˜TE aici dacÄƒ existÄƒ parÈ›iale - nu trece la colecÈ›ii noi
                if self.get_pending_partial_issues():
                    print("ğŸ”„ ÃncÄƒ mai existÄƒ issue-uri parÈ›iale - voi continua cu ele urmÄƒtoarea datÄƒ")
                    return True

            if self.remaining_quota() <= 0 or self.state.get("daily_limit_hit", False):
                print("âš  Limita zilnicÄƒ atinsÄƒ dupÄƒ procesarea issue-urilor parÈ›iale.")
                return True

            # ETAPA 1: ProceseazÄƒ colecÈ›ia principalÄƒ (dacÄƒ nu e completÄƒ)
            # FIXED: VerificÄƒ Ã®ntotdeauna dacÄƒ colecÈ›ia principalÄƒ e completÄƒ
            print(f"\nğŸ“š ETAPA 1: Verific colecÈ›ia principalÄƒ: {self.main_collection_url}")

            main_completed = self.run_collection(self.main_collection_url)

            if self.state.get("daily_limit_hit", False):
                print("âš  Limita zilnicÄƒ atinsÄƒ Ã®n colecÈ›ia principalÄƒ.")
                return True

            if main_completed:
                print("âœ… ColecÈ›ia principalÄƒ este completÄƒ!")
                self.state["main_collection_completed"] = True
                self._save_state()
            else:
                print("ğŸ”„ ColecÈ›ia principalÄƒ nu este completÄƒ Ã®ncÄƒ - continuez cu ea.")
                self.state["main_collection_completed"] = False  # RESETEAZÄ‚ dacÄƒ nu e completÄƒ
                self._save_state()
                return True

            # ETAPA 2: ProceseazÄƒ colecÈ›iile adiÈ›ionale
            if self.remaining_quota() > 0 and not self.state.get("daily_limit_hit", False):
                print(f"\nğŸ“š ETAPA 2: Procesez colecÈ›iile adiÈ›ionale")
                self.run_additional_collections()

            print("âœ… Toate operaÈ›iunile au fost iniÈ›iate.")
            self._finalize_session()
            return True

        except KeyboardInterrupt:
            print("\n\nâš  IntervenÈ›ie manualÄƒ: Ã®ntrerupt.")
            return False
        except Exception as e:
            print(f"\nâŒ Eroare neaÈ™teptatÄƒ: {e}")
            return False
        finally:
            if not self.attached_existing and self.driver:
                try:
                    self.driver.quit()
                except Exception:
                    pass

    def _finalize_session(self):
        if self.driver:
            if self.attached_existing:
                print("ğŸ”– Am pÄƒstrat sesiunea Chrome existentÄƒ deschisÄƒ (nu fac quit).")
            else:
                print("ğŸšª Ãnchid browserul.")
                try:
                    self.driver.quit()
                except Exception:
                    pass


def main():
    """
    MAIN FUNCTION CORECTATÄ‚ - FOCUSEAZÄ‚ PE StudiiSiCercetariMecanicaSiAplicata
    Nu mai sare la alte colecÈ›ii pÃ¢nÄƒ nu terminÄƒ cu aceasta complet!
    """

    print("ğŸš€ PORNIRE SCRIPT - ANALIZA INIÈšIALÄ‚")
    print("=" * 70)

    # PASUL 1: CreeazÄƒ downloader temporar pentru analiza stÄƒrii
    temp_downloader = ChromePDFDownloader("temp", download_dir="D:\\", batch_size=50)

    # PASUL 2: AnalizeazÄƒ starea curentÄƒ
    print("ğŸ” ANALIZA STÄ‚RII CURENTE:")
    current_state = temp_downloader.state

    main_completed = current_state.get("main_collection_completed", False)
    current_index = current_state.get("current_additional_collection_index", 0)
    total_issues = len(current_state.get("downloaded_issues", []))

    print(f"   ğŸ“Š Total issues Ã®n state: {total_issues}")
    print(f"   ğŸ Main collection completed: {main_completed}")
    print(f"   ğŸ”¢ Current additional index: {current_index}")

    # PASUL 3: VerificÄƒ issue-urile parÈ›iale (PRIORITATE ABSOLUTÄ‚)
    print(f"\nğŸ¯ VERIFICARE ISSUE-URI PARÈšIALE:")
    pending_partials = temp_downloader.get_pending_partial_issues()

    if pending_partials:
        print(f"ğŸš¨ GÄ‚SITE {len(pending_partials)} ISSUE-URI PARÈšIALE!")
        print(f"ğŸ”¥ PRIORITATE ABSOLUTÄ‚ - acestea trebuie continuate:")

        for item in pending_partials:
            url = item.get("url", "")
            progress = item.get("last_successful_segment_end", 0)
            total = item.get("total_pages", 0)
            title = item.get("title", "")
            print(f"   ğŸ”„ {title}")
            print(f"      ğŸ“ {url}")
            print(f"      ğŸ¯ CONTINUÄ‚ de la pagina {progress + 1} (progres: {progress}/{total})")

        print(f"\nâœ… VA PROCESA AUTOMAT issue-urile parÈ›iale primul!")
    else:
        print(f"âœ… Nu existÄƒ issue-uri parÈ›iale de procesat")

    # PASUL 4: AnalizeazÄƒ progresul StudiiSiCercetariMecanicaSiAplicata
    print(f"\nğŸ“š ANALIZA COLECÈšIEI StudiiSiCercetariMecanicaSiAplicata:")

    # Lista completÄƒ a anilor disponibili din HTML (1954-1992, minus 1964)
    expected_years = []
    for year in range(1954, 1993):  # 1954-1992
        if year != 1964:  # 1964 nu existÄƒ Ã®n colecÈ›ie
            expected_years.append(year)

    # VerificÄƒ care ani au fost descÄƒrcaÈ›i
    downloaded_years = []
    partial_years = []

    for item in current_state.get("downloaded_issues", []):
        url = item.get("url", "")
        if "StudiiSiCercetariMecanicaSiAplicata" in url:
            # Extrage anul din URL
            year_match = re.search(r'StudiiSiCercetariMecanicaSiAplicata_(\d{4})', url)
            if year_match:
                year = int(year_match.group(1))
                if item.get("completed_at"):
                    downloaded_years.append(year)
                else:
                    partial_years.append(year)

    downloaded_years.sort()
    partial_years.sort()
    missing_years = [year for year in expected_years if year not in downloaded_years and year not in partial_years]

    print(f"   ğŸ“… Ani disponibili: {len(expected_years)} (1954-1992, minus 1964)")
    print(f"   âœ… Ani descÄƒrcaÈ›i: {len(downloaded_years)} - {downloaded_years}")
    print(f"   ğŸ”„ Ani parÈ›iali: {len(partial_years)} - {partial_years}")
    print(f"   âŒ Ani lipsÄƒ: {len(missing_years)} - {missing_years[:10]}{'...' if len(missing_years) > 10 else ''}")

    # PASUL 5: DeterminÄƒ strategia
    total_remaining = len(partial_years) + len(missing_years)

    if total_remaining > 0:
        print(f"\nğŸ¯ STRATEGIA DE PROCESARE:")
        print(f"   ğŸ”¥ RÄ‚MÃ‚N {total_remaining} ani de procesat din StudiiSiCercetariMecanicaSiAplicata")
        print(f"   ğŸš« NU se trece la alte colecÈ›ii pÃ¢nÄƒ nu se terminÄƒ aceasta!")
        print(f"   ğŸ“ˆ Progres: {len(downloaded_years)}/{len(expected_years)} ani completaÈ›i ({len(downloaded_years)/len(expected_years)*100:.1f}%)")
    else:
        print(f"\nâœ… StudiiSiCercetariMecanicaSiAplicata este COMPLET!")
        print(f"   ğŸ¯ Va trece la urmÄƒtoarea colecÈ›ie din ADDITIONAL_COLLECTIONS")

    # PASUL 6: ReseteazÄƒ starea pentru a continua corect cu StudiiSiCercetariMecanicaSiAplicata
    if total_remaining > 0:
        print(f"\nğŸ”§ RESETEZ STAREA pentru a continua cu StudiiSiCercetariMecanicaSiAplicata:")

        # ReseteazÄƒ flag-urile greÈ™ite
        if main_completed:
            print(f"   ğŸ”„ Resetez main_collection_completed: True â†’ False")
            temp_downloader.state["main_collection_completed"] = False

        if current_index > 1:  # StudiiSiCercetariMecanicaSiAplicata e pe index 1
            print(f"   ğŸ”„ Resetez current_additional_collection_index: {current_index} â†’ 1")
            temp_downloader.state["current_additional_collection_index"] = 1

        temp_downloader._save_state()
        print(f"   âœ… Starea resetatÄƒ pentru a continua cu StudiiSiCercetariMecanicaSiAplicata")

    # PASUL 7: SeteazÄƒ URL-ul colecÈ›iei principale
    main_collection_url = "https://adt.arcanum.com/ro/collection/StudiiSiCercetariMecanicaSiAplicata/"

    print(f"\nğŸš€ ÃNCEPE PROCESAREA:")
    print(f"ğŸ“ URL principal: {main_collection_url}")
    print(f"ğŸ“ Director descÄƒrcare: D:\\")
    print(f"ğŸ“¦ Batch size: 50 pagini per segment")

    if pending_partials:
        print(f"âš¡ Va Ã®ncepe cu {len(pending_partials)} issue-uri parÈ›iale")
    if missing_years:
        print(f"ğŸ“… Va continua cu anii lipsÄƒ: {missing_years[:5]}{'...' if len(missing_years) > 5 else ''}")

    print("=" * 70)

    # PASUL 8: CreeazÄƒ downloader-ul principal È™i porneÈ™te procesarea
    try:
        downloader = ChromePDFDownloader(
            main_collection_url=main_collection_url,
            download_dir="D:\\",
            batch_size=50
        )

        print("ğŸ¯ ÃNCEPE EXECUÈšIA PRINCIPALÄ‚...")
        success = downloader.run()

        if success:
            print("\nâœ… EXECUÈšIE FINALIZATÄ‚ CU SUCCES!")
        else:
            print("\nâš  EXECUÈšIE FINALIZATÄ‚ CU PROBLEME!")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n\nâš  OPRIRE MANUALÄ‚ - Progresul a fost salvat")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ EROARE FATALÄ‚ Ã®n main(): {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"âŒ Eroare fatalÄƒ Ã®n __main__: {e}")
        sys.exit(1)
