#!/usr/bin/env python3
"""
MASS FRESH DAY INTERCEPTOR - MaximizeazÄƒ beneficiul zilnic
Strategia: DimineaÈ›a cÃ¢nd daily limit e resetat, descarcÄƒ rapid MULTE documente diferite
pentru a capta cÃ¢t mai multe URL-uri de bypass
"""

import time
import os
import json
import requests
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

class MassFreshDayInterceptor:
    def __init__(self, download_dir="D:\\"):
        self.download_dir = download_dir
        self.driver = None
        self.wait = None
        self.captured_urls = {}  # {document_info: [urls]}
        self.monitoring = False
        self.daily_limit_reached = False

    def setup_chrome_monitoring(self):
        """Setup Chrome pentru mass interceptare"""
        try:
            print("ğŸ”§ Setup Chrome pentru MASS FRESH DAY INTERCEPTOR...")

            chrome_options = Options()
            chrome_options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")
            chrome_options.set_capability('goog:loggingPrefs', {'performance': 'ALL'})

            self.driver = webdriver.Chrome(options=chrome_options)
            self.wait = WebDriverWait(self.driver, 30)

            self.driver.execute_cdp_cmd('Runtime.enable', {})
            self.driver.execute_cdp_cmd('Network.enable', {})

            print("âœ… Chrome mass interceptor ACTIVAT")
            return True

        except Exception as e:
            print(f"âŒ Setup eÈ™uat: {e}")
            return False

    def get_document_priority_list(self):
        """Lista de documente prioritare pentru captare"""
        documents = [
            {
                "name": "Gazeta MatematicÄƒ Seria B 1960",
                "url": "https://adt.arcanum.com/ro/view/GazetaMatematicaSiFizicaSeriaB_1960/?pg=0&layout=s",
                "segments": [(1, 50), (51, 100), (101, 150), (151, 200)]
            },
            {
                "name": "Gazeta MatematicÄƒ Seria B 1961",
                "url": "https://adt.arcanum.com/ro/view/GazetaMatematicaSiFizicaSeriaB_1961/?pg=0&layout=s",
                "segments": [(1, 50), (51, 100), (101, 150)]
            },
            {
                "name": "Gazeta MatematicÄƒ Seria B 1962",
                "url": "https://adt.arcanum.com/ro/view/GazetaMatematicaSiFizicaSeriaB_1962/?pg=0&layout=s",
                "segments": [(1, 50), (51, 100)]
            }
            # AdaugÄƒ mai multe documente dupÄƒ necesitate
        ]
        return documents

    def navigate_to_document(self, doc_info):
        """NavigheazÄƒ la un document specific"""
        try:
            print(f"\nğŸ“– NavigheazÄƒ la: {doc_info['name']}")
            self.driver.get(doc_info['url'])

            self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            time.sleep(3)  # LasÄƒ timpul sÄƒ se Ã®ncarce
            print(f"âœ… Document Ã®ncÄƒrcat: {doc_info['name']}")
            return True

        except Exception as e:
            print(f"âŒ Navigare eÈ™uatÄƒ pentru {doc_info['name']}: {e}")
            return False

    def attempt_segment_download(self, start_page, end_page):
        """ÃncearcÄƒ sÄƒ descarce un segment de pagini"""
        try:
            print(f"ğŸ¯ Ãncerc segment {start_page}-{end_page}...")

            # GÄƒseÈ™te butonul de salvare
            save_icon = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'svg[data-testid="SaveAltIcon"]')))
            save_button = save_icon.find_element(By.XPATH, "./ancestor::button")
            save_button.click()

            time.sleep(2)

            # CompleteazÄƒ input-urile
            first_input = self.wait.until(EC.presence_of_element_located((By.ID, "first page")))
            last_input = self.wait.until(EC.presence_of_element_located((By.ID, "last page")))

            # È˜terge È™i completeazÄƒ
            first_input.clear()
            first_input.send_keys(str(start_page))

            last_input.clear()
            last_input.send_keys(str(end_page))

            time.sleep(1)

            # ApasÄƒ Save final
            final_save = self.wait.until(EC.element_to_be_clickable((
                By.XPATH,
                '//button[contains(text(), "Save") or contains(text(), "SalvaÈ›i")]'
            )))

            final_save.click()
            print(f"âœ… Download iniÈ›iat pentru {start_page}-{end_page}")

            return True

        except Exception as e:
            print(f"âŒ Eroare la download {start_page}-{end_page}: {e}")
            return False

    def monitor_for_urls_and_limit(self, doc_name, timeout_seconds=30):
        """MonitorizeazÄƒ pentru URL-uri È™i detecteazÄƒ daily limit"""
        print(f"ğŸ‘€ Monitorizez URL-uri pentru {doc_name}...")

        found_urls = []
        start_time = time.time()

        while (time.time() - start_time) < timeout_seconds:
            try:
                logs = self.driver.get_log('performance')

                for log_entry in logs:
                    try:
                        message = json.loads(log_entry['message'])

                        if message['message']['method'] == 'Network.responseReceived':
                            url = message['message']['params']['response']['url']

                            # DetecteazÄƒ URL-uri PDF
                            if ('static-cdn.arcanum.com/downloads/' in url and
                                url.endswith('.pdf') and
                                url not in found_urls):

                                found_urls.append(url)
                                print(f"ğŸ¯ URL capturat pentru {doc_name}: {url}")

                            # DetecteazÄƒ daily limit
                            elif 'check-access-save' in url:
                                print(f"âš ï¸ DAILY LIMIT DETECTAT pentru {doc_name}!")
                                self.daily_limit_reached = True
                                return found_urls

                    except (KeyError, json.JSONDecodeError):
                        continue

                time.sleep(1)

            except Exception as e:
                print(f"âš ï¸ Eroare monitorizare: {e}")
                time.sleep(1)

        return found_urls

    def run_mass_fresh_day_capture(self):
        """RuleazÄƒ captura masivÄƒ fresh day"""
        print("ğŸŒ… MASS FRESH DAY INTERCEPTOR")
        print("=" * 60)
        print("STRATEGIA MAXIMIZÄ‚RII:")
        print("âœ… DimineaÈ›a cÃ¢nd daily limit e resetat")
        print("âœ… DescarcÄƒ RAPID multe documente diferite")
        print("âœ… CapteazÄƒ URL-uri pentru fiecare")
        print("âœ… OpreÈ™te cÃ¢nd daily limit devine activ")
        print("âœ… SalveazÄƒ toate URL-urile pentru bypass")
        print("=" * 60)

        if not self.setup_chrome_monitoring():
            return False

        documents = self.get_document_priority_list()
        total_urls_captured = 0

        for doc_info in documents:
            if self.daily_limit_reached:
                print(f"ğŸ›‘ Daily limit atins - opresc captura")
                break

            print(f"\nğŸ“š PROCESEZ DOCUMENTUL: {doc_info['name']}")

            if not self.navigate_to_document(doc_info):
                continue

            doc_urls = []

            for start_page, end_page in doc_info['segments']:
                if self.daily_limit_reached:
                    break

                print(f"\nğŸ“„ Segment {start_page}-{end_page} din {doc_info['name']}")

                if self.attempt_segment_download(start_page, end_page):
                    # MonitorizeazÄƒ pentru URL-uri
                    segment_urls = self.monitor_for_urls_and_limit(
                        f"{doc_info['name']} ({start_page}-{end_page})",
                        timeout_seconds=45
                    )

                    if segment_urls:
                        doc_urls.extend(segment_urls)
                        total_urls_captured += len(segment_urls)
                        print(f"âœ… {len(segment_urls)} URL-uri capturate pentru segment")

                    # PauzÄƒ Ã®ntre segmente
                    if not self.daily_limit_reached:
                        time.sleep(5)
                else:
                    print(f"âš ï¸ Nu am putut descÄƒrca segmentul {start_page}-{end_page}")

            if doc_urls:
                self.captured_urls[doc_info['name']] = doc_urls
                print(f"ğŸ“Š Total pentru {doc_info['name']}: {len(doc_urls)} URL-uri")

        # SumarizeazÄƒ rezultatele
        print(f"\nğŸ‰ CAPTURA MASIVÄ‚ COMPLETÄ‚!")
        print(f"ğŸ“Š Total URL-uri capturate: {total_urls_captured}")
        print(f"ğŸ“š Documente procesate: {len(self.captured_urls)}")

        for doc_name, urls in self.captured_urls.items():
            print(f"   ğŸ“– {doc_name}: {len(urls)} URL-uri")

        # SalveazÄƒ toate URL-urile
        self.save_mass_urls_to_file()

        # TesteazÄƒ cÃ¢teva URL-uri random
        self.test_random_urls()

        return len(self.captured_urls) > 0

    def save_mass_urls_to_file(self):
        """SalveazÄƒ toate URL-urile capturate"""
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            filename = f"mass_fresh_urls_{timestamp}.json"
            filepath = os.path.join(self.download_dir, filename)

            # SalveazÄƒ ca JSON pentru structurÄƒ
            data = {
                "timestamp": timestamp,
                "total_documents": len(self.captured_urls),
                "total_urls": sum(len(urls) for urls in self.captured_urls.values()),
                "daily_limit_reached": self.daily_limit_reached,
                "documents": self.captured_urls
            }

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            print(f"ğŸ’¾ Mass URLs salvate Ã®n: {filename}")

            # SalveazÄƒ È™i ca text pentru referinÈ›Äƒ rapidÄƒ
            txt_filename = f"mass_fresh_urls_{timestamp}.txt"
            txt_filepath = os.path.join(self.download_dir, txt_filename)

            with open(txt_filepath, 'w', encoding='utf-8') as f:
                f.write(f"Mass Fresh Day URLs - {timestamp}\n")
                f.write(f"{'='*50}\n\n")

                for doc_name, urls in self.captured_urls.items():
                    f.write(f"{doc_name}:\n")
                    for i, url in enumerate(urls, 1):
                        f.write(f"  {i}. {url}\n")
                    f.write(f"\n")

            print(f"ğŸ’¾ Text URLs salvate Ã®n: {txt_filename}")

        except Exception as e:
            print(f"âš ï¸ Nu am putut salva mass URLs: {e}")

    def test_random_urls(self, max_tests=3):
        """TesteazÄƒ cÃ¢teva URL-uri random pentru validare"""
        print(f"\nğŸ§ª Testez {max_tests} URL-uri random pentru validare...")

        all_urls = []
        for doc_name, urls in self.captured_urls.items():
            for url in urls:
                all_urls.append((doc_name, url))

        if not all_urls:
            print("âŒ Nu existÄƒ URL-uri de testat")
            return

        import random
        test_urls = random.sample(all_urls, min(max_tests, len(all_urls)))

        for i, (doc_name, url) in enumerate(test_urls, 1):
            print(f"\nğŸ§ª Test {i}/{len(test_urls)}: {doc_name}")
            self.test_direct_download(url, f"_mass_test_{i}")

    def test_direct_download(self, static_url, suffix=""):
        """TesteazÄƒ descÄƒrcarea directÄƒ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/pdf,*/*',
                'Referer': 'https://adt.arcanum.com/',
            }

            head_response = requests.head(static_url, headers=headers, timeout=10)

            if head_response.status_code == 200:
                content_length = int(head_response.headers.get('content-length', 0))
                print(f"âœ… URL valid - Size: {content_length/1024/1024:.2f} MB")

                # Download mic pentru test
                response = requests.get(static_url, headers=headers, timeout=30, stream=True)
                response.raise_for_status()

                filename = f"mass_test{suffix}_{int(time.time())}.pdf"
                filepath = os.path.join(self.download_dir, filename)

                # SalveazÄƒ doar primii cÃ¢teva MB pentru test
                total_size = 0
                max_size = 5 * 1024 * 1024  # 5 MB max pentru test

                with open(filepath, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk and total_size < max_size:
                            f.write(chunk)
                            total_size += len(chunk)
                        else:
                            break

                file_size_mb = total_size / (1024 * 1024)
                print(f"âœ… Test reuÈ™it: {filename} ({file_size_mb:.2f} MB)")
                return True
            else:
                print(f"âŒ URL invalid - Status: {head_response.status_code}")
                return False

        except Exception as e:
            print(f"âŒ Test eÈ™uat: {e}")
            return False

def main():
    """FuncÈ›ia principalÄƒ pentru mass fresh day"""
    print("ğŸŒ… MASS FRESH DAY INTERCEPTOR - MaximizeazÄƒ beneficiul")
    print("=" * 60)
    print("REALITATEA BYPASS-ULUI:")
    print("âœ… FuncÈ›ioneazÄƒ doar pentru documente capturate dimineaÈ›a")
    print("âŒ NU genereazÄƒ URL-uri pentru documente noi dupÄƒ daily limit")
    print("ğŸ’¡ SOLUÈšIA: CapteazÄƒ URL-uri pentru MULTE documente dimineaÈ›a!")
    print("=" * 60)
    print("STRATEGIA MASS CAPTURE:")
    print("âœ… DimineaÈ›a: DescarcÄƒ segmente din MULTE documente")
    print("âœ… CapteazÄƒ URL-uri pentru fiecare document")
    print("âœ… OpreÈ™te cÃ¢nd daily limit devine activ")
    print("âœ… SalveazÄƒ toate URL-urile pentru bypass")
    print("âœ… ToatÄƒ ziua: FoloseÈ™te URL-urile pentru orice segment")
    print("=" * 60)

    input("ApasÄƒ ENTER cÃ¢nd Chrome cu debugging este pregÄƒtit È™i daily limit e resetat...")

    interceptor = MassFreshDayInterceptor(download_dir="D:\\")

    success = interceptor.run_mass_fresh_day_capture()

    if success:
        print(f"\nğŸ‰ MASS FRESH DAY CAPTURE REUÈ˜IT!")
        print(f"âœ… Ai URL-uri pentru multiple documente!")
        print(f"âœ… FoloseÈ™te-le toatÄƒ ziua pentru bypass!")
    else:
        print(f"\nâŒ MASS CAPTURE INCOMPLET")
        print(f"ğŸ’¡ ÃncearcÄƒ mÃ¢ine dimineaÈ›a")

    input("\nApasÄƒ ENTER pentru a Ã®nchide...")

if __name__ == "__main__":
    main()